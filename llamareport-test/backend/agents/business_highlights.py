"""
ä¸šåŠ¡äº®ç‚¹ç« èŠ‚ç”Ÿæˆå·¥å…·
"""

import logging
from typing import Dict, Any, Annotated, Optional, List

import json
import re
import asyncio
import time
from pathlib import Path

from llama_index.core import Settings
from llama_index.core.llms import ChatMessage
from models.report_models import BusinessHighlights
from models.business_schema import (
    IndustryClassificationResult,
    SegmentSelectionResult,
    ExtractedSegmentMetrics,
    BusinessPerformanceReport
)

from agents.report_common import _validate_and_clean_data
from agents.business_schema_templates import get_business_schema, BUSINESS_SCHEMA_TEMPLATES

logger = logging.getLogger(__name__)

MAX_TOTAL_SECONDS = 180
QUERY_TIMEOUT_SECONDS = 35
LLM_TIMEOUT_SECONDS = 45
METRIC_RULES_PATH = Path(__file__).resolve().parent / "business_metric_rules.json"


def _load_metric_rules() -> Dict[str, Any]:
    try:
        with METRIC_RULES_PATH.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"âš ï¸ æŒ‡æ ‡è§„åˆ™åŠ è½½å¤±è´¥: {e}")
        return {}


METRIC_RULES = _load_metric_rules()


def _get_metric_rule(industry: str, segment_id: str) -> Dict[str, Any]:
    direct = (
        METRIC_RULES.get(industry, {})
        .get(segment_id, {})
    )
    if direct:
        return direct
    for industry_key, segments in METRIC_RULES.items():
        if segment_id in segments:
            logger.info(f"ğŸ” [business_highlights] æŒ‡æ ‡è§„åˆ™è¡Œä¸šå›é€€: {industry} -> {industry_key}")
            return segments.get(segment_id, {})
    return {}


def _normalize_metric_name(name: str) -> str:
    return name.replace(" ", "").replace("ï¼ˆ", "(").replace("ï¼‰", ")")


def _build_metric_aliases(metric_name: str) -> List[str]:
    if not metric_name:
        return []
    aliases = set()
    aliases.add(metric_name)
    replacements = {
        "ä½™é¢": ["è§„æ¨¡", "ä½™é¢"],
        "æ”¶å…¥": ["æ”¶å…¥", "è¥æ”¶"],
        "å‡€åˆ©æ¶¦": ["å‡€åˆ©æ¶¦", "åˆ©æ¶¦", "å‡€åˆ©"],
        "ä¸è‰¯ç‡": ["ä¸è‰¯ç‡", "ä¸è‰¯è´·æ¬¾ç‡"],
        "å‡å€¼æŸå¤±": ["å‡å€¼æŸå¤±", "ä¿¡ç”¨å‡å€¼æŸå¤±"],
        "AUM": ["AUM", "ç®¡ç†èµ„äº§è§„æ¨¡"],
        "å®¢æˆ·æ•°": ["å®¢æˆ·æ•°", "å®¢æˆ·æ•°é‡"],
    }
    for key, candidates in replacements.items():
        if key in metric_name:
            for candidate in candidates:
                aliases.add(metric_name.replace(key, candidate))
    return [a for a in aliases if a]


def _infer_industry_from_company_name(company_name: str) -> Optional[str]:
    if not company_name:
        return None
    if "é“¶è¡Œ" in company_name:
        return "banking"
    if "ä¿é™©" in company_name or "äººå¯¿" in company_name:
        return "insurance"
    if "è¯åˆ¸" in company_name:
        return "securities"
    if "äº’è”ç½‘" in company_name or "ç§‘æŠ€" in company_name:
        return "internet_platform"
    if "åˆ¶é€ " in company_name or "å·¥ä¸š" in company_name:
        return "manufacturing"
    return None


def _map_dimension_to_category(dimension: str) -> str:
    dim = (dimension or "").lower()
    if "profit" in dim or "ç›ˆåˆ©" in dim:
        return "profitability"
    if "risk" in dim or "é£é™©" in dim:
        return "risk"
    if "efficiency" in dim or "capability" in dim or "æ•ˆç‡" in dim or "èƒ½åŠ›" in dim:
        return "efficiency"
    return "scale"


def _extract_numeric_candidates(text: str) -> List[str]:
    if not text:
        return []
    pattern = r"(\d{1,3}(?:,\d{3})*|\d+)(?:\.\d+)?(?:ä¸‡äº¿|ä¸‡|äº¿|å…ƒ|%|äº¿å…ƒ|ä¸‡å…ƒ|bp|bps)?"
    return [m.group(0) for m in re.finditer(pattern, text)]


def _extract_year_value(text: str, target_year: str, exclude_values: Optional[set] = None) -> Optional[str]:
    if not text:
        return None
    exclude_values = exclude_values or set()
    pattern = rf"{re.escape(target_year)}[^\d]{{0,12}}([\d,\.]+(?:ä¸‡äº¿|ä¸‡|äº¿|å…ƒ|%|äº¿å…ƒ|ä¸‡å…ƒ|bp|bps)?)"
    matches = list(re.finditer(pattern, text))
    for match in matches:
        value = match.group(1)
        if value and value not in exclude_values and value != target_year:
            return value
    return None


def _extract_yoy_change(text: str) -> Optional[str]:
    if not text:
        return None
    pattern = r"(åŒæ¯”|å¢é•¿|ä¸‹é™|å‡å°‘|å¢é€Ÿ)[^\d]{0,8}([\d,\.]+%)"
    match = re.search(pattern, text)
    if match:
        return match.group(2)
    return None


async def _enrich_metrics_with_rules(
    metrics_mapping: Dict[str, Any],
    industry: str,
    company_name: str,
    year: str,
    query_engine: Any,
    time_remaining_func
) -> Dict[str, Any]:
    if not metrics_mapping.get("segments"):
        return metrics_mapping

    max_queries = 12
    query_count = 0

    for segment in metrics_mapping.get("segments", []):
        segment_id = segment.get("segment_id")
        segment_name = segment.get("segment_name", segment_id)
        if not segment_id:
            continue
        rule = _get_metric_rule(industry, segment_id) or {}
        required = rule.get("required", [])
        optional = rule.get("optional", [])
        metrics_to_fetch = required + optional
        if metrics_to_fetch:
            metric_names = [m.get("name") for m in metrics_to_fetch if m.get("name")]
            logger.info(f"ğŸ” [business_highlights] ä¸šåŠ¡æ¿å— {segment_id} æŒ‡æ ‡æ£€ç´¢åˆ—è¡¨: {metric_names}")

        mapped_metrics = segment.setdefault("mapped_metrics", {})

        existing = {}
        for category, items in mapped_metrics.items():
            if not isinstance(items, list):
                continue
            for item in items:
                metric_name = item.get("metric")
                if metric_name:
                    existing[_normalize_metric_name(metric_name)] = item

        for metric in metrics_to_fetch:
            if query_count >= max_queries or time_remaining_func() <= 15:
                return metrics_mapping
            metric_name = metric.get("name")
            if not metric_name:
                continue
            if _normalize_metric_name(metric_name) in existing:
                continue

            aliases = _build_metric_aliases(metric_name)
            query_terms = " ".join(aliases[:3]) if aliases else metric_name
            query = (
                f"{company_name} {year}å¹´ {segment_name} {query_terms} "
                f"ä¸Šå¹´ åŒæ¯” å˜åŠ¨ æ•°å€¼"
            )
            try:
                raw_text = await _run_query_with_timeout(
                    query_engine,
                    query,
                    QUERY_TIMEOUT_SECONDS
                )
            except Exception as e:
                logger.warning(f"âš ï¸ æŒ‡æ ‡æ£€ç´¢å¤±è´¥: {segment_id}-{metric_name}: {e}")
                raw_text = ""

            prev_year = str(int(year) - 1) if year.isdigit() else ""
            exclude = {year, prev_year} if prev_year else {year}
            current_val = _extract_year_value(str(raw_text), year, exclude) or None
            prev_val = _extract_year_value(str(raw_text), prev_year, exclude) if prev_year else None
            yoy_change = _extract_yoy_change(str(raw_text))
            if not current_val:
                candidates = _extract_numeric_candidates(str(raw_text))
                for candidate in candidates:
                    if candidate not in exclude:
                        current_val = candidate
                        break
            logger.info(
                f"ğŸ“Œ [business_highlights] {segment_id} - {metric_name}: "
                f"{year}={current_val or '/'} {prev_year or 'ä¸Šå¹´'}={prev_val or '/'} åŒæ¯”={yoy_change or '/'}"
            )

            category = _map_dimension_to_category(metric.get("dimension"))
            mapped_metrics.setdefault(category, [])
            mapped_metrics[category].append({
                "metric": metric_name,
                "current_year": current_val or "/",
                "previous_year": prev_val or "/",
                "yoy_change": yoy_change or "/",
                "evidence": str(raw_text)[:500]
            })
            query_count += 1

    return metrics_mapping


async def _run_with_timeout(coro, timeout: int, fallback, step_name: str):
    try:
        return await asyncio.wait_for(coro, timeout=timeout)
    except asyncio.TimeoutError:
        logger.warning(f"âš ï¸ [generate_business_highlights] {step_name} è¶…æ—¶({timeout}s)ï¼Œä½¿ç”¨é™çº§ç»“æœ")
        return fallback


async def _run_query_with_timeout(query_engine: Any, query: str, timeout: int) -> str:
    loop = asyncio.get_running_loop()
    return await asyncio.wait_for(
        loop.run_in_executor(None, query_engine.query, query),
        timeout=timeout
    )


def _extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    json_match = re.search(r'\{[\s\S]*\}', text)
    if not json_match:
        return None
    try:
        return json.loads(json_match.group(0))
    except json.JSONDecodeError:
        return None


def _extract_llm_content(raw_response: Any) -> str:
    if isinstance(raw_response, str):
        return raw_response
    if hasattr(raw_response, 'message') and hasattr(raw_response.message, 'content'):
        return str(raw_response.message.content)
    if hasattr(raw_response, 'content'):
        return str(raw_response.content)
    return str(raw_response)


async def _classify_industry(
    llm: Any,
    company_name: str,
    year: str,
    overview_data: str
) -> Dict[str, Any]:
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªè¡Œä¸šè¯†åˆ«ä¸“å®¶ï¼Œéœ€è¦åŸºäºå¹´æŠ¥å†…å®¹è¯†åˆ«å…¬å¸æ‰€å±è¡Œä¸šã€‚
æ³¨æ„ï¼šç¦æ­¢æ ¹æ®å…¬å¸åç§°çŒœæµ‹ï¼Œåªèƒ½ä½¿ç”¨æä¾›çš„å¹´æŠ¥æ–‡æœ¬è¯æ®ã€‚

å¯é€‰è¡Œä¸šï¼ˆå¿…é¡»ä»ä¸­é€‰æ‹©ä¸€ä¸ªï¼‰ï¼š
banking, insurance, securities, manufacturing, internet_platform, service, general_corporate

è¾“å…¥æ•°æ®ï¼ˆæ¥è‡ªå¹´æŠ¥å…¬å¸æ¦‚å†µ/ä¸»è¥ä¸šåŠ¡/è¡Œä¸šåˆ†ç±»æŠ«éœ²ï¼‰ï¼š
{overview_data}

è¯·è¾“å‡ºJSONï¼š
{{
  "industry": "banking",
  "confidence": 0.92,
  "evidence": [
    "è¯æ®1",
    "è¯æ®2"
  ]
}}
"""

    response = await llm.achat([
        ChatMessage(role="system", content="ä½ æ˜¯è¡Œä¸šåˆ†ç±»å™¨ï¼Œå¿…é¡»ä¸¥æ ¼è¾“å‡ºJSONã€‚"),
        ChatMessage(role="user", content=prompt)
    ])
    content = _extract_llm_content(response)
    parsed = _extract_json_from_text(content) or {}
    if parsed.get("industry") not in BUSINESS_SCHEMA_TEMPLATES:
        parsed["industry"] = "general_corporate"
    try:
        validated = IndustryClassificationResult.model_validate(parsed)
        return validated.model_dump()
    except Exception:
        return {
            "industry": "general_corporate",
            "confidence": 0.5,
            "evidence": []
        }


async def _select_segments(
    llm: Any,
    industry: str,
    schema: Dict[str, Any],
    business_data: str
) -> Dict[str, Any]:
    prompt = f"""
ä½ æ˜¯ä¼ä¸šå¹´æŠ¥â€œä¸šåŠ¡ç»“æ„è¯†åˆ«â€æ¨¡å—ã€‚ä½ ä¼šå¾—åˆ°ï¼š
- è¡Œä¸šåˆ¤æ–­ industry
- è¯¥è¡Œä¸šçš„ä¸šåŠ¡æ¿å—æ¨¡æ¿ï¼ˆsegmentsï¼Œæ¯ä¸ªå« segment_idã€segment_nameã€business_scopeã€å…¸å‹äº§å“ï¼‰
- å¹´æŠ¥ä¸šåŠ¡æè¿°æ–‡æœ¬ç‰‡æ®µ

ä»»åŠ¡ï¼šä»æ¨¡æ¿ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸šåŠ¡æ¿å—ï¼ˆselected_segmentsï¼‰ï¼Œç”¨äºåç»­â€œä¸šåŠ¡-è´¢åŠ¡-æˆ˜ç•¥è”åŠ¨â€åˆ†æã€‚

è¾“å‡ºå¿…é¡»æ˜¯JSONï¼Œä¸¥æ ¼åŒ¹é…ï¼š
{{
  "industry": "{industry}",
  "selected_segments": ["segment_id1","segment_id2"],
  "reasoning": ["...","..."],
  "evidence": ["...","..."]
}}

è§„åˆ™ï¼š
1) selected_segments åªèƒ½ä»æ¨¡æ¿æä¾›çš„ segment_id ä¸­é€‰
2) reasoning æ˜¯ä½ ä¸ºä»€ä¹ˆé€‰è¿™äº›æ¿å—ï¼ˆçŸ­å¥ï¼‰ï¼Œevidence å¿…é¡»å¼•ç”¨è¾“å…¥æ–‡æœ¬çš„çŸ­å¥
3) å¦‚æ— æ³•åŒ¹é…ï¼Œè¿”å› selected_segments=[] ä¸”è¯´æ˜åŸå› 

industry = {industry}
segments template =
{json.dumps(schema, ensure_ascii=False)}

annual report snippets =
<<<
{business_data}
>>>
"""

    response = await llm.achat([
        ChatMessage(role="system", content="ä½ æ˜¯ä¸šåŠ¡ç»“æ„è¯†åˆ«æ¨¡å—ï¼Œå¿…é¡»ä¸¥æ ¼è¾“å‡ºJSONã€‚"),
        ChatMessage(role="user", content=prompt)
    ])
    content = _extract_llm_content(response)
    parsed = _extract_json_from_text(content) or {}
    parsed["industry"] = industry
    try:
        validated = SegmentSelectionResult.model_validate(parsed)
        return validated.model_dump()
    except Exception:
        return {
            "industry": industry,
            "selected_segments": [],
            "reasoning": [],
            "evidence": []
        }


def _filter_schema_by_segments(schema: Dict[str, Any], selected_segments: list) -> Dict[str, Any]:
    if not selected_segments:
        return schema
    filtered_segments = [
        seg for seg in schema.get("segments", [])
        if seg.get("segment_id") in selected_segments
    ]
    if not filtered_segments:
        return schema
    filtered_schema = dict(schema)
    filtered_schema["segments"] = filtered_segments
    return filtered_schema


async def _map_metrics_to_schema(
    llm: Any,
    schema: Dict[str, Any],
    business_data: str,
    overview_data: str,
    industry: str
) -> Dict[str, Any]:
    logger.info(
        f"ğŸ” [business_highlights] æŒ‡æ ‡æ˜ å°„è¾“å…¥æ¦‚è§ˆ: "
        f"business_data_len={len(business_data)}, overview_data_len={len(overview_data)}"
    )
    if not business_data:
        logger.warning("âš ï¸ [business_highlights] business_data ä¸ºç©ºï¼ŒæŒ‡æ ‡æ˜ å°„å¯èƒ½å¤±è´¥")
    if not overview_data:
        logger.warning("âš ï¸ [business_highlights] overview_data ä¸ºç©ºï¼ŒæŒ‡æ ‡æ˜ å°„å¯èƒ½å¤±è´¥")
    logger.info(
        "ğŸ§¾ [business_highlights] business_data_snippet: "
        + (business_data[:800].replace("\n", " ") if business_data else "<empty>")
    )
    logger.info(
        "ğŸ§¾ [business_highlights] overview_data_snippet: "
        + (overview_data[:800].replace("\n", " ") if overview_data else "<empty>")
    )

    segment_rules = {}
    for segment in schema.get("segments", []):
        segment_id = segment.get("segment_id")
        if not segment_id:
            continue
        rule = _get_metric_rule(industry, segment_id)
        if rule:
            segment_rules[segment_id] = rule

    prompt = f"""
ä½ æ˜¯å¹´æŠ¥æŒ‡æ ‡æ˜ å°„åŠ©æ‰‹ï¼Œéœ€è¦æŠŠå¹´æŠ¥ä¸­çš„ä¸šåŠ¡æ•°æ®æ˜ å°„åˆ°æŒ‡å®šä¸šåŠ¡æ¨¡æ¿ã€‚

ä¸šåŠ¡æ¨¡æ¿ï¼š
{json.dumps(schema, ensure_ascii=False)}

å¹´æŠ¥ä¸šåŠ¡ç›¸å…³æ–‡æœ¬ï¼ˆä¸»è¥ä¸šåŠ¡ã€åˆ†éƒ¨ä¿¡æ¯ã€ä¸šåŠ¡ç»“æ„ã€äº§å“æœåŠ¡ï¼‰ï¼š
{business_data}

å¹´æŠ¥å…¬å¸æ¦‚å†µè¡¥å……ï¼š
{overview_data}

ä¸šåŠ¡æŒ‡æ ‡æå–è§„åˆ™ï¼ˆå¿…é€‰ä¼˜å…ˆï¼Œå¿…é¡»è¦†ç›–ï¼›å¯é€‰å°½é‡è¡¥é½ï¼‰ï¼š
{json.dumps(segment_rules, ensure_ascii=False)}

è¯·è¾“å‡ºJSONï¼š
{{
  "segments": [
    {{
      "segment_id": "retail_banking",
      "segment_name": "é›¶å”®é“¶è¡Œä¸šåŠ¡",
      "mapped_metrics": {{
        "scale": [{{"metric": "é›¶å”®è¥ä¸šæ”¶å…¥", "current_year": "xxx", "previous_year": "xxx", "yoy_change": "xx%", "evidence": "..."}}],
        "profitability": [{{"metric": "é›¶å”®å‡€æ¯å·®", "current_year": "xxx", "previous_year": "xxx", "yoy_change": "xx%", "evidence": "..."}}],
        "risk": [{{"metric": "é›¶å”®ä¸è‰¯ç‡", "current_year": "xxx", "previous_year": "xxx", "yoy_change": "xx%", "evidence": "..."}}],
        "efficiency": [{{"metric": "å®¢æˆ·æ•°", "current_year": "xxx", "previous_year": "xxx", "yoy_change": "xx%", "evidence": "..."}}]
      }},
      "business_scope_evidence": ["è¯æ®1", "è¯æ®2"]
    }}
  ],
  "notes": "æ— æ³•åŒ¹é…çš„æŒ‡æ ‡æˆ–ç¼ºå¤±è¯´æ˜"
}}
"""

    response = await llm.achat([
        ChatMessage(role="system", content="ä½ æ˜¯æŒ‡æ ‡æ˜ å°„åŠ©æ‰‹ï¼Œå¿…é¡»ä¸¥æ ¼è¾“å‡ºJSONã€‚"),
        ChatMessage(role="user", content=prompt)
    ])
    content = _extract_llm_content(response)
    parsed = _extract_json_from_text(content)
    return parsed or {"segments": [], "notes": "æœªèƒ½è§£ææŒ‡æ ‡æ˜ å°„ç»“æœ"}


def _build_highlights_prompt(
    company_name: str,
    year: str,
    schema: Dict[str, Any],
    metrics_mapping: Dict[str, Any],
    strategy_data: str
) -> str:
    prev_year_label = str(int(year) - 1) if year.isdigit() else "ä¸Šå¹´"
    return f"""
ä½ æ˜¯èµ„æ·±ä¸šåŠ¡åˆ†æå¸ˆï¼Œéœ€è¦åŸºäºä¸šåŠ¡æ¨¡æ¿ä¸å¹´æŠ¥æ•°æ®è¾“å‡ºä¸šåŠ¡äº®ç‚¹ã€‚

ä¸šåŠ¡æ¨¡æ¿ï¼š
{json.dumps(schema, ensure_ascii=False)}

æŒ‡æ ‡æ˜ å°„ç»“æœï¼š
{json.dumps(metrics_mapping, ensure_ascii=False)}

æˆ˜ç•¥/å‘å±•è§„åˆ’ä¿¡æ¯ï¼š
{strategy_data}

è¯·è¾“å‡ºç»“æ„åŒ–ä¸šåŠ¡äº®ç‚¹ï¼ˆJSONï¼‰ï¼š
{{
  "highlights": [
    {{
      "business_type": "ä¸šåŠ¡ç±»å‹åç§°",
      "highlights": "ä¸šåŠ¡äº®ç‚¹è¯¦ç»†æè¿°",
      "achievements": ["æˆå°±1", "æˆå°±2"]
    }}
  ],
  "overall_summary": "ä¸šåŠ¡äº®ç‚¹æ€»ç»“æ–‡å­—",
  "key_metrics_summary": {
    "title": "å…³é”®ä¸šåŠ¡æŒ‡æ ‡æ±‡æ€»",
    "headers": ["ä¸šåŠ¡æ¿å—", "å…³é”®æŒ‡æ ‡", "{year}", "{prev_year_label}", "åŒæ¯”å˜åŠ¨"],
    "rows": [
      ["ä¸šåŠ¡æ¿å—A", "æŒ‡æ ‡åç§°", "å½“å‰å€¼", "ä¸Šå¹´å€¼", "åŒæ¯”"],
      ["ä¸šåŠ¡æ¿å—B", "æŒ‡æ ‡åç§°", "å½“å‰å€¼", "ä¸Šå¹´å€¼", "åŒæ¯”"]
    ]
  }
}}

è¦æ±‚ï¼š
1. æ¯ä¸ªä¸šåŠ¡æ¿å—è¾“å‡º3-5æ¡äº®ç‚¹ï¼Œå¿…é¡»ç»“åˆæŒ‡æ ‡æ˜ å°„ç»“æœ
2. ä½“ç°ä¸šåŠ¡-è´¢åŠ¡-æˆ˜ç•¥è”åŠ¨ï¼ˆä¾‹å¦‚ï¼šä¸šåŠ¡å¢é•¿é©±åŠ¨â†’è´¢åŠ¡è¡¨ç°â†’æˆ˜ç•¥æ–¹å‘ï¼‰
3. ä¸è¦ç¼–é€ æœªæä¾›çš„æ•°æ®
4. å¿…é¡»è¾“å‡º key_metrics_summaryï¼Œæ— æ³•æå–çš„å€¼ç”¨"/"å ä½
5. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆJSONï¼Œä»…è¾“å‡ºJSON
"""


def _build_performance_prompt(
    company_name: str,
    year: str,
    industry: str,
    selected_schema: Dict[str, Any],
    extracted_metrics: list,
    strategy_data: str
) -> str:
    return f"""
ä½ æ˜¯â€œä¸šåŠ¡æ¿å—è´¢åŠ¡è¡¨ç°ä¸æˆ˜ç•¥è”åŠ¨â€è‡ªåŠ¨å†™ä½œä¸ç»“æ„åŒ–è¾“å‡ºæ¨¡å—ã€‚
è¯·åŸºäºè¾“å…¥æ•°æ®ï¼Œä¸ºæ¯ä¸ªä¸šåŠ¡æ¿å—ç”Ÿæˆç»“æ„åŒ–æ´å¯Ÿï¼Œå¹¶ç»™å‡ºç¬¬å››éƒ¨åˆ†æ€»è§ˆã€‚

è¾“å‡ºå¿…é¡»æ˜¯JSONï¼Œå¹¶ä¸¥æ ¼åŒ¹é…ä»¥ä¸‹ç»“æ„ï¼š
{{
  "company_name": "{company_name}",
  "fiscal_year": "{year}",
  "industry": "{industry}",
  "overall_summary": "...",
  "segment_insights": [
    {{
      "segment_id": "...",
      "headline": "...",
      "contribution": ["..."],
      "drivers": ["..."],
      "strategy_link": ["..."],
      "risks_and_watchlist": ["..."]
    }}
  ]
}}

å†™ä½œä¸æ¨ç†è§„åˆ™ï¼š
1) headline ä¸ºä¸€å¥è¯å®šæ€§ï¼ˆä¾‹ï¼šè½¬å‹é˜µç—›/å¢é•¿å¼•æ“/éæ¯æ”¯æŸ±/ç°é‡‘ç‰›æ‰¿å‹ç­‰ï¼‰ï¼Œä¸è¦è¶…è¿‡20å­—
2) contribution å¿…é¡»æ˜ç¡®â€œå¯¹å…¨å…¬å¸/å…¨è¡Œâ€çš„å½±å“ï¼ˆæ”¯æ’‘/æ‹–ç´¯ + å…·ä½“æŒ‡æ ‡ï¼‰
3) drivers ç”¨â€œå› æœé“¾â€è¡¨è¾¾ï¼Œä¼˜å…ˆä»ç»™å®šæ•°æ®ä¸­æ¨æ–­ï¼Œç¦æ­¢ç¼–é€ æ–°æ•°æ®
4) strategy_link å¿…é¡»æŠŠâ€œæˆ˜ç•¥åŠ¨ä½œâ€ä¸â€œè´¢åŠ¡ç»“æœâ€ä¸€ä¸€å¯¹åº”ï¼ˆå¯å¤šæ¡ï¼‰
5) risks_and_watchlist ç»™å‡ºé£é™©ç‚¹ + å¯è·Ÿè¸ªæŒ‡æ ‡ï¼ˆå°½é‡å¯é‡åŒ–ï¼‰

selected segment templates =
{json.dumps(selected_schema, ensure_ascii=False)}

extracted metrics by segment =
{json.dumps(extracted_metrics, ensure_ascii=False)}

strategy snippets =
<<<
{strategy_data}
>>>
"""


def _build_extracted_metrics(metrics_mapping: Dict[str, Any]) -> list:
    extracted_list = []
    for segment in metrics_mapping.get("segments", []):
        metrics: Dict[str, Any] = {}
        sources: Dict[str, list] = {}
        mapped_metrics = segment.get("mapped_metrics", {})
        for category, items in mapped_metrics.items():
            if not isinstance(items, list):
                continue
            for item in items:
                metric_name = item.get("metric")
                if not metric_name:
                    continue
                metrics[metric_name] = {
                    "current_year": item.get("current_year") or item.get("value"),
                    "previous_year": item.get("previous_year"),
                    "yoy_change": item.get("yoy_change"),
                    "category": category
                }
                evidence = item.get("evidence")
                if evidence:
                    sources[metric_name] = [evidence]
        try:
            extracted = ExtractedSegmentMetrics.model_validate({
                "segment_id": segment.get("segment_id"),
                "metrics": metrics,
                "sources": sources
            })
            extracted_list.append(extracted.model_dump())
        except Exception:
            extracted_list.append({
                "segment_id": segment.get("segment_id"),
                "metrics": metrics,
                "sources": sources
            })
    return extracted_list


def _build_segment_tables(
    metrics_mapping: Dict[str, Any],
    year: str,
    performance_report: Dict[str, Any],
    industry: str
) -> list:
    if year.isdigit():
        prev_year_label = str(int(year) - 1)
    else:
        prev_year_label = "ä¸Šå¹´"

    headers = ["æŒ‡æ ‡", year, prev_year_label, "åŒæ¯”å˜åŠ¨"]
    conclusion_by_segment = {}
    for insight in performance_report.get("segment_insights", []):
        segment_id = insight.get("segment_id")
        if segment_id:
            conclusion_by_segment[segment_id] = insight.get("headline") or insight.get("drivers", [])

    segment_tables = []
    for segment in metrics_mapping.get("segments", []):
        segment_id = segment.get("segment_id")
        segment_name = segment.get("segment_name", segment_id)
        rows = []
        mapped_metrics = segment.get("mapped_metrics", {})
        mapped_lookup = {}
        for category, items in mapped_metrics.items():
            if not isinstance(items, list):
                continue
            for item in items:
                metric_name = item.get("metric")
                if not metric_name:
                    continue
                mapped_lookup[_normalize_metric_name(metric_name)] = item

        rule = _get_metric_rule(industry, segment_id) or {}
        required_metrics = rule.get("required", [])
        optional_metrics = rule.get("optional", [])
        ordered_metrics = required_metrics + optional_metrics
        used_metrics = set()

        for metric in ordered_metrics:
            metric_name = metric.get("name")
            if not metric_name:
                continue
            item = mapped_lookup.get(_normalize_metric_name(metric_name))
            current_value = item.get("current_year") if item else "/"
            if not current_value and item:
                current_value = item.get("value") or "/"
            previous_value = item.get("previous_year") if item else "/"
            yoy_change = item.get("yoy_change") if item else "/"
            rows.append([metric_name, current_value or "/", previous_value or "/", yoy_change or "/"])
            used_metrics.add(_normalize_metric_name(metric_name))
        for category in ["scale", "profitability", "risk", "efficiency"]:
            items = mapped_metrics.get(category, [])
            if not isinstance(items, list):
                continue
            for item in items:
                metric_name = item.get("metric")
                if not metric_name:
                    continue
                if _normalize_metric_name(metric_name) in used_metrics:
                    continue
                current_value = item.get("current_year") or item.get("value") or "/"
                previous_value = item.get("previous_year") or "/"
                yoy_change = item.get("yoy_change") or "/"
                rows.append([metric_name, current_value, previous_value, yoy_change])

        conclusion = conclusion_by_segment.get(segment_id, "")
        if isinstance(conclusion, list):
            conclusion = "ï¼›".join(conclusion)
        if not conclusion:
            conclusion = "ä¸šåŠ¡ç»“è®ºå¾…è¡¥å……"

        table = {
            "title": f"{segment_name}æŒ‡æ ‡",
            "headers": headers,
            "rows": rows,
            "insight": conclusion
        }
        segment_tables.append({
            "segment_id": segment_id,
            "segment_name": segment_name,
            "table": table,
            "conclusion": conclusion
        })

    return segment_tables


def _build_key_metrics_summary(segment_tables: list, year: str) -> Dict[str, Any]:
    if year.isdigit():
        prev_year_label = str(int(year) - 1)
    else:
        prev_year_label = "ä¸Šå¹´"

    headers = ["ä¸šåŠ¡æ¿å—", "å…³é”®æŒ‡æ ‡", year, prev_year_label, "åŒæ¯”å˜åŠ¨"]
    rows = []
    for segment in segment_tables or []:
        segment_name = segment.get("segment_name") or segment.get("segment_id") or "ä¸šåŠ¡æ¿å—"
        table_rows = (segment.get("table") or {}).get("rows") or []
        picked = None
        for row in table_rows:
            if not row or len(row) < 2:
                continue
            metric_name = row[0]
            if metric_name and metric_name not in ("/", "-", "â€”"):
                picked = row
                break
        if picked:
            rows.append([
                segment_name,
                picked[0],
                picked[1] if len(picked) > 1 else "/",
                picked[2] if len(picked) > 2 else "/",
                picked[3] if len(picked) > 3 else "/"
            ])
        else:
            rows.append([segment_name, "æš‚æ— ", "/", "/", "/"])

    if not rows:
        rows = [["æš‚æ— ", "æš‚æ— ", "/", "/", "/"]]

    return {
        "title": "å…³é”®ä¸šåŠ¡æŒ‡æ ‡æ±‡æ€»",
        "headers": headers,
        "rows": rows
    }


async def generate_business_highlights(
    company_name: Annotated[str, "å…¬å¸åç§°"],
    year: Annotated[str, "å¹´ä»½"],
    query_engine: Any
) -> Dict[str, Any]:
    """
    ç”Ÿæˆä¸šåŠ¡äº®ç‚¹ç« èŠ‚
    
    åŒ…æ‹¬å„ä¸šåŠ¡æ¿å—çš„äº®ç‚¹å’Œæˆå°±
    
    Args:
        company_name: å…¬å¸åç§°
        year: å¹´ä»½
        query_engine: æŸ¥è¯¢å¼•æ“
    
    Returns:
        ä¸šåŠ¡äº®ç‚¹çš„ç»“æ„åŒ–æ•°æ®
    """
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆä¸šåŠ¡äº®ç‚¹: {company_name} {year}å¹´")
        start_time = time.time()

        def time_remaining() -> float:
            return MAX_TOTAL_SECONDS - (time.time() - start_time)
        
        # Step 1: è¡Œä¸šè¯†åˆ«ï¼ˆä¼˜å…ˆå…¬å¸åè§„åˆ™ï¼Œå‡å°‘æ£€ç´¢å¼€é”€ï¼‰
        inferred_industry = _infer_industry_from_company_name(company_name)
        llm = Settings.llm
        overview_data = ""
        if inferred_industry:
            industry_result = {
                "industry": inferred_industry,
                "confidence": 0.7,
                "evidence": ["company_name_rule"]
            }
            logger.info(f"ğŸ” [business_highlights] è¡Œä¸šè¯†åˆ«å‘½ä¸­è§„åˆ™: {inferred_industry}")
        else:
            overview_query = (
                f"{company_name} {year}å¹´ å…¬å¸æ¦‚å†µ ä¸»è¥ä¸šåŠ¡æè¿° è¡Œä¸šåˆ†ç±»æŠ«éœ² "
                "è¯ç›‘ä¼šè¡Œä¸š ä¸­ä¿¡è¡Œä¸š ä¸»è¥ä¸šåŠ¡èŒƒå›´"
            )
            if time_remaining() <= 0:
                raise TimeoutError("ä¸šåŠ¡äº®ç‚¹ç”Ÿæˆè¶…æ—¶ï¼Œæå‰ç»“æŸ")
            try:
                overview_data = await _run_query_with_timeout(
                    query_engine,
                    overview_query,
                    QUERY_TIMEOUT_SECONDS
                )
            except Exception as e:
                logger.warning(f"âš ï¸ ä¸šåŠ¡äº®ç‚¹-å…¬å¸æ¦‚å†µæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨ç©ºç™½: {e}")
                overview_data = ""
            industry_result = await _run_with_timeout(
                _classify_industry(llm, company_name, year, str(overview_data)),
                LLM_TIMEOUT_SECONDS,
                {"industry": "general_corporate", "confidence": 0.5, "evidence": []},
                "è¡Œä¸šè¯†åˆ«"
            )
        logger.info(f"âœ… ä¸šåŠ¡äº®ç‚¹è¡Œä¸šè¯†åˆ«: {industry_result.get('industry')}ï¼Œç½®ä¿¡åº¦: {industry_result.get('confidence')}")
        
        # Step 2: ä¸šåŠ¡æ‹†åˆ†æ¨¡æ¿é€‰æ‹©
        schema = get_business_schema(industry_result.get("industry", "general_corporate"))
        
        # Step 3: ä¸šåŠ¡æ¿å—æ•°æ®æŠ½å–ï¼ˆæŒ‡æ ‡æ˜ å°„ï¼‰
        business_query = (
            f"{company_name} {year}å¹´ åˆ†éƒ¨ä¿¡æ¯ ä¸šåŠ¡æ¿å— ä¸šåŠ¡ç»“æ„ ä¸šåŠ¡æ”¶å…¥ ä¸»è¦äº§å“ æœåŠ¡"
        )
        if time_remaining() <= 0:
            raise TimeoutError("ä¸šåŠ¡äº®ç‚¹ç”Ÿæˆè¶…æ—¶ï¼Œæå‰ç»“æŸ")
        try:
            business_data = await _run_query_with_timeout(
                query_engine,
                business_query,
                QUERY_TIMEOUT_SECONDS
            )
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸šåŠ¡äº®ç‚¹-ä¸šåŠ¡ç»“æ„æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨ç©ºç™½: {e}")
            business_data = ""
        # ä¸åšäºŒæ¬¡æ£€ç´¢ï¼Œé¿å…é¢å¤–è€—æ—¶

        segment_selection = await _run_with_timeout(
            _select_segments(
            llm,
            industry_result.get("industry", "general_corporate"),
            schema,
            str(business_data)
            ),
            LLM_TIMEOUT_SECONDS,
            {"industry": industry_result.get("industry", "general_corporate"), "selected_segments": [], "reasoning": [], "evidence": []},
            "ä¸šåŠ¡æ¿å—é€‰æ‹©"
        )
        if not segment_selection.get("selected_segments"):
            logger.warning("âš ï¸ ä¸šåŠ¡æ¿å—é€‰æ‹©ä¸ºç©ºï¼Œå›é€€åˆ°è¡Œä¸šæ¨¡æ¿å…¨é‡æ¿å—")

        selected_schema = _filter_schema_by_segments(
            schema,
            segment_selection.get("selected_segments", [])
        )

        metrics_mapping = await _run_with_timeout(
            _map_metrics_to_schema(
                llm,
                selected_schema,
                str(business_data),
                str(overview_data),
                industry_result.get("industry", "general_corporate")
            ),
            LLM_TIMEOUT_SECONDS,
            {"segments": [], "notes": "æŒ‡æ ‡æ˜ å°„è¶…æ—¶"},
            "æŒ‡æ ‡æ˜ å°„"
        )
        metrics_mapping = await _enrich_metrics_with_rules(
            metrics_mapping,
            industry_result.get("industry", "general_corporate"),
            company_name,
            year,
            query_engine,
            time_remaining
        )
        if not metrics_mapping.get("segments"):
            # æ²¡æœ‰æŠ½å–åˆ°æŒ‡æ ‡æ—¶ï¼Œè‡³å°‘ä¿ç•™ä¸šåŠ¡æ¿å—ï¼Œä¾¿äºç”Ÿæˆå ä½è¡¨æ ¼
            fallback_segments = []
            for segment in selected_schema.get("segments", []):
                segment_id = segment.get("segment_id")
                segment_name = segment.get("segment_name", segment_id)
                if not segment_id:
                    continue
                fallback_segments.append({
                    "segment_id": segment_id,
                    "segment_name": segment_name,
                    "mapped_metrics": {}
                })
            metrics_mapping["segments"] = fallback_segments
            metrics_mapping.setdefault("notes", "æŒ‡æ ‡æŠ½å–ä¸ºç©ºï¼Œå·²ä½¿ç”¨æ¨¡æ¿æ¿å—ç”Ÿæˆå ä½è¡¨æ ¼")
        extracted_metrics = _build_extracted_metrics(metrics_mapping)
        
        # Step 4: ä¸šåŠ¡-è´¢åŠ¡-æˆ˜ç•¥è”åŠ¨åˆ†æ
        strategy_query = f"{company_name} {year}å¹´ å‘å±•æˆ˜ç•¥ ç»è¥è®¡åˆ’ æˆ˜ç•¥è§„åˆ’ ç«äº‰ä¼˜åŠ¿"
        if time_remaining() <= 0:
            raise TimeoutError("ä¸šåŠ¡äº®ç‚¹ç”Ÿæˆè¶…æ—¶ï¼Œæå‰ç»“æŸ")
        try:
            strategy_data = await _run_query_with_timeout(
                query_engine,
                strategy_query,
                QUERY_TIMEOUT_SECONDS
            )
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸šåŠ¡äº®ç‚¹-æˆ˜ç•¥æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨ç©ºç™½: {e}")
            strategy_data = ""
        prompt = _build_highlights_prompt(
            company_name,
            year,
            selected_schema,
            metrics_mapping,
            str(strategy_data)
        )

        # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º - æ·»åŠ å¼‚å¸¸å¤„ç†å’Œæ€§èƒ½ç›‘æ§
        response = None
        structured_llm_start = time.time()
        try:
            sllm = llm.as_structured_llm(BusinessHighlights)
            raw_response = await _run_with_timeout(
                sllm.achat([
                    ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸šåŠ¡åˆ†æå¸ˆ,æ“…é•¿æ€»ç»“ä¸šåŠ¡äº®ç‚¹ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºï¼Œåªè¾“å‡ºJSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"),
                    ChatMessage(role="user", content=prompt)
                ]),
                LLM_TIMEOUT_SECONDS,
                {},
                "ä¸šåŠ¡äº®ç‚¹ç”Ÿæˆ"
            )
            
            # æ£€æŸ¥å“åº”ç±»å‹ - å¤„ç†å­—ç¬¦ä¸²å“åº”
            if isinstance(raw_response, str):
                logger.warning(f"âš ï¸ [generate_business_highlights] ç»“æ„åŒ–LLMè¿”å›å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æJSON")
                import json
                import re
                json_match = re.search(r'\{[\s\S]*\}', raw_response)
                if json_match:
                    parsed_data = json.loads(json_match.group(0))
                    if 'business_highlights' in parsed_data:
                        parsed_data = parsed_data['business_highlights']
                    response = BusinessHighlights(**parsed_data) if isinstance(parsed_data, dict) and 'highlights' in parsed_data else parsed_data
                else:
                    raise ValueError("æ— æ³•ä»å­—ç¬¦ä¸²å“åº”æå–JSON")
            elif isinstance(raw_response, BusinessHighlights):
                response = raw_response
            elif hasattr(raw_response, 'message') and hasattr(raw_response.message, 'content'):
                # å¤„ç†Responseå¯¹è±¡ï¼Œmessage.contentå¯èƒ½æ˜¯å­—ç¬¦ä¸²
                content = raw_response.message.content
                if isinstance(content, str):
                    logger.warning(f"âš ï¸ [generate_business_highlights] å“åº”message.contentæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æJSON")
                    import json
                    import re
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        parsed_data = json.loads(json_match.group(0))
                        if 'business_highlights' in parsed_data:
                            parsed_data = parsed_data['business_highlights']
                        response = BusinessHighlights(**parsed_data) if isinstance(parsed_data, dict) and 'highlights' in parsed_data else parsed_data
                    else:
                        raise ValueError("æ— æ³•ä»message.contentæå–JSON")
                else:
                    response = content
            else:
                response = raw_response
            
            structured_llm_time = time.time() - structured_llm_start
            logger.info(f"âœ… [generate_business_highlights] ç»“æ„åŒ–è¾“å‡ºæˆåŠŸï¼Œè€—æ—¶: {structured_llm_time:.2f}ç§’")
        except (AttributeError, ValueError, TypeError) as structured_error:
            error_type = type(structured_error).__name__
            error_msg = str(structured_error)
            structured_llm_time = time.time() - structured_llm_start
            
            # æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            if "model_dump_json" in error_msg or "AttributeError" in error_type:
                logger.warning(f"âš ï¸ [generate_business_highlights] ç»“æ„åŒ–LLMè¿”å›äº†å­—ç¬¦ä¸²è€ŒéPydanticæ¨¡å‹ï¼ˆè€—æ—¶: {structured_llm_time:.2f}ç§’ï¼‰")
                logger.warning(f"[generate_business_highlights] é”™è¯¯ç±»å‹: {error_type}, é”™è¯¯ä¿¡æ¯: {error_msg}")
                logger.info(f"[generate_business_highlights] è¿™æ˜¯LlamaIndexçš„å·²çŸ¥é—®é¢˜ï¼Œå°†å°è¯•ä»å­—ç¬¦ä¸²è§£æJSON")
            else:
                logger.warning(f"âš ï¸ [generate_business_highlights] ç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼ˆ{error_type}ï¼Œè€—æ—¶: {structured_llm_time:.2f}ç§’ï¼‰: {error_msg}")
            
            logger.info(f"[generate_business_highlights] å°è¯•ä½¿ç”¨æ™®é€šLLMè¾“å‡ºå¹¶æ‰‹åŠ¨è§£æJSON")
            # å›é€€åˆ°æ™®é€šLLMè¾“å‡º
            try:
                normal_response = await _run_with_timeout(
                    llm.achat([
                        ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸šåŠ¡åˆ†æå¸ˆ,æ“…é•¿æ€»ç»“ä¸šåŠ¡äº®ç‚¹ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºï¼Œåªè¾“å‡ºJSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"),
                        ChatMessage(role="user", content=prompt)
                    ]),
                    LLM_TIMEOUT_SECONDS,
                    "",
                    "ä¸šåŠ¡äº®ç‚¹å›é€€ç”Ÿæˆ"
                )
                
                # æå–å¹¶è§£æJSON
                if hasattr(normal_response, 'message'):
                    content = normal_response.message.content if hasattr(normal_response.message, 'content') else str(normal_response.message)
                else:
                    content = str(normal_response)
                
                import json
                import re
                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    json_str = json_match.group(0)
                    parsed_data = json.loads(json_str)
                    
                    # å¤„ç†åµŒå¥—ç»“æ„
                    if 'business_highlights' in parsed_data:
                        parsed_data = parsed_data['business_highlights']
                    elif len(parsed_data) == 1:
                        parsed_data = list(parsed_data.values())[0]
                    
                    try:
                        response = BusinessHighlights(**parsed_data)
                        logger.info(f"âœ… æ‰‹åŠ¨è§£æJSONæˆåŠŸ")
                    except Exception as validation_error:
                        logger.warning(f"âš ï¸ JSONéªŒè¯å¤±è´¥ï¼Œè¿”å›éƒ¨åˆ†æ•°æ®: {str(validation_error)}")
                        response = parsed_data if isinstance(parsed_data, dict) else {"content": content}
                else:
                    raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–JSON")
            except Exception as fallback_error:
                logger.error(f"âŒ å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(fallback_error)}")
                # è¿”å›é”™è¯¯ä¿¡æ¯ï¼Œä½†ä¸ä¸­æ–­æµç¨‹
                response = {
                    "error": f"ç”Ÿæˆå¤±è´¥: {str(fallback_error)}",
                    "content": content if 'content' in locals() else str(fallback_error)
                }

        logger.info(f"âœ… ä¸šåŠ¡äº®ç‚¹ç”ŸæˆæˆåŠŸ")

        # Step 5: ä¸šåŠ¡-è´¢åŠ¡-æˆ˜ç•¥è”åŠ¨ï¼ˆå¯¹é½ BusinessPerformanceReportï¼‰
        performance_report = {
            "company_name": company_name,
            "fiscal_year": year,
            "industry": industry_result.get("industry", "general_corporate"),
            "overall_summary": "",
            "segment_insights": []
        }
        if time_remaining() > 20:
            performance_prompt = _build_performance_prompt(
                company_name,
                year,
                industry_result.get("industry", "general_corporate"),
                selected_schema,
                extracted_metrics,
                str(strategy_data)
            )
            performance_response = await _run_with_timeout(
                llm.achat([
                    ChatMessage(role="system", content="ä½ æ˜¯ä¸šåŠ¡-è´¢åŠ¡-æˆ˜ç•¥è”åŠ¨åˆ†æä¸“å®¶ï¼Œå¿…é¡»ä¸¥æ ¼è¾“å‡ºJSONã€‚"),
                    ChatMessage(role="user", content=performance_prompt)
                ]),
                LLM_TIMEOUT_SECONDS,
                "",
                "ä¸šåŠ¡-è´¢åŠ¡-æˆ˜ç•¥è”åŠ¨"
            )
            performance_content = _extract_llm_content(performance_response)
            performance_parsed = _extract_json_from_text(performance_content) or {}
            try:
                performance_report = BusinessPerformanceReport.model_validate(performance_parsed).model_dump()
            except Exception:
                performance_report = performance_parsed or performance_report

        segment_tables = _build_segment_tables(
            metrics_mapping,
            year,
            performance_report if isinstance(performance_report, dict) else {},
            industry_result.get("industry", "general_corporate")
        )
        key_metrics_summary = _build_key_metrics_summary(segment_tables, year)
        
        # å¤„ç†å“åº” - ç¡®ä¿è¿”å›å­—å…¸æ ¼å¼
        result_dict = None
        
        # å¦‚æœresponseæ˜¯å­—å…¸ä¸”åŒ…å«errorï¼Œç›´æ¥è¿”å›
        if isinstance(response, dict) and 'error' in response:
            result_dict = response
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯Pydanticæ¨¡å‹
        elif isinstance(response, BusinessHighlights):
            result_dict = response.model_dump()
        elif hasattr(response, 'raw'):
            raw_data = response.raw
            if hasattr(raw_data, 'model_dump'):
                try:
                    result_dict = raw_data.model_dump()
                except Exception as e:
                    logger.warning(f"model_dump() å¤±è´¥: {e}")
            elif isinstance(raw_data, dict):
                result_dict = raw_data
            elif isinstance(raw_data, str):
                import json
                try:
                    result_dict = json.loads(raw_data)
                except json.JSONDecodeError:
                    result_dict = {"content": raw_data}
            else:
                result_dict = {"content": str(raw_data)}
        
        if result_dict is None:
            if hasattr(response, 'model_dump'):
                try:
                    result_dict = response.model_dump()
                except Exception:
                    pass
            elif isinstance(response, dict):
                result_dict = response
            else:
                result_dict = {"content": str(response)}
        
        if not isinstance(result_dict, dict):
            result_dict = {"content": str(result_dict)}
        
        extra_payload = {
            "company_name": company_name,
            "year": year,
            "industry": industry_result.get("industry"),
            "industry_confidence": industry_result.get("confidence"),
            "industry_evidence": industry_result.get("evidence"),
            "selected_segments": segment_selection.get("selected_segments", []),
            "segment_selection_evidence": segment_selection.get("evidence", []),
            "extracted_segment_metrics": extracted_metrics,
            "business_performance_report": performance_report,
            "metrics_mapping_notes": metrics_mapping.get("notes"),
            "segment_tables": segment_tables,
            "key_metrics_summary": key_metrics_summary
        }

        # æ•°æ®éªŒè¯å’Œæ¸…ç†ï¼ˆä»…é’ˆå¯¹ä¸šåŠ¡äº®ç‚¹ç»“æ„ï¼‰
        result_dict = _validate_and_clean_data(result_dict, BusinessHighlights)
        result_dict.update(extra_payload)
        
        return result_dict
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆä¸šåŠ¡äº®ç‚¹å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "error": f"ç”Ÿæˆä¸šåŠ¡äº®ç‚¹å¤±è´¥: {str(e)}",
            "company_name": company_name,
            "year": year,
            "segment_tables": [],
            "key_metrics_summary": _build_key_metrics_summary([], year)
        }

