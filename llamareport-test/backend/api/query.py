"""
æŸ¥è¯¢APIæ¥å£
"""

from typing import Dict, Any, Optional, List
from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import logging
from decimal import Decimal
from datetime import datetime

from core.rag_engine import RAGEngine
from agents.visualization_agent import VisualizationAgent
from models.report_models import FinancialSnapshot, KeyFinancialMetric
from agents.report_common import retrieve_financial_data
from agents.dupont_tools import parse_financial_data_response, extract_financial_data_for_dupont, generate_dupont_analysis
import re
from typing import Dict, Any, Optional, List
from decimal import Decimal

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/query", tags=["query"])

# å…¨å±€RAGå¼•æ“å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
rag_engine = None

def get_rag_engine():
    """è·å–RAGå¼•æ“å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global rag_engine
    if rag_engine is None:
        rag_engine = RAGEngine()
    return rag_engine

class QueryRequest(BaseModel):
    question: str
    context_filter: Optional[Dict[str, Any]] = None
    enable_visualization: bool = True  # æ˜¯å¦å¯ç”¨å¯è§†åŒ–

class BatchQueryRequest(BaseModel):
    questions: List[str]
    context_filter: Optional[Dict[str, Any]] = None
    enable_visualization: bool = True  # æ˜¯å¦å¯ç”¨å¯è§†åŒ–

class SimilarContentRequest(BaseModel):
    query: str
    top_k: int = 5

@router.post("/ask")
async def ask_question(request: QueryRequest):
    """
    æé—®æ¥å£ï¼ˆæ”¯æŒå¯è§†åŒ–ï¼‰

    Args:
        request: æŸ¥è¯¢è¯·æ±‚

    Returns:
        æŸ¥è¯¢ç»“æœï¼ˆå¯èƒ½åŒ…å«å¯è§†åŒ–é…ç½®ï¼‰
    """
    try:
        question = request.question.strip()
        if not question:
            raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")

        if len(question) > 1000:
            raise HTTPException(status_code=400, detail="é—®é¢˜è¿‡é•¿ï¼Œè¯·æ§åˆ¶åœ¨1000å­—ç¬¦ä»¥å†…")

        logger.info(f"æ”¶åˆ°æŸ¥è¯¢: {question[:50]}...")

        # è·å–RAGå¼•æ“å¹¶æ‰§è¡ŒæŸ¥è¯¢
        rag_engine = get_rag_engine()
        result = rag_engine.query(question, request.context_filter)

        if result.get('error'):
            # ç»Ÿä¸€é”™è¯¯å“åº”æ ¼å¼
            return JSONResponse(
                status_code=500,
                content={
                    "error": result.get('answer', 'æŸ¥è¯¢å¤±è´¥'),
                    "question": question,
                    "answer": result.get('answer', 'æŸ¥è¯¢å¤±è´¥'),
                    "sources": [],
                    "visualization": None
                }
            )

        # åŸºç¡€å“åº” - ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
        response = {
            "question": question,
            "answer": result.get('answer', ''),
            "sources": result.get('sources', []),
            "context_filter": request.context_filter,
            "enhanced_query": result.get('enhanced_query', question),
            "visualization": None  # åˆå§‹åŒ–ä¸ºNoneï¼Œåç»­å¯èƒ½è¢«å¡«å……
        }

        # å¦‚æœå¯ç”¨å¯è§†åŒ–ï¼Œå°è¯•ç”Ÿæˆå›¾è¡¨
        if request.enable_visualization:
            try:
                viz_agent = VisualizationAgent()
                
                # è®°å½•æŸ¥è¯¢å’Œå›ç­”ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•
                logger.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆå¯è§†åŒ– - æŸ¥è¯¢: {question[:100]}...")
                logger.info(f"ğŸ“Š å›ç­”é•¿åº¦: {len(result.get('answer', ''))} å­—ç¬¦")
                logger.info(f"ğŸ“Š æ¥æºæ•°é‡: {len(result.get('sources', []))}")
                
                viz_result = await viz_agent.generate_visualization(
                    query=question,
                    answer=result['answer'],
                    sources=result.get('sources', [])
                )

                # æ·»åŠ å¯è§†åŒ–æ•°æ®åˆ°å“åº”
                response['visualization'] = viz_result.model_dump()
                logger.info(f"âœ… å¯è§†åŒ–ç”ŸæˆæˆåŠŸ: {viz_result.has_visualization}")
                
                # å¦‚æœå¯è§†åŒ–ç”Ÿæˆå¤±è´¥ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                if not viz_result.has_visualization:
                    logger.warning(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ - æŸ¥è¯¢: {question}")
                    logger.warning(f"âš ï¸ å›ç­”é¢„è§ˆ: {result.get('answer', '')[:200]}...")
                    logger.warning(f"âš ï¸ æ¥æºé¢„è§ˆ: {[s.get('text', '')[:100] for s in result.get('sources', [])[:3]]}")

            except Exception as viz_error:
                logger.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {str(viz_error)}")
                import traceback
                logger.warning(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                response['visualization'] = {
                    "has_visualization": False,
                    "error": str(viz_error)
                }

        logger.info(f"æŸ¥è¯¢å®Œæˆ: {question[:50]}...")
        
        # ç¡®ä¿å“åº”æ ¼å¼ç»Ÿä¸€
        response_data = {
            "question": response.get("question", question),
            "answer": response.get("answer", ""),
            "sources": response.get("sources", []),
            "context_filter": response.get("context_filter"),
            "enhanced_query": response.get("enhanced_query", question),
            "visualization": response.get("visualization")
        }
        
        return JSONResponse(status_code=200, content=response_data)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

@router.post("/batch")
async def batch_query(request: BatchQueryRequest):
    """
    æ‰¹é‡æŸ¥è¯¢æ¥å£
    
    Args:
        request: æ‰¹é‡æŸ¥è¯¢è¯·æ±‚
        
    Returns:
        æ‰¹é‡æŸ¥è¯¢ç»“æœ
    """
    try:
        questions = request.questions
        if not questions:
            raise HTTPException(status_code=400, detail="é—®é¢˜åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(questions) > 10:
            raise HTTPException(status_code=400, detail="ä¸€æ¬¡æœ€å¤šæŸ¥è¯¢10ä¸ªé—®é¢˜")
        
        # éªŒè¯æ¯ä¸ªé—®é¢˜
        for i, question in enumerate(questions):
            if not question or not question.strip():
                raise HTTPException(status_code=400, detail=f"ç¬¬{i+1}ä¸ªé—®é¢˜ä¸èƒ½ä¸ºç©º")
            if len(question) > 1000:
                raise HTTPException(status_code=400, detail=f"ç¬¬{i+1}ä¸ªé—®é¢˜è¿‡é•¿")
        
        logger.info(f"æ”¶åˆ°æ‰¹é‡æŸ¥è¯¢: {len(questions)} ä¸ªé—®é¢˜")
        
        results = []
        for i, question in enumerate(questions):
            try:
                question = question.strip()
                result = rag_engine.query(question, request.context_filter)
                
                if result.get('error'):
                    results.append({
                        "question_index": i,
                        "question": question,
                        "status": "error",
                        "error": result.get('answer', 'æŸ¥è¯¢å¤±è´¥')
                    })
                else:
                    results.append({
                        "question_index": i,
                        "question": question,
                        "status": "success",
                        "answer": result['answer'],
                        "sources": result.get('sources', []),
                        "enhanced_query": result.get('enhanced_query', question)
                    })
                
            except Exception as e:
                results.append({
                    "question_index": i,
                    "question": question,
                    "status": "error",
                    "error": str(e)
                })
                logger.error(f"æ‰¹é‡æŸ¥è¯¢ä¸­ç¬¬{i+1}ä¸ªé—®é¢˜å¤±è´¥: {str(e)}")
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r["status"] == "success")
        error_count = len(results) - success_count
        
        response = {
            "total_questions": len(questions),
            "success_count": success_count,
            "error_count": error_count,
            "context_filter": request.context_filter,
            "results": results
        }
        
        logger.info(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆ: {success_count}/{len(questions)} æˆåŠŸ")
        return JSONResponse(status_code=200, content=response)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æŸ¥è¯¢å¤±è´¥: {str(e)}")

@router.post("/similar")
async def get_similar_content(request: SimilarContentRequest):
    """
    è·å–ç›¸ä¼¼å†…å®¹
    
    Args:
        request: ç›¸ä¼¼å†…å®¹è¯·æ±‚
        
    Returns:
        ç›¸ä¼¼å†…å®¹åˆ—è¡¨
    """
    try:
        query = request.query.strip()
        if not query:
            raise HTTPException(status_code=400, detail="æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        if request.top_k < 1 or request.top_k > 20:
            raise HTTPException(status_code=400, detail="top_kå¿…é¡»åœ¨1-20ä¹‹é—´")
        
        logger.info(f"è·å–ç›¸ä¼¼å†…å®¹: {query[:50]}...")
        
        # è·å–ç›¸ä¼¼å†…å®¹
        similar_content = rag_engine.get_similar_content(query, request.top_k)
        
        response = {
            "query": query,
            "top_k": request.top_k,
            "total_results": len(similar_content),
            "similar_content": similar_content
        }
        
        logger.info(f"ç›¸ä¼¼å†…å®¹æŸ¥è¯¢å®Œæˆ: æ‰¾åˆ° {len(similar_content)} ä¸ªç»“æœ")
        return JSONResponse(status_code=200, content=response)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç›¸ä¼¼å†…å®¹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç›¸ä¼¼å†…å®¹å¤±è´¥: {str(e)}")

@router.get("/suggestions")
async def get_query_suggestions():
    """
    è·å–æŸ¥è¯¢å»ºè®®
    
    Returns:
        æŸ¥è¯¢å»ºè®®åˆ—è¡¨
    """
    try:
        suggestions = [
            {
                "category": "è´¢åŠ¡æ•°æ®",
                "questions": [
                    "å…¬å¸çš„è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ",
                    "å‡€åˆ©æ¶¦å¢é•¿ç‡å¦‚ä½•ï¼Ÿ",
                    "èµ„äº§è´Ÿå€ºç‡æ˜¯å¤šå°‘ï¼Ÿ",
                    "èµ„äº§æ€»é¢æ˜¯å¤šå°‘ï¼Ÿ"
                ]
            },
            {
                "category": "ä¸šåŠ¡åˆ†æ",
                "questions": [
                    "ä¸»è¦ä¸šåŠ¡æ¿å—æœ‰å“ªäº›ï¼Ÿ",
                    "å¸‚åœºä»½é¢å¦‚ä½•ï¼Ÿ",
                    "ç«äº‰ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "é£é™©å› ç´ æœ‰å“ªäº›ï¼Ÿ"
                ]
            },
            {
                "category": "å‘å±•è¶‹åŠ¿",
                "questions": [
                    "æœªæ¥å‘å±•æˆ˜ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "æŠ•èµ„è®¡åˆ’æœ‰å“ªäº›ï¼Ÿ",
                    "é¢„æœŸå¢é•¿ç‡å¦‚ä½•ï¼Ÿ",
                    "è¡Œä¸šå‰æ™¯å¦‚ä½•ï¼Ÿ"
                ]
            }
        ]
        
        return JSONResponse(status_code=200, content={
            "message": "æŸ¥è¯¢å»ºè®®",
            "suggestions": suggestions
        })
        
    except Exception as e:
        logger.error(f"è·å–æŸ¥è¯¢å»ºè®®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æŸ¥è¯¢å»ºè®®å¤±è´¥: {str(e)}")

@router.get("/history")
async def get_query_history():
    """
    è·å–æŸ¥è¯¢å†å²ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä»æ•°æ®åº“è·å–ï¼‰
    
    Returns:
        æŸ¥è¯¢å†å²
    """
    try:
        # è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä»æ•°æ®åº“æˆ–ç¼“å­˜ä¸­è·å–
        history = {
            "message": "æŸ¥è¯¢å†å²åŠŸèƒ½æš‚æœªå®ç°",
            "note": "åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥è¿”å›ç”¨æˆ·çš„æŸ¥è¯¢å†å²è®°å½•",
            "recent_queries": []
        }
        
        return JSONResponse(status_code=200, content=history)
        
    except Exception as e:
        logger.error(f"è·å–æŸ¥è¯¢å†å²å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æŸ¥è¯¢å†å²å¤±è´¥: {str(e)}")

@router.get("/stats")
async def get_query_stats():
    """
    è·å–æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        æŸ¥è¯¢ç»Ÿè®¡
    """
    try:
        # è·å–ç´¢å¼•ç»Ÿè®¡
        rag_engine = get_rag_engine()
        index_stats = rag_engine.get_index_stats()
        
        stats = {
            "index_status": index_stats,
            "query_capabilities": {
                "max_question_length": 1000,
                "max_batch_size": 10,
                "max_similar_results": 20,
                "supported_filters": ["company", "year", "document_type"]
            },
            "performance_info": {
                "average_response_time": "1-3ç§’",
                "supported_languages": ["ä¸­æ–‡", "è‹±æ–‡"],
                "context_window": "4000 tokens"
            }
        }
        
        return JSONResponse(status_code=200, content=stats)
        
    except Exception as e:
        logger.error(f"è·å–æŸ¥è¯¢ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æŸ¥è¯¢ç»Ÿè®¡å¤±è´¥: {str(e)}")

@router.get("/index-documents")
async def list_index_documents():
    """
    åˆ—å‡ºç´¢å¼•ä¸­çš„æ‰€æœ‰æ–‡æ¡£åŠå…¶å…ƒæ•°æ®
    
    Returns:
        ç´¢å¼•ä¸­çš„æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨
    """
    try:
        rag_engine = get_rag_engine()
        
        if not rag_engine.query_engine:
            if not rag_engine.load_existing_index():
                raise HTTPException(
                    status_code=400,
                    detail="ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
                )
        
        if not rag_engine.index:
            return JSONResponse(status_code=200, content={
                "message": "ç´¢å¼•æœªåˆå§‹åŒ–",
                "documents": [],
                "files": []
            })
        
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = rag_engine.index.docstore.docs
        
        # ç»Ÿè®¡æ–‡ä»¶
        files_dict = {}
        documents_list = []
        
        for doc_id, doc in all_docs.items():
            metadata = doc.metadata or {}
            filename = metadata.get('filename') or metadata.get('source_file', 'unknown')
            doc_type = metadata.get('document_type', 'text')
            
            # æ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨
            documents_list.append({
                "doc_id": doc_id,
                "filename": filename,
                "document_type": doc_type,
                "page_number": metadata.get('page_number'),
                "table_id": metadata.get('table_id'),
                "text_preview": doc.text[:200] if doc.text else "",
                "text_length": len(doc.text) if doc.text else 0,
                "metadata": metadata
            })
            
            # ç»Ÿè®¡æ–‡ä»¶
            if filename not in files_dict:
                files_dict[filename] = {
                    'count': 0,
                    'types': set(),
                    'sample_text': doc.text[:100] if doc.text else ''
                }
            files_dict[filename]['count'] += 1
            files_dict[filename]['types'].add(doc_type)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        files_list = []
        for filename, info in sorted(files_dict.items()):
            files_list.append({
                "filename": filename,
                "document_count": info['count'],
                "document_types": list(info['types']),
                "sample_text": info['sample_text']
            })
        
        # æ£€æŸ¥ChromaDBé›†åˆ
        chroma_info = {}
        if rag_engine.chroma_collection:
            try:
                all_data = rag_engine.chroma_collection.get()
                if all_data and 'ids' in all_data:
                    chroma_info = {
                        "vector_count": len(all_data['ids']),
                        "metadata_count": len(all_data.get('metadatas', []))
                    }
                    
                    # ç»Ÿè®¡ChromaDBä¸­çš„æ–‡ä»¶å
                    metadatas = all_data.get('metadatas', [])
                    chroma_files = {}
                    for metadata in metadatas:
                        if metadata:
                            filename = metadata.get('filename') or metadata.get('source_file', 'unknown')
                            if filename not in chroma_files:
                                chroma_files[filename] = 0
                            chroma_files[filename] += 1
                    
                    chroma_info["files"] = chroma_files
            except Exception as e:
                chroma_info = {"error": str(e)}
        
        return JSONResponse(status_code=200, content={
            "message": f"ç´¢å¼•ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£",
            "total_documents": len(all_docs),
            "total_files": len(files_dict),
            "files": files_list,
            "documents": documents_list[:100],  # é™åˆ¶è¿”å›å‰100ä¸ªæ–‡æ¡£ï¼Œé¿å…å“åº”è¿‡å¤§
            "chroma_info": chroma_info
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ—å‡ºç´¢å¼•æ–‡æ¡£å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"åˆ—å‡ºç´¢å¼•æ–‡æ¡£å¤±è´¥: {str(e)}")

@router.post("/feedback")
async def submit_feedback(feedback_data: Dict[str, Any]):
    """
    æäº¤æŸ¥è¯¢åé¦ˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        feedback_data: åé¦ˆæ•°æ®
        
    Returns:
        åé¦ˆæäº¤ç»“æœ
    """
    try:
        # è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä¿å­˜åˆ°æ•°æ®åº“
        logger.info(f"æ”¶åˆ°æŸ¥è¯¢åé¦ˆ: {feedback_data}")
        
        return JSONResponse(status_code=200, content={
            "message": "åé¦ˆæäº¤æˆåŠŸ",
            "note": "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼Œæˆ‘ä»¬ä¼šæŒç»­æ”¹è¿›æœåŠ¡è´¨é‡"
        })
        
    except Exception as e:
        logger.error(f"æäº¤åé¦ˆå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æäº¤åé¦ˆå¤±è´¥: {str(e)}")

class DupontAnalysisRequest(BaseModel):
    """æœé‚¦åˆ†æè¯·æ±‚"""
    company_name: Optional[str] = None  # å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»æ–‡æ¡£ä¸­æå–
    year: Optional[str] = None  # å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»æ–‡æ¡£ä¸­æå–
    filename: Optional[str] = None  # é€‰ä¸­çš„æ–‡ä»¶åï¼Œç”¨äºé™åˆ¶æŸ¥è¯¢èŒƒå›´

@router.post("/dupont-analysis")
async def generate_dupont_analysis_api(request: DupontAnalysisRequest):
    """
    ç”Ÿæˆæœé‚¦åˆ†ææŠ¥å‘Š
    
    è‡ªåŠ¨ä»æ–‡æ¡£ä¸­æå–è´¢åŠ¡æ•°æ®ï¼Œç”Ÿæˆå®Œæ•´çš„æœé‚¦åˆ†ææŠ¥å‘Š
    å¦‚æœæœªæä¾›å…¬å¸åç§°å’Œå¹´ä»½ï¼Œå°†å°è¯•ä»æ–‡æ¡£ä¸­è‡ªåŠ¨æå–
    
    Returns:
        æœé‚¦åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
        - level1: ROEé¡¶å±‚åˆ†è§£
        - level2: ROAå’Œæƒç›Šä¹˜æ•°åˆ†è§£
        - level3: åº•å±‚è´¢åŠ¡æ•°æ®
        - tree_structure: æ ‘çŠ¶ç»“æ„
        - insights: AIåˆ†ææ´å¯Ÿ
    """
    try:
        logger.info("æ”¶åˆ°æœé‚¦åˆ†æè¯·æ±‚")
        
        # è·å–RAGå¼•æ“
        rag_engine = get_rag_engine()
        
        if not rag_engine.query_engine:
            if not rag_engine.load_existing_index():
                raise HTTPException(
                    status_code=400,
                    detail="ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
                )
        
        query_engine = rag_engine.query_engine
        
        # æå–å…¬å¸åç§°å’Œå¹´ä»½
        company_name = request.company_name
        year = request.year
        filename = request.filename
        
        # æ„å»ºä¸Šä¸‹æ–‡è¿‡æ»¤å™¨ï¼Œé™åˆ¶æŸ¥è¯¢èŒƒå›´åˆ°é€‰ä¸­çš„æ–‡ä»¶
        context_filter = None
        if filename:
            context_filter = {"filename": filename}
            logger.info(f"é™åˆ¶æŸ¥è¯¢èŒƒå›´åˆ°æ–‡ä»¶: {filename}")
        
        # å¦‚æœæœªæä¾›ï¼Œå°è¯•ä»æ–‡æ¡£ä¸­æå–
        if not company_name or not year:
            logger.info(f"å°è¯•ä»æ–‡æ¡£ä¸­æå–å…¬å¸åç§°å’Œå¹´ä»½... (æ–‡ä»¶: {filename or 'å…¨éƒ¨'})")
            
            import re
            
            # ç¬¬ä¸€æ­¥ï¼šä¼˜å…ˆä»æ–‡ä»¶åä¸­æå–å…¬å¸åç§°å’Œå¹´ä»½
            if filename:
                # ä»æ–‡ä»¶åæå–å…¬å¸åç§°ï¼ˆæ›´æ™ºèƒ½çš„åŒ¹é…ï¼‰
                # åŒ¹é…æ¨¡å¼ï¼šå…¬å¸å + æŠ¥è¡¨ç±»å‹ + å¹´ä»½ï¼ˆå¯é€‰ï¼‰
                # ä¾‹å¦‚ï¼š"å¹³å®‰é“¶è¡Œåˆ©æ¶¦è¡¨.xlsx" -> "å¹³å®‰é“¶è¡Œ"
                # ä¾‹å¦‚ï¼š"å¹³å®‰é“¶è¡Œ2024å¹´å¹´æŠ¥.pdf" -> "å¹³å®‰é“¶è¡Œ"
                if not company_name:
                    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
                    name_without_ext = re.sub(r'\.[^.]+$', '', filename)
                    # ç§»é™¤å¸¸è§çš„æŠ¥è¡¨ç±»å‹å…³é”®è¯
                    name_clean = re.sub(r'(åˆ©æ¶¦è¡¨|èµ„äº§è´Ÿå€ºè¡¨|ç°é‡‘æµé‡è¡¨|å¹´æŠ¥|æŠ¥å‘Š|è´¢åŠ¡æŠ¥è¡¨|è´¢åŠ¡æŠ¥å‘Š)', '', name_without_ext, flags=re.IGNORECASE)
                    # ç§»é™¤å¹´ä»½
                    name_clean = re.sub(r'\d{4}å¹´?', '', name_clean)
                    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                    name_clean = re.sub(r'[_\-\s]+', '', name_clean).strip()
                    if name_clean and len(name_clean) >= 2:
                        company_name = name_clean
                        logger.info(f"ä»æ–‡ä»¶åæå–å…¬å¸åç§°: {company_name}")
                
                # ä»æ–‡ä»¶åæå–å¹´ä»½
                if not year:
                    year_match = re.search(r'(\d{4})', filename)
                    if year_match:
                        year = year_match.group(1)
                        logger.info(f"ä»æ–‡ä»¶åæå–å¹´ä»½: {year}")
            
            # ç¬¬äºŒæ­¥ï¼šå¦‚æœè¿˜æ²¡æœ‰ï¼Œä»æ–‡æ¡£å†…å®¹ä¸­æå–
            if not company_name or not year:
                try:
                    # ä»ç´¢å¼•ä¸­æ£€ç´¢è¯¥æ–‡ä»¶çš„æ–‡æ¡£
                    retriever = rag_engine.index.as_retriever(similarity_top_k=20)  # å¢åŠ æ£€ç´¢æ•°é‡
                    
                    # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ï¼Œå…ˆå°è¯•ä»è¯¥æ–‡ä»¶æå–ï¼›å¦‚æœæ²¡æœ‰ï¼Œä»æ‰€æœ‰æ–‡ä»¶æå–
                    if filename:
                        # å…ˆå°è¯•ä»æŒ‡å®šæ–‡ä»¶æå–
                        nodes = retriever.retrieve("å…¬å¸åç§° å¹´ä»½ æŠ¥å‘Šå¹´åº¦")
                        matching_nodes = [
                            node for node in nodes 
                            if node.metadata.get('filename') == filename or 
                               node.metadata.get('source_file') == filename
                        ]
                        # å¦‚æœæŒ‡å®šæ–‡ä»¶æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»æ‰€æœ‰æ–‡ä»¶æå–ï¼ˆå¯èƒ½æ˜¯å…¶ä»–ç›¸å…³æ–‡ä»¶ï¼‰
                        if not matching_nodes:
                            logger.info(f"æŒ‡å®šæ–‡ä»¶ {filename} ä¸­æœªæ‰¾åˆ°å…¬å¸ä¿¡æ¯ï¼Œå°è¯•ä»æ‰€æœ‰æ–‡ä»¶æå–...")
                            matching_nodes = nodes[:10]  # ä½¿ç”¨å‰10ä¸ªèŠ‚ç‚¹
                    else:
                        # ä»æ‰€æœ‰æ–‡ä»¶æå–
                        nodes = retriever.retrieve("å…¬å¸åç§° å¹´ä»½ æŠ¥å‘Šå¹´åº¦")
                        matching_nodes = nodes[:10]
                    
                    if matching_nodes:
                        # åˆå¹¶æ‰€æœ‰åŒ¹é…èŠ‚ç‚¹çš„æ–‡æœ¬
                        all_text = "\n".join([node.text for node in matching_nodes[:10]])  # å¢åŠ æ–‡æœ¬é‡
                        
                        if not company_name:
                            # ä»æ–‡æœ¬ä¸­æå–å…¬å¸åç§°ï¼ˆå¤šç§æ¨¡å¼ï¼‰
                            patterns = [
                                r'([^ï¼Œ,ã€‚\n]{2,30}(?:è‚¡ä»½|æœ‰é™|å…¬å¸|é›†å›¢|é“¶è¡Œ|è¯åˆ¸|ä¿é™©))',
                                r'å…¬å¸åç§°[ï¼š:]\s*([^ï¼Œ,ã€‚\n]{2,30})',
                                r'([A-Za-z0-9\u4e00-\u9fa5]{2,20}(?:è‚¡ä»½|æœ‰é™|å…¬å¸|é›†å›¢))',
                            ]
                            for pattern in patterns:
                                company_match = re.search(pattern, all_text)
                                if company_match:
                                    candidate = company_match.group(1).strip()
                                    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å…¬å¸åçš„å†…å®¹
                                    if len(candidate) >= 2 and len(candidate) <= 30:
                                        company_name = candidate
                                        logger.info(f"ä»æ–‡æ¡£å†…å®¹æå–å…¬å¸åç§°: {company_name}")
                                        break
                        
                        if not year:
                            # ä»æ–‡æœ¬ä¸­æå–å¹´ä»½ï¼ˆæ›´å…¨é¢çš„æ¨¡å¼ï¼‰
                            year_patterns = [
                                r'æŠ¥å‘Šå¹´åº¦[ï¼š:]\s*(\d{4})',
                                r'(\d{4})å¹´åº¦',
                                r'(\d{4})å¹´[åº¦]?æŠ¥å‘Š',
                                r'(\d{4})å¹´[åº¦]?',
                                r'æˆªè‡³(\d{4})å¹´',
                            ]
                            for pattern in year_patterns:
                                year_matches = re.findall(pattern, all_text)
                                if year_matches:
                                    # é€‰æ‹©æœ€å¸¸è§çš„å¹´ä»½ï¼ˆé€šå¸¸æ˜¯æŠ¥å‘Šå¹´ä»½ï¼‰
                                    from collections import Counter
                                    year_counts = Counter(year_matches)
                                    candidate_year = year_counts.most_common(1)[0][0]
                                    # éªŒè¯å¹´ä»½åˆç†æ€§
                                    if 2000 <= int(candidate_year) <= 2030:
                                        year = candidate_year
                                        logger.info(f"ä»æ–‡æ¡£å†…å®¹æå–å¹´ä»½: {year} (å‡ºç° {year_counts[candidate_year]} æ¬¡)")
                                        break
                        
                        logger.info(f"ä»æ–‡æ¡£å†…å®¹æå–: {company_name or 'æœªæ‰¾åˆ°'} - {year or 'æœªæ‰¾åˆ°'}")
                except Exception as e:
                    logger.warning(f"ä»æ–‡æ¡£å†…å®¹æå–å¤±è´¥: {str(e)}")
            
            # å¦‚æœä»ç„¶æ²¡æœ‰ï¼Œä½¿ç”¨query_engineæŸ¥è¯¢
            if not company_name or not year:
                extract_query = """
                è¯·ä»æ–‡æ¡£ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                1. å…¬å¸åç§°ï¼ˆå®Œæ•´çš„å…¬å¸å…¨ç§°ï¼‰
                2. æŠ¥å‘Šå¹´ä»½ï¼ˆå¦‚2023ã€2022ç­‰ï¼‰
                
                è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼ï¼š{"company_name": "å…¬å¸åç§°", "year": "å¹´ä»½"}
                """
                
                try:
                    # å¦‚æœæœ‰context_filterï¼Œä½¿ç”¨retrieveré™åˆ¶èŒƒå›´
                    if context_filter and filename:
                        retriever = rag_engine.index.as_retriever(similarity_top_k=10)
                        nodes = retriever.retrieve(extract_query)
                        # è¿‡æ»¤å‡ºåŒ¹é…çš„æ–‡ä»¶ï¼ˆæ£€æŸ¥filenameå’Œsource_fileï¼‰
                        matching_nodes = [
                            node for node in nodes 
                            if node.metadata.get('filename') == filename or 
                               node.metadata.get('source_file') == filename
                        ]
                        if matching_nodes:
                            response_text = "\n".join([node.text for node in matching_nodes[:3]])
                        else:
                            response_text = ""
                    else:
                        response = query_engine.query(extract_query)
                        response_text = str(response)
                    
                    # å°è¯•è§£æJSON
                    import json
                    import re
                    json_match = re.search(r'\{[^{}]*"company_name"[^{}]*"year"[^{}]*\}', response_text)
                    if json_match:
                        extracted_data = json.loads(json_match.group())
                        if not company_name:
                            company_name = extracted_data.get('company_name', '')
                        if not year:
                            year = extracted_data.get('year', '')
                    
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æ­£åˆ™æå–
                    if not company_name:
                        company_match = re.search(r'å…¬å¸åç§°[ï¼š:]\s*([^\nï¼Œ,ã€‚]+)', response_text)
                        if company_match:
                            company_name = company_match.group(1).strip()
                    
                    if not year:
                        year_match = re.search(r'(\d{4})å¹´', response_text)
                        if year_match:
                            year = year_match.group(1)
                        else:
                            # å°è¯•ä»æ–‡æ¡£å…ƒæ•°æ®ä¸­è·å–
                            year_match = re.search(r'(\d{4})', response_text)
                            if year_match:
                                year = year_match.group(1)
                
                except Exception as e:
                    logger.warning(f"ä»æ–‡æ¡£æå–å…¬å¸ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not company_name:
            company_name = "æœªçŸ¥å…¬å¸"
            logger.warning("æœªæ‰¾åˆ°å…¬å¸åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        
        if not year:
            # ä½¿ç”¨å½“å‰å¹´ä»½çš„å‰ä¸€å¹´ä½œä¸ºé»˜è®¤å€¼
            from datetime import datetime
            year = str(datetime.now().year - 1)
            logger.warning(f"æœªæ‰¾åˆ°å¹´ä»½ï¼Œä½¿ç”¨é»˜è®¤å€¼: {year}")
        
        logger.info(f"å¼€å§‹ç”Ÿæˆæœé‚¦åˆ†æ: {company_name} - {year}")
        
        # æ³¨æ„ï¼šä¸å†åœ¨è¿™é‡Œé¢„å…ˆæå–æ•°æ®ï¼Œè®© generate_dupont_analysis å†…éƒ¨çš„ extract_financial_data_for_dupont 
        # ä½¿ç”¨ç»“æ„åŒ–LLMè¾“å‡ºæ–¹æ³•æå–æ•°æ®ï¼Œè¿™æ ·æ›´å‡†ç¡®ï¼ˆå’Œ quick-overview ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ï¼‰
        # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ï¼Œä¼ é€’ç»™ generate_dupont_analysisï¼Œè®©å®ƒå†…éƒ¨é™åˆ¶æŸ¥è¯¢èŒƒå›´
        
        # è°ƒç”¨æœé‚¦åˆ†æå‡½æ•°ï¼ˆç°åœ¨æ˜¯asyncï¼‰
        # generate_dupont_analysis å†…éƒ¨ä¼šè°ƒç”¨ extract_financial_data_for_dupont
        # è¯¥æ–¹æ³•ä½¿ç”¨ç»“æ„åŒ–LLMè¾“å‡ºï¼ˆas_structured_llmï¼‰ï¼Œæ¯”æ­£åˆ™è¡¨è¾¾å¼æå–æ›´å‡†ç¡®
        # å’Œ quick-overview ä½¿ç”¨ç›¸åŒçš„æ•°æ®æå–æ–¹æ³•
        dupont_result = await generate_dupont_analysis(
            company_name=company_name,
            year=year,
            query_engine=query_engine,
            financial_data=None,  # è®©å‡½æ•°å†…éƒ¨ä½¿ç”¨ç»“æ„åŒ–LLMæ–¹æ³•æå–ï¼Œæ›´å‡†ç¡®
            filename=filename  # ä¼ é€’æ–‡ä»¶åï¼Œç”¨äºé™åˆ¶æŸ¥è¯¢èŒƒå›´
        )
        
        logger.info(f"âœ… æœé‚¦åˆ†æç”ŸæˆæˆåŠŸ")
        
        # å°†Decimalç±»å‹è½¬æ¢ä¸ºfloatï¼Œç¡®ä¿JSONå¯åºåˆ—åŒ–
        def convert_decimal_to_float(obj):
            """é€’å½’åœ°å°†Decimalè½¬æ¢ä¸ºfloat"""
            from decimal import Decimal
            from datetime import datetime
            if isinstance(obj, Decimal):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert_decimal_to_float(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_decimal_to_float(item) for item in obj]
            elif isinstance(obj, datetime):
                return obj.isoformat()
            else:
                return obj
        
        # è½¬æ¢ç»“æœ
        serializable_result = convert_decimal_to_float(dupont_result)

        # è¯»å–ç»“æ„åŒ–æŒ‡æ ‡JSONï¼ˆç”¨äºå‰ç«¯å¹´ä»½åˆ‡æ¢ï¼‰
        metrics_json = None
        try:
            import json
            import re
            from pathlib import Path

            safe_company = re.sub(r'[^\w\u4e00-\u9fff\-]+', '_', company_name or 'unknown')
            safe_year = re.sub(r'[^\d]+', '', str(year or ''))
            metrics_path = Path("storage") / f"dupont_metrics_{safe_company}_{safe_year or 'unknown'}.json"
            if metrics_path.exists():
                with open(metrics_path, "r", encoding="utf-8") as f:
                    metrics_json = json.load(f)
        except Exception as e:
            logger.warning(f"è¯»å–ç»“æ„åŒ–æŒ‡æ ‡JSONå¤±è´¥: {str(e)}")
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "company_name": company_name,
                "year": year,
                "analysis": serializable_result,
                "metrics": metrics_json
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç”Ÿæˆæœé‚¦åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"ç”Ÿæˆæœé‚¦åˆ†æå¤±è´¥: {str(e)}"
        )

@router.post("/quick-overview")
async def get_quick_overview():
    """
    å¿«é€Ÿç”Ÿæˆè´¢åŠ¡æ¦‚å†µæ¥å£ï¼ˆç»“æ„åŒ–è´¢åŠ¡å¿«ç…§ï¼‰
    
    ä¸¤é˜¶æ®µç”Ÿæˆï¼š
    1. ç¬¬ä¸€é˜¶æ®µï¼šæå–å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼ˆROEã€è¥æ”¶ã€å‡€åˆ©æ¶¦ã€èµ„äº§æ€»é¢ã€å‡€æ¯å·®ã€æˆæœ¬æ”¶å…¥æ¯”ï¼‰
    2. ç¬¬äºŒé˜¶æ®µï¼šåŸºäºæŒ‡æ ‡ç”Ÿæˆä¸€å¥è¯ç»“è®º
    
    æ³¨æ„ï¼šåªæ£€ç´¢ä¸Šä¼ æ–‡æ¡£å¯¹åº”å…¬å¸çš„æ•°æ®
    
    Returns:
        è´¢åŠ¡å¿«ç…§æ•°æ®ï¼ŒåŒ…å«ï¼š
        - å…³é”®æŒ‡æ ‡ï¼ˆroe, revenue, net_profit, total_assets, net_interest_margin, cost_income_ratioï¼‰
        - verdict: ä¸€å¥è¯ç»“è®º
        - stage: å…¬å¸é˜¶æ®µï¼ˆå¢é•¿/ç¨³æ€/ä¸‹è¡Œï¼‰
        - profit_quality: èµšé’±è´¨é‡æè¿°
        - risk_level: é£é™©çº§åˆ«ï¼ˆä½/ä¸­/é«˜ï¼‰
    """
    try:
        from llama_index.core.llms import ChatMessage
        from llama_index.core import Settings
        import re
        
        logger.info("å¼€å§‹ç”Ÿæˆè´¢åŠ¡å¿«ç…§ï¼ˆä¸¤é˜¶æ®µç”Ÿæˆï¼‰...")
        
        # è·å–RAGå¼•æ“
        rag_engine = get_rag_engine()
        
        if not rag_engine.query_engine:
            if not rag_engine.load_existing_index():
                raise HTTPException(
                    status_code=400,
                    detail="ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
                )
        
        llm = Settings.llm
        
        # ========== ç¬¬é›¶é˜¶æ®µï¼šä»ä¸Šä¼ çš„æ–‡æ¡£ä¸­æå–å…¬å¸åç§°å’Œå¹´ä»½ ==========
        logger.info("ç¬¬é›¶é˜¶æ®µï¼šä»ä¸Šä¼ çš„æ–‡æ¡£ä¸­æå–å…¬å¸åç§°å’Œå¹´ä»½...")
        
        company_name = None
        year = None
        context_filter = {}
        uploaded_filenames = set()  # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ååˆ—è¡¨ï¼Œç”¨äºæ›´ä¸¥æ ¼çš„è¿‡æ»¤
        
        try:
            # æ–¹æ³•1ï¼šç›´æ¥ä»uploadsç›®å½•çš„æ–‡ä»¶åä¸­æå–å…¬å¸åç§°ï¼ˆæœ€å‡†ç¡®ï¼‰
            from pathlib import Path
            upload_dir = Path("uploads")
            
            if upload_dir.exists():
                seen_companies = set()
                seen_years = set()
                # éå†æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶
                for file_path in upload_dir.iterdir():
                    if file_path.is_file():
                        filename = file_path.name
                        uploaded_filenames.add(filename)  # ä¿å­˜æ–‡ä»¶å
                        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
                        name_without_ext = re.sub(r'\.[^.]+$', '', filename)
                        # ç§»é™¤å¸¸è§çš„æŠ¥è¡¨ç±»å‹å…³é”®è¯
                        name_clean = re.sub(r'(åˆ©æ¶¦è¡¨|èµ„äº§è´Ÿå€ºè¡¨|ç°é‡‘æµé‡è¡¨|å¹´æŠ¥|æŠ¥å‘Š|è´¢åŠ¡æŠ¥è¡¨|è´¢åŠ¡æŠ¥å‘Š)', '', name_without_ext, flags=re.IGNORECASE)
                        
                        # ä»æ–‡ä»¶åæå–å¹´ä»½ï¼ˆåœ¨ç§»é™¤å¹´ä»½ä¹‹å‰ï¼‰
                        if not year:
                            year_match = re.search(r'(\d{4})', filename)
                            if year_match:
                                candidate_year = year_match.group(1)
                                # éªŒè¯å¹´ä»½åˆç†æ€§
                                if 2000 <= int(candidate_year) <= 2030:
                                    year = candidate_year
                                    seen_years.add(year)
                                    logger.info(f"  ä»æ–‡ä»¶ '{filename}' æå–åˆ°å¹´ä»½: {year}")
                        
                        # ç§»é™¤å¹´ä»½ï¼ˆ4ä½æ•°å­—ï¼‰
                        name_clean = re.sub(r'\d{4}å¹´?', '', name_clean)
                        # ç§»é™¤"å¹´åº¦"å’Œåé¢çš„æ•°å­—ï¼ˆå¦‚"å¹´åº¦60"ï¼‰
                        name_clean = re.sub(r'å¹´åº¦\d+', '', name_clean)
                        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                        name_clean = re.sub(r'[_\-\s\.]+', '', name_clean).strip()
                        
                        if name_clean and len(name_clean) >= 2 and len(name_clean) <= 30:
                            seen_companies.add(name_clean)
                            logger.info(f"  ä»æ–‡ä»¶ '{filename}' æå–åˆ°å…¬å¸å: {name_clean}")
                
                # å¦‚æœæ‰¾åˆ°å¤šä¸ªå¯èƒ½çš„å…¬å¸åï¼Œé€‰æ‹©æœ€å¸¸è§çš„
                if seen_companies:
                    from collections import Counter
                    company_counts = Counter(seen_companies)
                    # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„
                    company_name = company_counts.most_common(1)[0][0]
                    logger.info(f"âœ… ä»ä¸Šä¼ æ–‡ä»¶æå–å…¬å¸åç§°: {company_name} (å‡ºç° {company_counts[company_name]} æ¬¡)")
                    logger.info(f"âœ… ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨: {list(uploaded_filenames)}")
            
                # å¦‚æœæ‰¾åˆ°å¤šä¸ªå¹´ä»½ï¼Œé€‰æ‹©æœ€å¸¸è§çš„ï¼ˆé€šå¸¸åº”è¯¥åªæœ‰ä¸€ä¸ªï¼‰
                if seen_years:
                    from collections import Counter
                    year_counts = Counter(seen_years)
                    year = year_counts.most_common(1)[0][0]
                    logger.info(f"âœ… ä»ä¸Šä¼ æ–‡ä»¶æå–å¹´ä»½: {year} (å‡ºç° {year_counts[year]} æ¬¡)")
            
            # æ–¹æ³•2ï¼šå¦‚æœè¿˜æ²¡æ‰¾åˆ°å¹´ä»½ï¼Œä»æ–‡æ¡£å†…å®¹ä¸­æå–
            if not year:
                try:
                    if upload_dir.exists():
                        uploaded_files = [f.name for f in upload_dir.iterdir() if f.is_file()]
                        if uploaded_files:
                            # å°è¯•ä»æ–‡æ¡£å†…å®¹ä¸­æå–å¹´ä»½
                            year_query = "æŠ¥å‘Šå¹´åº¦ å¹´ä»½ å¹´åº¦æŠ¥å‘Š æŠ¥å‘Šå¹´ä»½"
                            try:
                                retriever = rag_engine.index.as_retriever(similarity_top_k=20)
                                nodes = retriever.retrieve(year_query)
                                
                                # åªä»å½“å‰ä¸Šä¼ çš„æ–‡ä»¶ä¸­æå–
                                for node in nodes:
                                    filename = node.metadata.get('filename') or node.metadata.get('source_file', '')
                                    if filename in uploaded_filenames:
                                        # ä»æ–‡æœ¬ä¸­æå–å¹´ä»½
                                        node_text = node.text
                                        year_patterns = [
                                            r'æŠ¥å‘Šå¹´åº¦[ï¼š:]\s*(\d{4})',
                                            r'(\d{4})å¹´åº¦',
                                            r'(\d{4})å¹´[åº¦]?æŠ¥å‘Š',
                                        ]
                                        for pattern in year_patterns:
                                            year_match = re.search(pattern, node_text)
                                            if year_match:
                                                candidate_year = year_match.group(1)
                                                # éªŒè¯å¹´ä»½åˆç†æ€§
                                                if 2000 <= int(candidate_year) <= 2030:
                                                    year = candidate_year
                                                    logger.info(f"âœ… ä»æ–‡æ¡£å†…å®¹æå–å¹´ä»½: {year}")
                                                    break
                                        if year:
                                            break
                            except Exception as e:
                                logger.warning(f"ä»æ–‡æ¡£å†…å®¹æå–å¹´ä»½å¤±è´¥: {str(e)}")
                except Exception as e:
                    logger.warning(f"æå–å¹´ä»½å¤±è´¥: {str(e)}")
            
            # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œä»ç´¢å¼•ä¸­çš„æ–‡æ¡£å…ƒæ•°æ®ä¸­æå–ï¼ˆåªä»å½“å‰ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
            if not company_name and rag_engine.index:
                try:
                    # è·å–æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶å
                    uploaded_filenames = set()
                    if upload_dir.exists():
                        uploaded_filenames = {f.name for f in upload_dir.iterdir() if f.is_file()}
                    
                    if uploaded_filenames:
                        retriever = rag_engine.index.as_retriever(similarity_top_k=50)
                        nodes = retriever.retrieve("å…¬å¸åç§°")
                        
                        # åªä»å½“å‰ä¸Šä¼ çš„æ–‡ä»¶ä¸­æå–
                        seen_companies = set()
                        for node in nodes:
                            filename = node.metadata.get('filename') or node.metadata.get('source_file', '')
                            # åªå¤„ç†å½“å‰ä¸Šä¼ çš„æ–‡ä»¶
                            if filename in uploaded_filenames:
                                # ç§»é™¤æ–‡ä»¶æ‰©å±•å
                                name_without_ext = re.sub(r'\.[^.]+$', '', filename)
                                # ç§»é™¤å¸¸è§çš„æŠ¥è¡¨ç±»å‹å…³é”®è¯
                                name_clean = re.sub(r'(åˆ©æ¶¦è¡¨|èµ„äº§è´Ÿå€ºè¡¨|ç°é‡‘æµé‡è¡¨|å¹´æŠ¥|æŠ¥å‘Š|è´¢åŠ¡æŠ¥è¡¨|è´¢åŠ¡æŠ¥å‘Š)', '', name_without_ext, flags=re.IGNORECASE)
                                # ç§»é™¤å¹´ä»½
                                name_clean = re.sub(r'\d{4}å¹´?', '', name_clean)
                                # ç§»é™¤"å¹´åº¦"å’Œåé¢çš„æ•°å­—
                                name_clean = re.sub(r'å¹´åº¦\d+', '', name_clean)
                                # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                                name_clean = re.sub(r'[_\-\s\.]+', '', name_clean).strip()
                                if name_clean and len(name_clean) >= 2 and len(name_clean) <= 30:
                                    seen_companies.add(name_clean)
                        
                        if seen_companies:
                            from collections import Counter
                            company_counts = Counter(seen_companies)
                            company_name = company_counts.most_common(1)[0][0]
                            logger.info(f"âœ… ä»ç´¢å¼•å…ƒæ•°æ®æå–å…¬å¸åç§°: {company_name}")
                except Exception as e:
                    logger.warning(f"ä»ç´¢å¼•å…ƒæ•°æ®æå–å…¬å¸åç§°å¤±è´¥: {str(e)}")
            
            # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œä»æ–‡æ¡£å†…å®¹ä¸­æå–ï¼ˆä½†é™åˆ¶åœ¨å½“å‰ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
            if not company_name:
                try:
                    # å¦‚æœæœ‰ä¸Šä¼ çš„æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºä¸Šä¸‹æ–‡
                    if upload_dir.exists():
                        uploaded_files = [f.name for f in upload_dir.iterdir() if f.is_file()]
                        if uploaded_files:
                            files_context = "ã€".join(uploaded_files[:5])  # æœ€å¤š5ä¸ªæ–‡ä»¶å
                            extract_query = f"è¯·ä»ä»¥ä¸‹æ–‡ä»¶åçš„æ–‡æ¡£ä¸­æå–å…¬å¸åç§°ï¼ˆå®Œæ•´çš„å…¬å¸å…¨ç§°ï¼‰ï¼š{files_context}ã€‚åªè¿”å›å…¬å¸åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
                        else:
                            extract_query = "è¯·ä»æ–‡æ¡£ä¸­æå–å…¬å¸åç§°ï¼ˆå®Œæ•´çš„å…¬å¸å…¨ç§°ï¼‰ï¼Œåªè¿”å›å…¬å¸åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹"
                    else:
                        extract_query = "è¯·ä»æ–‡æ¡£ä¸­æå–å…¬å¸åç§°ï¼ˆå®Œæ•´çš„å…¬å¸å…¨ç§°ï¼‰ï¼Œåªè¿”å›å…¬å¸åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹"
                    
                    response = rag_engine.query_engine.query(extract_query)
                    response_text = str(response).strip()
                    
                    # å°è¯•æå–å…¬å¸åç§°
                    patterns = [
                        r'([^ï¼Œ,ã€‚\n]{2,30}(?:è‚¡ä»½|æœ‰é™|å…¬å¸|é›†å›¢|é“¶è¡Œ|è¯åˆ¸|ä¿é™©))',
                        r'å…¬å¸åç§°[ï¼š:]\s*([^ï¼Œ,ã€‚\n]{2,30})',
                        r'([A-Za-z0-9\u4e00-\u9fa5]{2,20}(?:è‚¡ä»½|æœ‰é™|å…¬å¸|é›†å›¢))',
                    ]
                    for pattern in patterns:
                        match = re.search(pattern, response_text)
                        if match:
                            candidate = match.group(1).strip()
                            # ç§»é™¤"å¹´åº¦"å’Œåé¢çš„æ•°å­—
                            candidate = re.sub(r'å¹´åº¦\d+', '', candidate).strip()
                            if len(candidate) >= 2 and len(candidate) <= 30:
                                company_name = candidate
                                logger.info(f"âœ… ä»æ–‡æ¡£å†…å®¹æå–å…¬å¸åç§°: {company_name}")
                                break
                except Exception as e:
                    logger.warning(f"ä»æ–‡æ¡£å†…å®¹æå–å…¬å¸åç§°å¤±è´¥: {str(e)}")
            
            # å¦‚æœæ‰¾åˆ°äº†å…¬å¸åç§°ï¼Œè®¾ç½®context_filter
            if company_name:
                context_filter['company'] = company_name
                logger.info(f"âœ… è®¾ç½®å…¬å¸è¿‡æ»¤æ¡ä»¶: {company_name}")
                
            # å¦‚æœæ‰¾åˆ°äº†å¹´ä»½ï¼Œè®¾ç½®context_filter
            if year:
                context_filter['year'] = year
                logger.info(f"âœ… è®¾ç½®å¹´ä»½è¿‡æ»¤æ¡ä»¶: {year}")
                
                # å¦‚æœæ‰¾åˆ°äº†ä¸Šä¼ çš„æ–‡ä»¶åï¼Œä¹Ÿæ·»åŠ åˆ°è¿‡æ»¤æ¡ä»¶ä¸­ï¼ˆæ›´ä¸¥æ ¼çš„è¿‡æ»¤ï¼‰
                if uploaded_filenames:
                    # æ³¨æ„ï¼šHybridRetrieverçš„context_filteræ”¯æŒfilenameåˆ—è¡¨ï¼Œä½†è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å…¬å¸åè¿‡æ»¤
                    # æ–‡ä»¶åè¿‡æ»¤ä¼šåœ¨HybridRetrieverå†…éƒ¨é€šè¿‡å…¬å¸ååŒ¹é…æ–‡ä»¶åæ¥å®ç°
                    logger.info(f"âœ… ä¸Šä¼ çš„æ–‡ä»¶æ•°é‡: {len(uploaded_filenames)}")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å…¬å¸åç§°ï¼Œå°†æ£€ç´¢æ‰€æœ‰æ–‡æ¡£")
                # å³ä½¿æ²¡æœ‰æ‰¾åˆ°å…¬å¸åï¼Œå¦‚æœæœ‰ä¸Šä¼ çš„æ–‡ä»¶åï¼Œä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨æ–‡ä»¶åè¿‡æ»¤
                if uploaded_filenames and len(uploaded_filenames) == 1:
                    # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨æ–‡ä»¶åè¿‡æ»¤
                    single_filename = list(uploaded_filenames)[0]
                    context_filter['filename'] = single_filename
                    logger.info(f"âœ… ä½¿ç”¨æ–‡ä»¶åè¿‡æ»¤: {single_filename}")
                
        except Exception as e:
            logger.warning(f"æå–å…¬å¸åç§°å¤±è´¥: {str(e)}ï¼Œå°†æ£€ç´¢æ‰€æœ‰æ–‡æ¡£")
            import traceback
            logger.warning(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # è®°å½•ä½¿ç”¨çš„è¿‡æ»¤æ¡ä»¶
        if context_filter:
            logger.info(f"ğŸ“‹ æ£€ç´¢è¿‡æ»¤æ¡ä»¶: {context_filter}")
        
        # ========== ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæå–å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼ˆä¼˜å…ˆä»Excelè¡¨æ ¼ï¼‰==========
        logger.info("ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæå–å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼ˆä¼˜å…ˆä»Excelè¡¨æ ¼ï¼‰...")
        
        snapshot_dict = {
            "roe": None,
            "revenue": None,
            "net_profit": None,
            "total_assets": None,
            "net_interest_margin": None,
            "cost_income_ratio": None
        }
        
        import re
        
        # ä¼˜åŒ–ï¼šå…ˆä½¿ç”¨RAGæ£€ç´¢æ‰€æœ‰æ–‡æ¡£ï¼ˆPDFå’ŒExcelï¼‰çš„è¡¨æ ¼æ•°æ®ï¼Œç„¶åä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºæå–
        try:
            # ä½¿ç”¨HybridRetrieveræ£€ç´¢ï¼ˆä¸æ™®é€šæŸ¥è¯¢ç›¸åŒçš„æ–¹æ³•ï¼‰
            logger.info("ğŸ” ä½¿ç”¨HybridRetrieveræ£€ç´¢è´¢åŠ¡æ•°æ®ï¼ˆä¸æ™®é€šæŸ¥è¯¢ç›¸åŒçš„æ–¹æ³•ï¼‰...")
            
            # æ£€æŸ¥HybridRetrieveræ˜¯å¦å¯ç”¨
            use_hybrid = (rag_engine.use_hybrid_retriever and 
                         rag_engine.hybrid_retriever and 
                         rag_engine.hybrid_retriever.text_index and 
                         rag_engine.hybrid_retriever.table_index)
            
            if use_hybrid:
                logger.info("âœ… ä½¿ç”¨HybridRetrieverè¿›è¡Œæ··åˆæ£€ç´¢ï¼ˆå¤šæŒ‡æ ‡åˆ†åˆ«æ£€ç´¢ï¼‰")
                
                # å®šä¹‰6ä¸ªå…³é”®æŒ‡æ ‡åŠå…¶æŸ¥è¯¢å…³é”®è¯
                indicators = {
                    "roe": ["åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡", "ROE", "å‡€èµ„äº§æ”¶ç›Šç‡"],
                    "revenue": ["è¥ä¸šæ”¶å…¥", "è¥ä¸šæ€»æ”¶å…¥", "æ”¶å…¥"],
                    "net_profit": ["å‡€åˆ©æ¶¦", "å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦", "å½’å±äºæœ¬è¡Œè‚¡ä¸œçš„å‡€åˆ©æ¶¦"],
                    "total_assets": ["èµ„äº§æ€»é¢", "æ€»èµ„äº§", "èµ„äº§åˆè®¡"],
                    "net_interest_margin": ["å‡€æ¯å·®"],
                    "cost_income_ratio": ["æˆæœ¬æ”¶å…¥æ¯”"]
                }
                
                # ä¸ºæ¯ä¸ªæŒ‡æ ‡å•ç‹¬æ£€ç´¢ï¼Œç¡®ä¿éƒ½èƒ½æ‰¾åˆ°
                all_hybrid_results = []
                all_table_results = []
                found_indicators = set()
                
                year_prefix = f"{year}å¹´ " if year else ""
                company_prefix = f"{company_name} " if company_name else ""
                
                for indicator_key, keywords in indicators.items():
                    # ä¸ºæ¯ä¸ªæŒ‡æ ‡æ„å»ºæŸ¥è¯¢
                    query_keywords = " ".join(keywords)
                    if year and company_name:
                        query_text = f"{company_prefix}{year_prefix}{query_keywords} {year}å¹´åº¦æ•°å€¼"
                    elif year:
                        query_text = f"{year_prefix}{query_keywords} {year}å¹´åº¦æ•°å€¼"
                    elif company_name:
                        query_text = f"{company_prefix}{query_keywords} æœ€æ–°å¹´åº¦æ•°å€¼"
                    else:
                        query_text = f"{query_keywords} æœ€æ–°å¹´åº¦æ•°å€¼"
                    
                    logger.info(f"  ğŸ” æ£€ç´¢æŒ‡æ ‡: {indicator_key} ({keywords[0]})")
                    indicator_results = rag_engine.hybrid_retriever.retrieve(
                        query_text,
                        top_k=30,  # æ¯ä¸ªæŒ‡æ ‡æ£€ç´¢30ä¸ªç»“æœ
                        context_filter=context_filter if context_filter else None
                    )
                    
                    if indicator_results:
                        logger.info(f"    âœ… {indicator_key} æ£€ç´¢åˆ° {len(indicator_results)} ä¸ªç»“æœ")
                        all_hybrid_results.extend(indicator_results)
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æ•°æ®
                        for r in indicator_results:
                            doc = r.get('document')
                            if not doc:
                                continue
                            # æ£€æŸ¥metadata
                            is_table = (
                                'table' in str(doc.metadata).lower() or 
                                doc.metadata.get('document_type') == 'table_data' or
                                doc.metadata.get('is_financial', False)
                            )
                            # æ£€æŸ¥æ–‡æœ¬å†…å®¹
                            text_preview = str(doc.text)[:500] if hasattr(doc, 'text') else ''
                            is_table_by_text = (
                                any(kw in text_preview for kw in keywords) or
                                'èµ„äº§è´Ÿå€ºè¡¨' in text_preview or
                                'åˆ©æ¶¦è¡¨' in text_preview or
                                '|' in text_preview or
                                ('é¡¹ ç›®' in text_preview and (year in text_preview if year else True))
                            )
                            if is_table or is_table_by_text:
                                all_table_results.append(r)
                                found_indicators.add(indicator_key)
                    else:
                        logger.warning(f"    âš ï¸ {indicator_key} æœªæ£€ç´¢åˆ°ç»“æœ")
                
                # å»é‡ï¼ˆåŸºäºæ–‡æ¡£IDæˆ–æ–‡æœ¬å†…å®¹ï¼‰
                seen_docs = set()
                unique_hybrid_results = []
                unique_table_results = []
                
                for r in all_hybrid_results:
                    doc = r.get('document')
                    if not doc:
                        continue
                    doc_id = doc.metadata.get('file_path') or doc.metadata.get('filename') or str(doc.text)[:100]
                    if doc_id not in seen_docs:
                        seen_docs.add(doc_id)
                        unique_hybrid_results.append(r)
                
                for r in all_table_results:
                    doc = r.get('document')
                    if not doc:
                        continue
                    doc_id = doc.metadata.get('file_path') or doc.metadata.get('filename') or str(doc.text)[:100]
                    if doc_id not in seen_docs:
                        seen_docs.add(doc_id)
                        unique_table_results.append(r)
                
                logger.info(f"âœ… å»é‡åå…±æ£€ç´¢åˆ° {len(unique_hybrid_results)} ä¸ªç»“æœï¼Œå…¶ä¸­ {len(unique_table_results)} ä¸ªæ˜¯è¡¨æ ¼æ•°æ®")
                logger.info(f"âœ… æ‰¾åˆ°çš„æŒ‡æ ‡: {', '.join(found_indicators) if found_indicators else 'æ— '}")
                
                # ä¼˜å…ˆä½¿ç”¨è¡¨æ ¼æ•°æ®ï¼Œå¦‚æœè¡¨æ ¼æ•°æ®ä¸è¶³ï¼Œè¡¥å……å…¶ä»–ç»“æœ
                if unique_table_results:
                    logger.info(f"  âœ… ä¼˜å…ˆä½¿ç”¨ {len(unique_table_results)} ä¸ªè¡¨æ ¼æ•°æ®")
                    all_context_text = "\n\n".join([r['document'].text for r in unique_table_results[:20]])
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰æŒ‡æ ‡
                    missing_indicators = []
                    for indicator_key, keywords in indicators.items():
                        if not any(kw in all_context_text for kw in keywords):
                            missing_indicators.append(indicator_key)
                    
                    if missing_indicators:
                        logger.warning(f"  âš ï¸ è¡¨æ ¼æ•°æ®ä¸­ç¼ºå°‘ä»¥ä¸‹æŒ‡æ ‡: {', '.join(missing_indicators)}ï¼Œè¡¥å……å…¶ä»–ç»“æœ")
                        # è¡¥å……éè¡¨æ ¼ç»“æœ
                        non_table_results = [r for r in unique_hybrid_results if r not in unique_table_results]
                        if non_table_results:
                            additional_text = "\n\n".join([r['document'].text for r in non_table_results[:15]])
                            all_context_text = all_context_text + "\n\n" + additional_text
                    else:
                        logger.info(f"  âœ… è¡¨æ ¼æ•°æ®ä¸­åŒ…å«æ‰€æœ‰6ä¸ªæŒ‡æ ‡")
                else:
                    logger.info(f"  âš ï¸ æœªè¯†åˆ«å‡ºè¡¨æ ¼æ•°æ®ï¼Œä½¿ç”¨æ‰€æœ‰ç»“æœ")
                    all_context_text = "\n\n".join([r['document'].text for r in unique_hybrid_results[:30]])
                
                logger.info(f"âœ… æ„å»ºä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(all_context_text)}å­—ç¬¦")
                
                # æœ€ç»ˆæ£€æŸ¥æ‰€æœ‰æŒ‡æ ‡
                final_missing = []
                for indicator_key, keywords in indicators.items():
                    if not any(kw in all_context_text for kw in keywords):
                        final_missing.append(indicator_key)
                
                if final_missing:
                    logger.warning(f"  âš ï¸ æœ€ç»ˆä¸Šä¸‹æ–‡ä¸­ä»ç¼ºå°‘ä»¥ä¸‹æŒ‡æ ‡: {', '.join(final_missing)}")
                else:
                    logger.info(f"  âœ… æœ€ç»ˆä¸Šä¸‹æ–‡ä¸­åŒ…å«æ‰€æœ‰6ä¸ªæŒ‡æ ‡")
                
                if not all_context_text:
                    logger.warning("âš ï¸ HybridRetrieveræœªæ‰¾åˆ°ç»“æœï¼Œä½¿ç”¨query_engine")
                    use_hybrid = False
            
            if not use_hybrid:
                # å›é€€åˆ°ä½¿ç”¨query_engineæŸ¥è¯¢ï¼ˆä¸æ™®é€šæŸ¥è¯¢ç›¸åŒï¼‰
                logger.info("ğŸ” ä½¿ç”¨query_engineæŸ¥è¯¢è´¢åŠ¡æ•°æ®...")
                
                # ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢ä¸€ä¸ªæŒ‡æ ‡ï¼ˆè¿™æ ·æ›´å‡†ç¡®ï¼‰
                # å¦‚æœæœ‰å…¬å¸åç§°å’Œå¹´ä»½ï¼Œåœ¨æŸ¥è¯¢ä¸­åŠ å…¥å…¬å¸åç§°å’Œå¹´ä»½é™åˆ¶
                if company_name and year:
                    queries = [
                        f"{company_name}{year}å¹´çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ç™¾åˆ†æ¯”",
                        f"{company_name}{year}å¹´çš„è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„è¥ä¸šæ”¶å…¥æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        f"{company_name}{year}å¹´çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½",
                        f"{company_name}{year}å¹´çš„èµ„äº§æ€»é¢æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„èµ„äº§æ€»é¢æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        f"{company_name}{year}å¹´çš„å‡€æ¯å·®æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„å‡€æ¯å·®ç™¾åˆ†æ¯”",
                        f"{company_name}{year}å¹´çš„æˆæœ¬æ”¶å…¥æ¯”æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„æˆæœ¬æ”¶å…¥æ¯”ç™¾åˆ†æ¯”"
                    ]
                elif company_name:
                    queries = [
                        f"{company_name}çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ç™¾åˆ†æ¯”",
                        f"{company_name}çš„è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„è¥ä¸šæ”¶å…¥æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        f"{company_name}çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½",
                        f"{company_name}çš„èµ„äº§æ€»é¢æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„èµ„äº§æ€»é¢æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        f"{company_name}çš„å‡€æ¯å·®æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„å‡€æ¯å·®ç™¾åˆ†æ¯”",
                        f"{company_name}çš„æˆæœ¬æ”¶å…¥æ¯”æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„æˆæœ¬æ”¶å…¥æ¯”ç™¾åˆ†æ¯”"
                    ]
                elif year:
                    queries = [
                        f"{year}å¹´çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ç™¾åˆ†æ¯”",
                        f"{year}å¹´çš„è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„è¥ä¸šæ”¶å…¥æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        f"{year}å¹´çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½",
                        f"{year}å¹´çš„èµ„äº§æ€»é¢æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„èµ„äº§æ€»é¢æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        f"{year}å¹´çš„å‡€æ¯å·®æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„å‡€æ¯å·®ç™¾åˆ†æ¯”",
                        f"{year}å¹´çš„æˆæœ¬æ”¶å…¥æ¯”æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡º{year}å¹´åº¦çš„æˆæœ¬æ”¶å…¥æ¯”ç™¾åˆ†æ¯”"
                    ]
                else:
                    queries = [
                        "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ç™¾åˆ†æ¯”",
                        "è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„è¥ä¸šæ”¶å…¥æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        "å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½",
                        "èµ„äº§æ€»é¢æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„èµ„äº§æ€»é¢æ•°å€¼ï¼ŒåŒ…æ‹¬å•ä½ï¼ˆå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰",
                        "å‡€æ¯å·®æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„å‡€æ¯å·®ç™¾åˆ†æ¯”",
                        "æˆæœ¬æ”¶å…¥æ¯”æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„æˆæœ¬æ”¶å…¥æ¯”ç™¾åˆ†æ¯”"
                    ]
                
                all_context_parts = []
                for query in queries:
                    try:
                        # å¦‚æœæœ‰context_filterï¼Œä½¿ç”¨rag_engine.queryæ–¹æ³•ï¼ˆå®ƒä¼šåº”ç”¨è¿‡æ»¤ï¼‰
                        if context_filter:
                            result = rag_engine.query(query, context_filter)
                            response_text = result.get('answer', '')
                        else:
                            response = rag_engine.query_engine.query(query)
                            response_text = str(response).strip()
                        
                        if response_text and len(response_text) > 20:
                            all_context_parts.append(response_text)
                            logger.info(f"  âœ… æŸ¥è¯¢æˆåŠŸ: {query[:30]}...")
                    except Exception as e:
                        logger.warning(f"æŸ¥è¯¢å¤±è´¥: {query[:30]}... - {str(e)}")
                
                # åˆå¹¶æ‰€æœ‰æŸ¥è¯¢ç»“æœ
                all_context_text = "\n\n".join(all_context_parts) if all_context_parts else ""
                
                if not all_context_text:
                    # å¦‚æœè¿˜æ˜¯æ²¡æ•°æ®ï¼Œä½¿ç”¨ä¸€ä¸ªç»¼åˆæŸ¥è¯¢
                    logger.info("ğŸ”„ ä½¿ç”¨ç»¼åˆæŸ¥è¯¢...")
                    try:
                        if company_name and year:
                            comprehensive_query = f"è¯·ä»{company_name}{year}å¹´çš„æ–‡æ¡£ä¸­æå–ä»¥ä¸‹è´¢åŠ¡æŒ‡æ ‡çš„å…·ä½“æ•°å€¼ï¼š1.{year}å¹´åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰ 2.{year}å¹´è¥ä¸šæ”¶å…¥ 3.{year}å¹´å‡€åˆ©æ¶¦ 4.{year}å¹´èµ„äº§æ€»é¢ 5.{year}å¹´å‡€æ¯å·® 6.{year}å¹´æˆæœ¬æ”¶å…¥æ¯”ã€‚è¯·ç»™å‡º{year}å¹´åº¦çš„æ•°å€¼å’Œå•ä½ã€‚"
                        elif company_name:
                            comprehensive_query = f"è¯·ä»{company_name}çš„æ–‡æ¡£ä¸­æå–ä»¥ä¸‹è´¢åŠ¡æŒ‡æ ‡çš„å…·ä½“æ•°å€¼ï¼š1.åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰ 2.è¥ä¸šæ”¶å…¥ 3.å‡€åˆ©æ¶¦ 4.èµ„äº§æ€»é¢ 5.å‡€æ¯å·® 6.æˆæœ¬æ”¶å…¥æ¯”ã€‚è¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„æ•°å€¼å’Œå•ä½ã€‚"
                        elif year:
                            comprehensive_query = f"è¯·ä»{year}å¹´çš„æ–‡æ¡£ä¸­æå–ä»¥ä¸‹è´¢åŠ¡æŒ‡æ ‡çš„å…·ä½“æ•°å€¼ï¼š1.{year}å¹´åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰ 2.{year}å¹´è¥ä¸šæ”¶å…¥ 3.{year}å¹´å‡€åˆ©æ¶¦ 4.{year}å¹´èµ„äº§æ€»é¢ 5.{year}å¹´å‡€æ¯å·® 6.{year}å¹´æˆæœ¬æ”¶å…¥æ¯”ã€‚è¯·ç»™å‡º{year}å¹´åº¦çš„æ•°å€¼å’Œå•ä½ã€‚"
                        else:
                            comprehensive_query = "è¯·ä»æ‰€æœ‰æ–‡æ¡£ä¸­æå–ä»¥ä¸‹è´¢åŠ¡æŒ‡æ ‡çš„å…·ä½“æ•°å€¼ï¼š1.åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰ 2.è¥ä¸šæ”¶å…¥ 3.å‡€åˆ©æ¶¦ 4.èµ„äº§æ€»é¢ 5.å‡€æ¯å·® 6.æˆæœ¬æ”¶å…¥æ¯”ã€‚è¯·ç»™å‡ºæœ€æ–°å¹´åº¦çš„æ•°å€¼å’Œå•ä½ã€‚"
                        
                        # å¦‚æœæœ‰context_filterï¼Œä½¿ç”¨rag_engine.queryæ–¹æ³•
                        if context_filter:
                            result = rag_engine.query(comprehensive_query, context_filter)
                            all_context_text = result.get('answer', '')
                        else:
                            response = rag_engine.query_engine.query(comprehensive_query)
                            all_context_text = str(response).strip()
                        logger.info(f"  âœ… ç»¼åˆæŸ¥è¯¢æˆåŠŸï¼Œé•¿åº¦: {len(all_context_text)}å­—ç¬¦")
                    except Exception as e:
                        logger.warning(f"ç»¼åˆæŸ¥è¯¢ä¹Ÿå¤±è´¥: {str(e)}")
                        all_context_text = ""
            
            # å¦‚æœä¸Šä¸‹æ–‡å¤ªçŸ­ï¼Œå°è¯•ç›´æ¥æ£€ç´¢è¡¨æ ¼æ•°æ®ï¼ˆåŒ…æ‹¬PDFå’ŒExcelï¼‰
            if len(all_context_text) < 500:
                logger.info("âš ï¸ ä¸Šä¸‹æ–‡å¤ªçŸ­ï¼Œå°è¯•ç›´æ¥æ£€ç´¢è¡¨æ ¼æ•°æ®ï¼ˆPDFå’ŒExcelï¼‰...")
                try:
                    retriever = rag_engine.index.as_retriever(similarity_top_k=50)
                    # æ„å»ºæŸ¥è¯¢ï¼Œæ˜ç¡®åŒ…å«èµ„äº§æ€»é¢
                    table_query = f"{year}å¹´ èµ„äº§è´Ÿå€ºè¡¨ èµ„äº§æ€»é¢ æ€»èµ„äº§ èµ„äº§åˆè®¡ åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ ROE è¥ä¸šæ”¶å…¥ å‡€åˆ©æ¶¦ å‡€æ¯å·® æˆæœ¬æ”¶å…¥æ¯” {year}å¹´åº¦æ•°å€¼" if year else "èµ„äº§è´Ÿå€ºè¡¨ èµ„äº§æ€»é¢ æ€»èµ„äº§ èµ„äº§åˆè®¡ åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ ROE è¥ä¸šæ”¶å…¥ å‡€åˆ©æ¶¦ å‡€æ¯å·® æˆæœ¬æ”¶å…¥æ¯” æœ€æ–°å¹´åº¦æ•°å€¼"
                    nodes = retriever.retrieve(table_query)
                    
                    # åº”ç”¨å…¬å¸è¿‡æ»¤
                    if context_filter and 'company' in context_filter:
                        nodes = rag_engine._filter_nodes(nodes, context_filter)
                        logger.info(f"  âœ… åº”ç”¨å…¬å¸è¿‡æ»¤åï¼Œå‰©ä½™ {len(nodes)} ä¸ªèŠ‚ç‚¹")
                    
                    # æ‰‹åŠ¨è¿‡æ»¤è¡¨æ ¼æ•°æ®ï¼ˆåŒ…æ‹¬PDFå’ŒExcelè¡¨æ ¼ï¼‰
                    table_nodes = [n for n in nodes if (
                        n.metadata.get('document_type') == 'table_data' or 
                        n.metadata.get('is_financial', False) or
                        'table' in str(n.metadata).lower() or
                        'èµ„äº§è´Ÿå€ºè¡¨' in str(n.text)[:200] or  # æ£€æŸ¥æ–‡æœ¬å†…å®¹
                        'èµ„äº§æ€»é¢' in str(n.text)[:200] or
                        'æ€»èµ„äº§' in str(n.text)[:200]
                    )]
                    
                    if table_nodes:
                        table_text = "\n\n".join([node.text for node in table_nodes[:15]])  # å¢åŠ æ•°é‡
                        if len(table_text) > len(all_context_text):
                            all_context_text = table_text
                            logger.info(f"  âœ… ä»è¡¨æ ¼æ•°æ®è·å–åˆ° {len(table_text)} å­—ç¬¦çš„ä¸Šä¸‹æ–‡")
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«èµ„äº§æ€»é¢
                            if 'èµ„äº§æ€»é¢' in table_text or 'æ€»èµ„äº§' in table_text or 'èµ„äº§åˆè®¡' in table_text:
                                logger.info(f"  âœ… è¡¨æ ¼æ•°æ®ä¸­åŒ…å«èµ„äº§æ€»é¢ç›¸å…³ä¿¡æ¯")
                            else:
                                logger.warning(f"  âš ï¸ è¡¨æ ¼æ•°æ®ä¸­æœªæ‰¾åˆ°èµ„äº§æ€»é¢ç›¸å…³ä¿¡æ¯")
                    else:
                        logger.warning(f"  âš ï¸ æœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®èŠ‚ç‚¹")
                except Exception as e:
                    logger.warning(f"æ£€ç´¢è¡¨æ ¼æ•°æ®å¤±è´¥: {str(e)}")
            
            # æ„å»ºä¸€æ¬¡æ€§æå–æ‰€æœ‰æŒ‡æ ‡çš„æç¤ºè¯
            year_emphasis = f"{year}å¹´" if year else "æœ€æ–°å¹´åº¦"
            extract_prompt = f"""è¯·ä»ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ä¸­æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡çš„å…·ä½“æ•°å€¼ã€‚

ã€é‡è¦æç¤ºã€‘
1. è¿™äº›å†…å®¹æ˜¯ä»PDFå’ŒExcelæ–‡ä»¶ä¸­æ£€ç´¢åˆ°çš„ï¼Œå·²ç»åŒ…å«äº†æœ€ç›¸å…³çš„æ•°æ®
2. **ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœå†…å®¹ä¸­åŒ…å«è¡¨æ ¼ï¼ˆMarkdownæ ¼å¼æˆ–æ–‡æœ¬è¡¨æ ¼ï¼‰ï¼Œè¯·ä»”ç»†ä»è¡¨æ ¼ä¸­æå–æ•°æ®**
3. è¥ä¸šæ”¶å…¥å¯èƒ½æœ‰å¤šç§è¡¨è¿°ï¼šè¥ä¸šæ”¶å…¥ã€è¥ä¸šæ€»æ”¶å…¥ã€ä¸»è¥ä¸šåŠ¡æ”¶å…¥ã€æ”¶å…¥ç­‰
4. **é‡è¦ï¼šè¯·åªæå–{year_emphasis}çš„æ•°æ®ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–å¹´åº¦çš„æ•°æ®**
5. å¦‚æœæ–‡æ¡£ä¸­æœ‰å¤šä¸ªå¹´åº¦æ•°æ®ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨{year_emphasis}çš„æ•°æ®
6. è¯·ä»”ç»†æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ•°æ®ï¼Œç‰¹åˆ«æ˜¯è¡¨æ ¼ä¸­çš„æ•°æ®

ã€è¡¨æ ¼æ•°æ®ç¤ºä¾‹æ ¼å¼ã€‘
å¦‚æœçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¡¨æ ¼ï¼š
| é¡¹ ç›® | {year_emphasis} | 2023å¹´ | æœ¬å¹´åŒæ¯”å¢å‡ |
|---|---|---|---|
| è¥ä¸šæ”¶å…¥ | 146,695 | 164,699 | (10.9%) |
| åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ | 10.08% | 11.38% | -1.30ä¸ªç™¾åˆ†ç‚¹ |

è¯·ä»{year_emphasis}åˆ—ä¸­æå–å¯¹åº”çš„æ•°å€¼ã€‚

ã€éœ€è¦æå–çš„æŒ‡æ ‡ã€‘
1. åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼Œ{year_emphasis}ï¼Œç™¾åˆ†æ¯”ï¼‰
   - å¯èƒ½çš„è¡¨è¿°ï¼šåŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ã€ROEã€å‡€èµ„äº§æ”¶ç›Šç‡ç­‰
   - **å¿…é¡»ç¡®ä¿æ˜¯{year_emphasis}çš„æ•°æ®ï¼Œä½¿ç”¨å¹´æŠ¥æŠ«éœ²çš„åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡**
   - ç¤ºä¾‹ï¼š10.08% æˆ– 10.08
2. è¥ä¸šæ”¶å…¥ï¼ˆ{year_emphasis}ï¼Œå•ä½ï¼šå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰
   - å¯èƒ½çš„è¡¨è¿°ï¼šè¥ä¸šæ”¶å…¥ã€è¥ä¸šæ€»æ”¶å…¥ã€ä¸»è¥ä¸šåŠ¡æ”¶å…¥ã€æ”¶å…¥æ€»é¢ç­‰
   - **å¿…é¡»ç¡®ä¿æ˜¯{year_emphasis}çš„æ•°æ®**
   - ç¤ºä¾‹ï¼š146,695ï¼ˆä¸‡å…ƒï¼‰æˆ– 146,695ä¸‡å…ƒ
3. å‡€åˆ©æ¶¦ï¼ˆ{year_emphasis}ï¼Œå½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦ï¼Œå•ä½ï¼šå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰
   - å¯èƒ½çš„è¡¨è¿°ï¼šå‡€åˆ©æ¶¦ã€å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦ã€å½’æ¯å‡€åˆ©æ¶¦ã€å½’å±äºæœ¬è¡Œè‚¡ä¸œçš„å‡€åˆ©æ¶¦ç­‰
   - **å¿…é¡»ç¡®ä¿æ˜¯{year_emphasis}çš„æ•°æ®**
   - ç¤ºä¾‹ï¼š44,508ï¼ˆä¸‡å…ƒï¼‰æˆ– 44,508ä¸‡å…ƒ
4. èµ„äº§æ€»é¢ï¼ˆ{year_emphasis}ï¼Œå•ä½ï¼šå…ƒã€ä¸‡å…ƒæˆ–äº¿å…ƒï¼‰
   - å¯èƒ½çš„è¡¨è¿°ï¼šèµ„äº§æ€»é¢ã€æ€»èµ„äº§ã€èµ„äº§åˆè®¡ç­‰
   - **å¿…é¡»ç¡®ä¿æ˜¯{year_emphasis}çš„æ•°æ®**
5. å‡€æ¯å·®ï¼ˆ{year_emphasis}ï¼Œç™¾åˆ†æ¯”ï¼‰
   - å¯èƒ½çš„è¡¨è¿°ï¼šå‡€æ¯å·®ã€å‡€åˆ©æ¯æ”¶ç›Šç‡ç­‰
   - **å¿…é¡»ç¡®ä¿æ˜¯{year_emphasis}çš„æ•°æ®**
   - ç¤ºä¾‹ï¼š1.87% æˆ– 1.87
6. æˆæœ¬æ”¶å…¥æ¯”ï¼ˆ{year_emphasis}ï¼Œç™¾åˆ†æ¯”ï¼‰
   - å¯èƒ½çš„è¡¨è¿°ï¼šæˆæœ¬æ”¶å…¥æ¯”ã€æˆæœ¬æ”¶å…¥æ¯”ç‡ç­‰
   - **å¿…é¡»ç¡®ä¿æ˜¯{year_emphasis}çš„æ•°æ®**
   - ç¤ºä¾‹ï¼š27.66% æˆ– 27.66

ã€æå–è¦æ±‚ã€‘
- **ä¼˜å…ˆä»è¡¨æ ¼ä¸­æå–æ•°æ®**ï¼Œè¡¨æ ¼æ•°æ®æœ€å‡†ç¡®
- ä»”ç»†æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„æ‰€æœ‰æ•°æ®ï¼Œä¸è¦é—æ¼
- **ä¸¥æ ¼åªæå–{year_emphasis}çš„æ•°æ®ï¼Œå¿½ç•¥å…¶ä»–å¹´åº¦çš„æ•°æ®**
- åªæå–æ•°å€¼ï¼Œä¸è¦åˆ†æ
- å¦‚æœæœ‰åŒæ¯”å˜åŒ–ç‡ï¼Œè¯·ä¸€å¹¶æå–ï¼ˆå¦‚"+20%"ã€"-5%"ã€"ä¸‹é™"ã€"å¢é•¿"ç­‰ï¼‰
- å¦‚æœæŸä¸ªæŒ‡æ ‡ç¼ºå¤±ï¼Œis_missingè®¾ä¸ºtrue
- æ•°å€¼æ ¼å¼ï¼šå¦‚"100äº¿å…ƒ"ã€"10.5äº¿å…ƒ"ã€"30.5%"ã€"146,695ä¸‡å…ƒ"ã€"44,508ä¸‡å…ƒ"
- å¿…é¡»ä»æä¾›çš„æ–‡æ¡£å†…å®¹ä¸­æå–ï¼Œä¸è¦ç¼–é€ æ•°æ®

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ã€‘
{all_context_text if all_context_text else "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹"}

è¯·ä»”ç»†æŸ¥æ‰¾å¹¶æå–æ‰€æœ‰èƒ½æ‰¾åˆ°çš„{year_emphasis}è´¢åŠ¡æ•°æ®ã€‚ç‰¹åˆ«æ³¨æ„è¡¨æ ¼ä¸­çš„æ•°æ®ï¼"""
            
            # ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»è¡¨æ ¼ä¸­ç›´æ¥æå–ï¼ˆæœ€å¯é ï¼‰
            logger.info("ğŸ” å¼€å§‹æå–è´¢åŠ¡æŒ‡æ ‡ï¼ˆä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰...")
            logger.info(f"ğŸ” ä¸Šä¸‹æ–‡æ–‡æœ¬é•¿åº¦: {len(all_context_text)}å­—ç¬¦")
            
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»è¡¨æ ¼æ–‡æœ¬ä¸­ç›´æ¥æå–
            import re
            patterns = {
                "roe": [
                    r'åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡[|\s]+([\d,\.]+%?)',
                    r'åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡[ï¼š:]\s*([\d,\.]+%?)',
                    r'ROE[|\s]+([\d,\.]+%?)',
                    r'ROE[ï¼š:]\s*([\d,\.]+%?)',
                ],
                "revenue": [
                    r'è¥ä¸šæ”¶å…¥[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'è¥ä¸šæ”¶å…¥[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'è¥ä¸šæ€»æ”¶å…¥[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                ],
                "net_profit": [
                    r'å½’å±äºæœ¬è¡Œè‚¡ä¸œçš„å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'å‡€åˆ©æ¶¦[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                ],
                "total_assets": [
                    r'èµ„äº§æ€»é¢[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'æ€»èµ„äº§[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'èµ„äº§åˆè®¡[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    r'èµ„äº§æ€»é¢[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                ],
                "net_interest_margin": [
                    r'å‡€æ¯å·®[|\s]+([\d,\.]+%?)',
                    r'å‡€æ¯å·®[ï¼š:]\s*([\d,\.]+%?)',
                ],
                "cost_income_ratio": [
                    r'æˆæœ¬æ”¶å…¥æ¯”[|\s]+([\d,\.]+%?)',
                    r'æˆæœ¬æ”¶å…¥æ¯”[ï¼š:]\s*([\d,\.]+%?)',
                ]
            }
            
            # ä»ä¸Šä¸‹æ–‡ä¸­æå–å¹´ä»½åˆ—çš„æ•°æ®ï¼ˆå¦‚æœæŒ‡å®šäº†å¹´ä»½ï¼‰
            regex_extracted = {}
            for key, pattern_list in patterns.items():
                for pattern in pattern_list:
                    matches = list(re.finditer(pattern, all_context_text, re.IGNORECASE | re.MULTILINE))
                    if matches:
                        # å¦‚æœæœ‰å¹´ä»½ï¼Œä¼˜å…ˆé€‰æ‹©å¹´ä»½åˆ—çš„æ•°æ®
                        best_match = None
                        if year:
                            # æŸ¥æ‰¾è¡¨æ ¼æ ¼å¼ï¼š| æŒ‡æ ‡å | 2024å¹´ | 2023å¹´ |
                            # æˆ–è€…ï¼šæŒ‡æ ‡å | æ•°å€¼(2024å¹´) | æ•°å€¼(2023å¹´)
                            for match in matches:
                                # æ£€æŸ¥åŒ¹é…ä½ç½®é™„è¿‘æ˜¯å¦æœ‰å¹´ä»½
                                start = max(0, match.start() - 200)
                                end = min(len(all_context_text), match.end() + 200)
                                context_around = all_context_text[start:end]
                                
                                # æ£€æŸ¥æ˜¯å¦åœ¨å¹´ä»½åˆ—ä¸­ï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰
                                # æŸ¥æ‰¾å¹´ä»½åˆ—çš„æ¨¡å¼ï¼š| 2024å¹´ | æ•°å€¼ | æˆ– | 2024 | æ•°å€¼ |
                                year_in_context = f"{year}å¹´" in context_around or str(year) in context_around
                                
                                # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„å¹´ä»½åˆ—ï¼ˆé€šè¿‡æŸ¥æ‰¾è¡¨æ ¼ç»“æ„ï¼‰
                                # å¦‚æœåŒ¹é…å€¼å‰é¢æœ‰å¹´ä»½ï¼Œè¯´æ˜æ˜¯æ­£ç¡®çš„åˆ—
                                match_start = match.start()
                                before_match = all_context_text[max(0, match_start-50):match_start]
                                
                                # æ£€æŸ¥è¡¨æ ¼è¡Œï¼š| æŒ‡æ ‡ | 2024å¹´ | æ•°å€¼ |
                                if year_in_context or (str(year) in before_match and '|' in before_match):
                                    best_match = match
                                    logger.info(f"  âœ… æ‰¾åˆ°{year}å¹´çš„æ•°æ®: {key}")
                                    break
                        
                        if not best_match:
                            best_match = matches[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…
                        
                        value = best_match.group(1).strip()
                        # ä¸ºç™¾åˆ†æ¯”æŒ‡æ ‡æ·»åŠ %ç¬¦å·
                        if key in ["roe", "net_interest_margin", "cost_income_ratio"] and not value.endswith('%'):
                            value = value + '%'
                        
                        # æå–åŒæ¯”å¢å‡æ•°æ®ï¼ˆåœ¨åŒ¹é…ä½ç½®é™„è¿‘æŸ¥æ‰¾ï¼‰
                        change_rate = None
                        change_direction = None
                        match_start = best_match.start()
                        match_end = best_match.end()
                        # åœ¨åŒ¹é…ä½ç½®åæŸ¥æ‰¾åŒæ¯”å¢å‡ï¼ˆé€šå¸¸åœ¨è¡¨æ ¼çš„ä¸‹ä¸€åˆ—ï¼‰
                        after_match = all_context_text[match_end:match_end+200]
                        
                        # æŸ¥æ‰¾åŒæ¯”å¢å‡æ¨¡å¼ï¼ˆåœ¨è¡¨æ ¼çš„"æœ¬å¹´åŒæ¯”å¢å‡"åˆ—ä¸­ï¼‰
                        # æŸ¥æ‰¾è¡¨æ ¼æ ¼å¼ï¼š| æŒ‡æ ‡ | 2024å¹´ | 2023å¹´ | åŒæ¯”å¢å‡ |
                        change_patterns = [
                            r'([\+\-]?\d+\.?\d*%?)',  # å¦‚ +10.9%ã€-5.2%
                            r'\(([\+\-]?\d+\.?\d*%?)\)',  # å¦‚ (10.9%)ã€(-5.2%)
                            r'([\+\-]?\d+\.?\d*)\s*ä¸ªç™¾åˆ†ç‚¹',  # å¦‚ -1.30ä¸ªç™¾åˆ†ç‚¹
                            r'(å¢é•¿|ä¸‹é™|æŒå¹³)',  # æ–‡å­—æè¿°
                        ]
                        
                        for change_pattern in change_patterns:
                            change_match = re.search(change_pattern, after_match)
                            if change_match:
                                change_text = change_match.group(1).strip()
                                # åˆ¤æ–­æ˜¯å˜åŒ–ç‡è¿˜æ˜¯æ–¹å‘
                                if any(c in change_text for c in ['+', '-', '%', 'ç™¾åˆ†ç‚¹']):
                                    change_rate = change_text
                                    # æ ¹æ®æ­£è´Ÿå·åˆ¤æ–­æ–¹å‘
                                    if change_text.startswith('+') or ('%' in change_text and not change_text.startswith('-')):
                                        change_direction = 'å¢é•¿'
                                    elif change_text.startswith('-') or ('-' in change_text):
                                        change_direction = 'ä¸‹é™'
                                    else:
                                        change_direction = 'æŒå¹³'
                                elif change_text in ['å¢é•¿', 'ä¸‹é™', 'æŒå¹³']:
                                    change_direction = change_text
                                if change_rate or change_direction:
                                    break
                        
                        regex_extracted[key] = {
                            "name": {
                                "roe": "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰",
                                "revenue": "è¥ä¸šæ”¶å…¥",
                                "net_profit": "å‡€åˆ©æ¶¦",
                                "total_assets": "èµ„äº§æ€»é¢",
                                "net_interest_margin": "å‡€æ¯å·®",
                                "cost_income_ratio": "æˆæœ¬æ”¶å…¥æ¯”"
                            }.get(key, key),
                            "value": value,
                            "change_rate": change_rate,
                            "change_direction": change_direction,
                            "is_missing": False
                        }
                        logger.info(f"  âœ… æ­£åˆ™æå–åˆ° {key}: {value}" + (f", åŒæ¯”: {change_rate}" if change_rate else ""))
                        break
            
            # æ›´æ–°snapshot_dict
            snapshot_dict.update(regex_extracted)
            logger.info(f"âœ… æ­£åˆ™è¡¨è¾¾å¼æå–å®Œæˆï¼Œæå–åˆ° {len(regex_extracted)} ä¸ªæŒ‡æ ‡")
            
            # ç¬¬äºŒæ­¥ï¼šå¦‚æœæ­£åˆ™æå–ä¸å®Œæ•´ï¼Œä½¿ç”¨JSONæ ¼å¼çš„LLMæå–è¡¥å……
            missing_keys = [k for k in ["roe", "revenue", "net_profit", "total_assets", "net_interest_margin", "cost_income_ratio"] 
                          if snapshot_dict.get(k) is None]
            
            if missing_keys:
                logger.info(f"âš ï¸ ä»¥ä¸‹æŒ‡æ ‡æœªé€šè¿‡æ­£åˆ™æå–ï¼Œä½¿ç”¨JSONæ ¼å¼LLMæå–: {missing_keys}")
                
                # ä½¿ç”¨ç®€åŒ–çš„JSONæ ¼å¼æå–
                json_prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–è´¢åŠ¡æŒ‡æ ‡ï¼Œä»¥JSONæ ¼å¼è¿”å›ã€‚

è¦æ±‚ï¼š
1. åªæå–{year_emphasis}çš„æ•°æ®
2. è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼ˆå¿…é¡»åŒ…å«change_rateå’Œchange_directionå­—æ®µï¼‰ï¼š
{{
  "roe": {{"name": "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰", "value": "10.08%", "change_rate": "-1.30ä¸ªç™¾åˆ†ç‚¹", "change_direction": "ä¸‹é™", "is_missing": false}},
  "revenue": {{"name": "è¥ä¸šæ”¶å…¥", "value": "146,695ä¸‡å…ƒ", "change_rate": "-10.9%", "change_direction": "ä¸‹é™", "is_missing": false}},
  "net_profit": {{"name": "å‡€åˆ©æ¶¦", "value": "44,508ä¸‡å…ƒ", "change_rate": "-4.2%", "change_direction": "ä¸‹é™", "is_missing": false}},
  "total_assets": {{"name": "èµ„äº§æ€»é¢", "value": "5,000,000ä¸‡å…ƒ", "change_rate": "+3.7%", "change_direction": "å¢é•¿", "is_missing": false}},
  "net_interest_margin": {{"name": "å‡€æ¯å·®", "value": "1.87%", "change_rate": "-0.51ä¸ªç™¾åˆ†ç‚¹", "change_direction": "ä¸‹é™", "is_missing": false}},
  "cost_income_ratio": {{"name": "æˆæœ¬æ”¶å…¥æ¯”", "value": "27.66%", "change_rate": "-0.24ä¸ªç™¾åˆ†ç‚¹", "change_direction": "ä¸‹é™", "is_missing": false}}
}}

3. å¦‚æœæ‰¾ä¸åˆ°æŸä¸ªæŒ‡æ ‡ï¼Œè®¾ç½® "is_missing": true, "value": null
4. å¦‚æœæ‰¾ä¸åˆ°åŒæ¯”å¢å‡æ•°æ®ï¼Œchange_rateå’Œchange_directionå¯ä»¥è®¾ä¸ºnull
5. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—è¯´æ˜
6. ä¼˜å…ˆä»è¡¨æ ¼ä¸­æå–æ•°æ®ï¼Œç‰¹åˆ«æ³¨æ„"æœ¬å¹´åŒæ¯”å¢å‡"æˆ–"åŒæ¯”å¢å‡"åˆ—

æ–‡æ¡£å†…å®¹ï¼š
{all_context_text[:3000] if len(all_context_text) > 3000 else all_context_text}

è¯·è¿”å›JSONæ ¼å¼çš„æ•°æ®ï¼š"""
                
                try:
                    json_response = await llm.acomplete(json_prompt)
                    json_text = str(json_response).strip()
                    
                    # æå–JSONéƒ¨åˆ†
                    json_match = re.search(r'\{[\s\S]*\}', json_text)
                    if json_match:
                        import json
                        json_data = json.loads(json_match.group(0))
                        
                        # æ›´æ–°ç¼ºå¤±çš„æŒ‡æ ‡
                        for key in missing_keys:
                            if key in json_data and json_data[key]:
                                metric_data = json_data[key]
                                if isinstance(metric_data, dict) and not metric_data.get('is_missing'):
                                    snapshot_dict[key] = metric_data
                                    logger.info(f"  âœ… JSONæå–åˆ° {key}: {metric_data.get('value')}")
                except Exception as e:
                    logger.warning(f"  âŒ JSONæå–å¤±è´¥: {str(e)}")
            
            # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœè¿˜æœ‰ç¼ºå¤±çš„æŒ‡æ ‡ï¼Œè¿›è¡Œè¡¥å……æ£€ç´¢
            still_missing = [k for k in ["roe", "revenue", "net_profit", "total_assets", "net_interest_margin", "cost_income_ratio"] 
                           if snapshot_dict.get(k) is None]
            
            if still_missing and use_hybrid:
                logger.info(f"âš ï¸ ä»¥ä¸‹æŒ‡æ ‡ä»æœªæå–åˆ°ï¼Œè¿›è¡Œè¡¥å……æ£€ç´¢: {still_missing}")
                
                # ä¸ºæ¯ä¸ªç¼ºå¤±çš„æŒ‡æ ‡å•ç‹¬è¿›è¡Œè¡¥å……æ£€ç´¢
                indicator_keywords = {
                    "roe": ["åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡", "ROE"],
                    "revenue": ["è¥ä¸šæ”¶å…¥", "è¥ä¸šæ€»æ”¶å…¥"],
                    "net_profit": ["å‡€åˆ©æ¶¦", "å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦"],
                    "total_assets": ["èµ„äº§æ€»é¢", "æ€»èµ„äº§", "èµ„äº§åˆè®¡"],
                    "net_interest_margin": ["å‡€æ¯å·®"],
                    "cost_income_ratio": ["æˆæœ¬æ”¶å…¥æ¯”"]
                }
                
                supplement_contexts = []
                for missing_key in still_missing:
                    keywords = indicator_keywords.get(missing_key, [missing_key])
                    if year and company_name:
                        supplement_query = f"{company_name} {year}å¹´ {' '.join(keywords)} {year}å¹´åº¦æ•°å€¼"
                    elif year:
                        supplement_query = f"{year}å¹´ {' '.join(keywords)} {year}å¹´åº¦æ•°å€¼"
                    elif company_name:
                        supplement_query = f"{company_name} {' '.join(keywords)} æœ€æ–°å¹´åº¦æ•°å€¼"
                    else:
                        supplement_query = f"{' '.join(keywords)} æœ€æ–°å¹´åº¦æ•°å€¼"
                    
                    logger.info(f"  ğŸ” è¡¥å……æ£€ç´¢: {missing_key} ({keywords[0]})")
                    try:
                        supplement_results = rag_engine.hybrid_retriever.retrieve(
                            supplement_query,
                            top_k=20,
                            context_filter=context_filter if context_filter else None
                        )
                        
                        if supplement_results:
                            supplement_text = "\n\n".join([r['document'].text for r in supplement_results[:10]])
                            supplement_contexts.append(supplement_text)
                            logger.info(f"    âœ… {missing_key} è¡¥å……æ£€ç´¢åˆ° {len(supplement_results)} ä¸ªç»“æœ")
                        else:
                            logger.warning(f"    âš ï¸ {missing_key} è¡¥å……æ£€ç´¢æœªæ‰¾åˆ°ç»“æœ")
                    except Exception as e:
                        logger.warning(f"    âŒ {missing_key} è¡¥å……æ£€ç´¢å¤±è´¥: {str(e)}")
                
                # å°†è¡¥å……çš„ä¸Šä¸‹æ–‡æ·»åŠ åˆ°all_context_text
                if supplement_contexts:
                    all_context_text = all_context_text + "\n\n" + "\n\n".join(supplement_contexts)
                    logger.info(f"âœ… è¡¥å……æ£€ç´¢åï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(all_context_text)}å­—ç¬¦")
                    
                    # å¯¹è¡¥å……çš„ä¸Šä¸‹æ–‡å†æ¬¡è¿›è¡Œæ­£åˆ™æå–
                    for missing_key in still_missing:
                        keywords = indicator_keywords.get(missing_key, [missing_key])
                        patterns = {
                            "roe": [r'åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡[|\s]+([\d,\.]+%?)', r'ROE[|\s]+([\d,\.]+%?)'],
                            "revenue": [r'è¥ä¸šæ”¶å…¥[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)', r'è¥ä¸šæ€»æ”¶å…¥[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)'],
                            "net_profit": [r'å½’å±äºæœ¬è¡Œè‚¡ä¸œçš„å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)', r'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)', r'å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)'],
                            "total_assets": [r'èµ„äº§æ€»é¢[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)', r'æ€»èµ„äº§[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)', r'èµ„äº§åˆè®¡[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)'],
                            "net_interest_margin": [r'å‡€æ¯å·®[|\s]+([\d,\.]+%?)'],
                            "cost_income_ratio": [r'æˆæœ¬æ”¶å…¥æ¯”[|\s]+([\d,\.]+%?)']
                        }
                        
                        key_patterns = patterns.get(missing_key, [])
                        for pattern in key_patterns:
                            matches = list(re.finditer(pattern, all_context_text, re.IGNORECASE | re.MULTILINE))
                            if matches:
                                best_match = matches[0]
                                value = best_match.group(1).strip()
                                if missing_key in ["roe", "net_interest_margin", "cost_income_ratio"] and not value.endswith('%'):
                                    value = value + '%'
                                
                                snapshot_dict[missing_key] = {
                                    "name": {
                                        "roe": "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰",
                                        "revenue": "è¥ä¸šæ”¶å…¥",
                                        "net_profit": "å‡€åˆ©æ¶¦",
                                        "total_assets": "èµ„äº§æ€»é¢",
                                        "net_interest_margin": "å‡€æ¯å·®",
                                        "cost_income_ratio": "æˆæœ¬æ”¶å…¥æ¯”"
                                    }.get(missing_key, missing_key),
                                    "value": value,
                                    "change_rate": None,
                                    "change_direction": None,
                                    "is_missing": False
                                }
                                logger.info(f"  âœ… è¡¥å……æ£€ç´¢åæ­£åˆ™æå–åˆ° {missing_key}: {value}")
                                break
            
            logger.info(f"âœ… æå–å®Œæˆï¼Œæœ€ç»ˆæå–åˆ°çš„æŒ‡æ ‡: {[k for k, v in snapshot_dict.items() if v is not None and (not isinstance(v, dict) or not v.get('is_missing'))]}")
            
        except Exception as e:
            logger.warning(f"ç»“æ„åŒ–æå–å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            import traceback
            logger.warning(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä¼˜å…ˆä»Excelè¡¨æ ¼æŸ¥è¯¢ï¼Œç„¶åä½¿ç”¨æ­£åˆ™æå–
            try:
                logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šä¼˜å…ˆæ£€ç´¢Excelè¡¨æ ¼...")
                
                # ä¼˜å…ˆæŸ¥è¯¢Excelè¡¨æ ¼ï¼Œå¼ºè°ƒå¹´ä»½
                if year:
                    excel_query = f"Excelè¡¨æ ¼ Excelæ–‡ä»¶ åˆ©æ¶¦è¡¨ èµ„äº§è´Ÿå€ºè¡¨ ç°é‡‘æµé‡è¡¨ {year}å¹´ åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ ROE è¥ä¸šæ”¶å…¥ å‡€åˆ©æ¶¦ èµ„äº§æ€»é¢ å‡€æ¯å·® æˆæœ¬æ”¶å…¥æ¯” {year}å¹´åº¦æ•°å€¼"
                else:
                    excel_query = "Excelè¡¨æ ¼ Excelæ–‡ä»¶ åˆ©æ¶¦è¡¨ èµ„äº§è´Ÿå€ºè¡¨ ç°é‡‘æµé‡è¡¨ åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ ROE è¥ä¸šæ”¶å…¥ å‡€åˆ©æ¶¦ èµ„äº§æ€»é¢ å‡€æ¯å·® æˆæœ¬æ”¶å…¥æ¯” æœ€æ–°å¹´åº¦æ•°å€¼"
                
                # å°è¯•ä»è¡¨æ ¼æ•°æ®ä¸­æ£€ç´¢ï¼ˆåº”ç”¨å…¬å¸è¿‡æ»¤ï¼‰
                try:
                    retriever = rag_engine.index.as_retriever(similarity_top_k=30)  # æ‰©å¤§æ£€ç´¢èŒƒå›´ä»¥ä¾¿è¿‡æ»¤
                    nodes = retriever.retrieve(excel_query)
                    
                    # åº”ç”¨å…¬å¸è¿‡æ»¤
                    if context_filter and 'company' in context_filter:
                        nodes = rag_engine._filter_nodes(nodes, context_filter)
                        logger.info(f"  âœ… åº”ç”¨å…¬å¸è¿‡æ»¤åï¼Œå‰©ä½™ {len(nodes)} ä¸ªèŠ‚ç‚¹")
                    
                    # æ‰‹åŠ¨è¿‡æ»¤è¡¨æ ¼æ•°æ®
                    table_nodes = [n for n in nodes if n.metadata.get('document_type') == 'table_data' or n.metadata.get('is_financial', False)]
                    
                    if table_nodes:
                        response_text = "\n".join([node.text for node in table_nodes[:10]])  # å¢åŠ è¡¨æ ¼æ•°é‡ï¼Œç¡®ä¿åŒ…å«èµ„äº§è´Ÿå€ºè¡¨
                        logger.info(f"  âœ… ä»Excelè¡¨æ ¼æ£€ç´¢åˆ° {len(table_nodes)} ä¸ªè¡¨æ ¼æ•°æ®ï¼ˆå·²åº”ç”¨å…¬å¸è¿‡æ»¤ï¼‰")
                        logger.info(f"  ğŸ“Š è¡¨æ ¼æ–‡æœ¬é•¿åº¦: {len(response_text)}å­—ç¬¦")
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«èµ„äº§æ€»é¢ç›¸å…³å…³é”®è¯
                        if 'èµ„äº§æ€»é¢' in response_text or 'æ€»èµ„äº§' in response_text or 'èµ„äº§åˆè®¡' in response_text:
                            logger.info(f"  âœ… è¡¨æ ¼ä¸­åŒ…å«èµ„äº§æ€»é¢ç›¸å…³æ•°æ®")
                        else:
                            logger.warning(f"  âš ï¸ è¡¨æ ¼ä¸­æœªæ‰¾åˆ°èµ„äº§æ€»é¢ç›¸å…³å…³é”®è¯")
                    elif nodes:
                        # å¦‚æœæ²¡æœ‰è¡¨æ ¼ï¼Œä½¿ç”¨æ‰€æœ‰æ£€ç´¢åˆ°çš„æ•°æ®
                        response_text = "\n".join([node.text for node in nodes[:3]])
                        logger.info(f"  âœ… ä»æ–‡æ¡£æ£€ç´¢åˆ°æ•°æ®ï¼ˆå·²åº”ç”¨å…¬å¸è¿‡æ»¤ï¼‰")
                    else:
                        # å›é€€åˆ°æ™®é€šæŸ¥è¯¢ï¼ˆåº”ç”¨å…¬å¸è¿‡æ»¤ï¼‰
                        if context_filter:
                            result = rag_engine.query(excel_query, context_filter)
                            response_text = result.get('answer', '').strip()
                        else:
                            response = rag_engine.query_engine.query(excel_query)
                            response_text = str(response).strip()
                except Exception as e:
                    logger.warning(f"è¡¨æ ¼æ£€ç´¢å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æ™®é€šæŸ¥è¯¢")
                    # å¦‚æœè¡¨æ ¼æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæŸ¥è¯¢ï¼ˆåº”ç”¨å…¬å¸è¿‡æ»¤ï¼‰
                    if context_filter:
                        result = rag_engine.query(excel_query, context_filter)
                        response_text = result.get('answer', '').strip()
                    else:
                        response = rag_engine.query_engine.query(excel_query)
                        response_text = str(response).strip()
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¿«é€Ÿæå–ï¼ˆå¢åŠ æ›´å¤šæ¨¡å¼ï¼ŒåŒ…æ‹¬è¡¨æ ¼æ ¼å¼ï¼‰
                patterns = {
                    "roe": [
                        # è¡¨æ ¼æ ¼å¼ï¼š| åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ | 10.08% | 11.38% |
                        r'åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡[|\s]+([\d,\.]+%?)',
                        r'åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡[ï¼š:]\s*([\d,\.]+%?)',
                        r'åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡\s+([\d,\.]+%?)',
                        r'ROE[|\s]+([\d,\.]+%?)',
                        r'ROE[ï¼š:]\s*([\d,\.]+%?)',
                        r'ROE\s+([\d,\.]+%?)',
                        r'å‡€èµ„äº§æ”¶ç›Šç‡[|\s]+([\d,\.]+%?)',
                        r'å‡€èµ„äº§æ”¶ç›Šç‡[ï¼š:]\s*([\d,\.]+%?)',
                        r'å‡€èµ„äº§æ”¶ç›Šç‡\s+([\d,\.]+%?)',
                    ],
                    "revenue": [
                        # è¡¨æ ¼æ ¼å¼ï¼š| è¥ä¸šæ”¶å…¥ | 146,695 | 164,699 |
                        r'è¥ä¸šæ”¶å…¥[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'è¥ä¸šæ”¶å…¥[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'è¥æ”¶[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'è¥ä¸šæ€»æ”¶å…¥[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'ä¸»è¥ä¸šåŠ¡æ”¶å…¥[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'æ”¶å…¥[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'è¥ä¸šæ”¶å…¥\s+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'è¥ä¸šæ€»æ”¶å…¥\s+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        # è¡¨æ ¼æ ¼å¼ï¼šè¥ä¸šæ”¶å…¥ | æ•°å€¼
                        r'è¥ä¸šæ”¶å…¥[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'è¥ä¸šæ€»æ”¶å…¥[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    ],
                    "net_profit": [
                        # è¡¨æ ¼æ ¼å¼ï¼š| å½’å±äºæœ¬è¡Œè‚¡ä¸œçš„å‡€åˆ©æ¶¦ | 44,508 | 46,455 |
                        r'å½’å±äºæœ¬è¡Œè‚¡ä¸œçš„å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å½’æ¯å‡€åˆ©æ¶¦[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å‡€åˆ©æ¶¦[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å½’æ¯å‡€åˆ©æ¶¦[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å‡€åˆ©æ¶¦\s+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'å½’æ¯å‡€åˆ©æ¶¦\s+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    ],
                    "total_assets": [
                        # è¡¨æ ¼æ ¼å¼ï¼š| èµ„äº§æ€»é¢ | 5,000,000 | 4,800,000 |
                        r'èµ„äº§æ€»é¢[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'æ€»èµ„äº§[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'èµ„äº§åˆè®¡[|\s]+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'èµ„äº§æ€»é¢[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'æ€»èµ„äº§[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'èµ„äº§åˆè®¡[ï¼š:]\s*([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'èµ„äº§æ€»é¢\s+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'æ€»èµ„äº§\s+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                        r'èµ„äº§åˆè®¡\s+([\d,\.]+[ä¸‡åƒç™¾åäº¿]?å…ƒ?)',
                    ],
                    "net_interest_margin": [
                        # è¡¨æ ¼æ ¼å¼ï¼š| å‡€æ¯å·® | 1.87% | 2.38% |
                        r'å‡€æ¯å·®[|\s]+([\d,\.]+%?)',
                        r'å‡€æ¯å·®[ï¼š:]\s*([\d,\.]+%?)',
                        r'å‡€æ¯å·®\s+([\d,\.]+%?)',
                        r'å‡€åˆ©æ¯æ”¶ç›Šç‡[|\s]+([\d,\.]+%?)',
                        r'å‡€åˆ©æ¯æ”¶ç›Šç‡[ï¼š:]\s*([\d,\.]+%?)',
                        r'å‡€åˆ©æ¯æ”¶ç›Šç‡\s+([\d,\.]+%?)',
                    ],
                    "cost_income_ratio": [
                        # è¡¨æ ¼æ ¼å¼ï¼š| æˆæœ¬æ”¶å…¥æ¯” | 27.66% | 27.90% |
                        r'æˆæœ¬æ”¶å…¥æ¯”[|\s]+([\d,\.]+%?)',
                        r'æˆæœ¬æ”¶å…¥æ¯”[ï¼š:]\s*([\d,\.]+%?)',
                        r'æˆæœ¬æ”¶å…¥æ¯”\s+([\d,\.]+%?)',
                        r'æˆæœ¬æ”¶å…¥æ¯”ç‡[|\s]+([\d,\.]+%?)',
                        r'æˆæœ¬æ”¶å…¥æ¯”ç‡[ï¼š:]\s*([\d,\.]+%?)',
                        r'æˆæœ¬æ”¶å…¥æ¯”ç‡\s+([\d,\.]+%?)',
                    ]
                }
                
                for key, pattern_list in patterns.items():
                    found = False
                    for pattern in pattern_list:
                        match = re.search(pattern, response_text, re.IGNORECASE | re.MULTILINE)
                        if match:
                            value = match.group(1).strip()
                            # ä¸ºç™¾åˆ†æ¯”æŒ‡æ ‡æ·»åŠ %ç¬¦å·ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                            if key in ["roe", "net_interest_margin", "cost_income_ratio"] and not value.endswith('%'):
                                value = value + '%'
                            # ä¸ºé‡‘é¢ç±»æŒ‡æ ‡æ·»åŠ å•ä½ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                            if key in ["revenue", "net_profit", "total_assets"] and not any(unit in value for unit in ['å…ƒ', 'ä¸‡å…ƒ', 'äº¿å…ƒ', 'åƒå…ƒ']):
                                # å¦‚æœæ•°å€¼å¾ˆå¤§ï¼ˆè¶…è¿‡1000ï¼‰ï¼Œå¯èƒ½æ˜¯ä¸‡å…ƒæˆ–äº¿å…ƒ
                                num_value = value.replace(',', '').replace('ï¼Œ', '')
                                try:
                                    num = float(num_value)
                                    if num >= 100000000:
                                        value = value + 'äº¿å…ƒ'
                                    elif num >= 10000:
                                        value = value + 'ä¸‡å…ƒ'
                                    else:
                                        value = value + 'å…ƒ'
                                except:
                                    pass
                            snapshot_dict[key] = {
                                "name": {
                                    "roe": "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ï¼ˆROEï¼‰",
                                    "revenue": "è¥ä¸šæ”¶å…¥",
                                    "net_profit": "å‡€åˆ©æ¶¦",
                                    "total_assets": "èµ„äº§æ€»é¢",
                                    "net_interest_margin": "å‡€æ¯å·®",
                                    "cost_income_ratio": "æˆæœ¬æ”¶å…¥æ¯”"
                                }.get(key, key),
                                "value": value,
                                "is_missing": False
                            }
                            logger.info(f"  âœ… æ­£åˆ™æå–åˆ° {key}: {value} (æ¨¡å¼: {pattern[:50]}...)")
                            found = True
                            break
                    if not found:
                        logger.warning(f"  âš ï¸ æœªæ‰¾åˆ° {key} çš„æ•°æ®")
                
                logger.info(f"âœ… å¤‡ç”¨æ–¹æ¡ˆæå–å®Œæˆ")
                
            except Exception as e2:
                logger.warning(f"å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(e2)}")
        
        # ========== ç¬¬äºŒé˜¶æ®µï¼šå¿«é€Ÿç”Ÿæˆç»“è®ºï¼ˆåŸºäºå·²æå–çš„æŒ‡æ ‡ï¼‰==========
        logger.info("ç¬¬äºŒé˜¶æ®µï¼šå¿«é€Ÿç”Ÿæˆç»“è®º...")
        
        # æ„å»ºç®€åŒ–çš„æŒ‡æ ‡æ‘˜è¦ï¼ˆä¸è®¡ç®—æ¯”ç‡ï¼ŒåŠ å¿«é€Ÿåº¦ï¼‰
        metrics_summary = []
        if snapshot_dict.get("roe"):
            roe = snapshot_dict["roe"]
            if isinstance(roe, dict) and not roe.get("is_missing"):
                metrics_summary.append(f"ROE: {roe.get('value', 'N/A')}")
        if snapshot_dict.get("revenue"):
            rev = snapshot_dict["revenue"]
            if isinstance(rev, dict) and not rev.get("is_missing"):
                metrics_summary.append(f"è¥ä¸šæ”¶å…¥: {rev.get('value', 'N/A')}")
        if snapshot_dict.get("net_profit"):
            profit = snapshot_dict["net_profit"]
            if isinstance(profit, dict) and not profit.get("is_missing"):
                metrics_summary.append(f"å‡€åˆ©æ¶¦: {profit.get('value', 'N/A')}")
        if snapshot_dict.get("total_assets"):
            assets = snapshot_dict["total_assets"]
            if isinstance(assets, dict) and not assets.get("is_missing"):
                metrics_summary.append(f"èµ„äº§æ€»é¢: {assets.get('value', 'N/A')}")
        if snapshot_dict.get("net_interest_margin"):
            nim = snapshot_dict["net_interest_margin"]
            if isinstance(nim, dict) and not nim.get("is_missing"):
                metrics_summary.append(f"å‡€æ¯å·®: {nim.get('value', 'N/A')}")
        if snapshot_dict.get("cost_income_ratio"):
            cir = snapshot_dict["cost_income_ratio"]
            if isinstance(cir, dict) and not cir.get("is_missing"):
                metrics_summary.append(f"æˆæœ¬æ”¶å…¥æ¯”: {cir.get('value', 'N/A')}")
        
        metrics_text = "\n".join(metrics_summary) if metrics_summary else "è´¢åŠ¡æ•°æ®ä¸è¶³"
        year_info = f"{year}å¹´" if year else "æœ€æ–°å¹´åº¦"
        
        # æ„å»ºä¼˜åŒ–çš„æç¤ºè¯ï¼ˆå¼ºè°ƒä½¿ç”¨Excelè¡¨æ ¼æ•°æ®å’Œå¹´ä»½ï¼‰
        verdict_prompt = f"""åŸºäºä»¥ä¸‹{year_info}è´¢åŠ¡æŒ‡æ ‡ï¼Œå¿«é€Ÿç”Ÿæˆä¸€å¥è¯æ ¸å¿ƒç»“è®ºï¼š

{metrics_text}

ã€é‡è¦æç¤ºã€‘
- è¿™äº›æ•°æ®æ¥è‡ªExcelè¡¨æ ¼ï¼Œæ˜¯å‡†ç¡®çš„è´¢åŠ¡æ•°æ®
- **è¿™äº›æ•°æ®æ˜¯{year_info}çš„æ•°æ®ï¼Œè¯·åŸºäº{year_info}çš„æ•°å€¼è¿›è¡Œåˆ†æ**
- è¯·åŸºäºè¿™äº›å…·ä½“æ•°å€¼è¿›è¡Œåˆ†æï¼Œä¸è¦çŒœæµ‹

è¦æ±‚ï¼š
1. åªè¾“å‡ºä¸€å¥è¯æ ¸å¿ƒç»“è®ºï¼Œä¸è¦è¯¦ç»†åˆ†æ
2. å¿…é¡»åŒ…å«ä¸‰ä¸ªç»´åº¦ï¼š
   - å…¬å¸é˜¶æ®µï¼ˆå¢é•¿/ç¨³æ€/ä¸‹è¡Œï¼‰- æ ¹æ®{year_info}è¥æ”¶å’Œå‡€åˆ©æ¶¦æ•°å€¼åˆ¤æ–­
   - èµšé’±è´¨é‡ï¼ˆåˆ©æ¶¦è´¨é‡/èµ„äº§è´¨é‡ï¼‰- æ ¹æ®{year_info}å‡€åˆ©æ¶¦å’Œèµ„äº§æ€»é¢åˆ¤æ–­
   - é£é™©çº§åˆ«ï¼ˆä½/ä¸­/é«˜ï¼‰- æ ¹æ®{year_info}è´¢åŠ¡æŒ‡æ ‡ç»¼åˆåˆ¤æ–­
3. æ ¼å¼ï¼šå…¬å¸å¤„äº[é˜¶æ®µ]é˜¶æ®µï¼Œ[èµšé’±è´¨é‡æè¿°]ï¼Œé£é™©çº§åˆ«[çº§åˆ«]
4. ä¸è¶…è¿‡60å­—
5. å¦‚æœæ•°æ®ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
6. å¿«é€Ÿåˆ¤æ–­ï¼ŒåŸºäºExcelè¡¨æ ¼ä¸­{year_info}çš„å…·ä½“æ•°å€¼

ç¤ºä¾‹ï¼šå…¬å¸å¤„äºå¢é•¿é˜¶æ®µï¼Œåˆ©æ¶¦è´¨é‡è‰¯å¥½ä½†ç°é‡‘è´¨é‡ä¸€èˆ¬ï¼Œé£é™©çº§åˆ«ä¸­ç­‰ã€‚"""
        
        try:
            # ä¼˜å…ˆä»Excelè¡¨æ ¼æ£€ç´¢ç›¸å…³æ•°æ®æ¥ç”Ÿæˆç»“è®º
            logger.info("ğŸ” æ£€ç´¢Excelè¡¨æ ¼æ•°æ®ç”¨äºç”Ÿæˆç»“è®º...")
            
            # å…ˆå°è¯•ä»è¡¨æ ¼ä¸­æ£€ç´¢ç›¸å…³æ•°æ®ï¼ˆåº”ç”¨å…¬å¸è¿‡æ»¤ï¼‰
            try:
                conclusion_retriever = rag_engine.index.as_retriever(similarity_top_k=30)  # æ‰©å¤§æ£€ç´¢èŒƒå›´ä»¥ä¾¿è¿‡æ»¤
                if year:
                    conclusion_query = f"{year}å¹´ è´¢åŠ¡æŒ‡æ ‡ ROE åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ è¥ä¸šæ”¶å…¥ å‡€åˆ©æ¶¦ èµ„äº§æ€»é¢ å‡€æ¯å·® æˆæœ¬æ”¶å…¥æ¯” è¶‹åŠ¿ å˜åŒ– {year}å¹´åº¦"
                else:
                    conclusion_query = "è´¢åŠ¡æŒ‡æ ‡ ROE åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ è¥ä¸šæ”¶å…¥ å‡€åˆ©æ¶¦ èµ„äº§æ€»é¢ å‡€æ¯å·® æˆæœ¬æ”¶å…¥æ¯” è¶‹åŠ¿ å˜åŒ–"
                all_conclusion_nodes = conclusion_retriever.retrieve(conclusion_query)
                
                # åº”ç”¨å…¬å¸è¿‡æ»¤
                if context_filter and 'company' in context_filter:
                    all_conclusion_nodes = rag_engine._filter_nodes(all_conclusion_nodes, context_filter)
                    logger.info(f"  âœ… åº”ç”¨å…¬å¸è¿‡æ»¤åï¼Œå‰©ä½™ {len(all_conclusion_nodes)} ä¸ªèŠ‚ç‚¹")
                
                # æ‰‹åŠ¨è¿‡æ»¤è¡¨æ ¼æ•°æ®
                conclusion_nodes = [n for n in all_conclusion_nodes if n.metadata.get('document_type') == 'table_data' or n.metadata.get('is_financial', False)]
                
                if conclusion_nodes:
                    conclusion_context = "\n\n".join([node.text for node in conclusion_nodes[:3]])
                    verdict_prompt = f"""{verdict_prompt}

ã€è¡¥å……çš„Excelè¡¨æ ¼æ•°æ®ã€‘
{conclusion_context}

è¯·ç»“åˆä¸Šè¿°Excelè¡¨æ ¼æ•°æ®å’Œè´¢åŠ¡æŒ‡æ ‡ï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„ç»“è®ºã€‚"""
                    logger.info(f"  âœ… å·²æ·»åŠ  {len(conclusion_nodes)} ä¸ªExcelè¡¨æ ¼ä¸Šä¸‹æ–‡")
            except Exception as e:
                logger.warning(f"æ£€ç´¢Excelè¡¨æ ¼æ•°æ®å¤±è´¥: {str(e)}")
            
            # ä½¿ç”¨rag_engine.queryç”Ÿæˆç»“è®ºï¼ˆå®ƒä¼šåº”ç”¨å…¬å¸è¿‡æ»¤ï¼‰
            if context_filter:
                result = rag_engine.query(verdict_prompt, context_filter)
                verdict_text = result.get('answer', '').strip()
            else:
                response_obj = rag_engine.query_engine.query(verdict_prompt)
                verdict_text = str(response_obj).strip()
            
            # æ¸…ç†ç»“è®ºæ–‡æœ¬
            verdict_text = re.sub(r'^(?:æ ¸å¿ƒç»“è®º[ï¼š:]|æ ¸å¿ƒç»“è®º\*\*[ï¼š:])\s*\*?\*?', '', verdict_text)
            verdict_text = re.sub(r'\*\*', '', verdict_text)
            
            # åªå–ç¬¬ä¸€å¥è¯
            if len(verdict_text) > 150:
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', verdict_text)
                if sentences and len(sentences[0]) > 15:
                    verdict_text = sentences[0].strip() + 'ã€‚'
                else:
                    verdict_text = verdict_text[:100] + '...'
            
            if not verdict_text or len(verdict_text) < 15:
                verdict_text = "æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´ç»“è®ºã€‚"
            
            # è§£æç»“è®ºï¼Œæå–ä¸‰ä¸ªç»´åº¦
            stage = None
            profit_quality = None
            risk_level = None
            
            if 'å¢é•¿' in verdict_text:
                stage = 'å¢é•¿'
            elif 'ç¨³æ€' in verdict_text or 'ç¨³å®š' in verdict_text:
                stage = 'ç¨³æ€'
            elif 'ä¸‹è¡Œ' in verdict_text or 'ä¸‹é™' in verdict_text:
                stage = 'ä¸‹è¡Œ'
            
            # æå–èµšé’±è´¨é‡
            profit_match = re.search(r'åˆ©æ¶¦è´¨é‡[^ï¼Œ,ã€‚ã€]+', verdict_text)
            cash_match = re.search(r'ç°é‡‘è´¨é‡[^ï¼Œ,ã€‚ã€]+', verdict_text)
            if profit_match and cash_match:
                profit_quality = profit_match.group(0) + 'ã€' + cash_match.group(0)
            elif profit_match:
                profit_quality = profit_match.group(0)
            elif cash_match:
                profit_quality = cash_match.group(0)
            elif 'åˆ©æ¶¦è´¨é‡å·®' in verdict_text:
                profit_quality = 'åˆ©æ¶¦è´¨é‡å·®'
            elif 'åˆ©æ¶¦è´¨é‡è‰¯å¥½' in verdict_text:
                profit_quality = 'åˆ©æ¶¦è´¨é‡è‰¯å¥½'
            elif 'ç°é‡‘è´¨é‡æ— æ³•è¯„ä¼°' in verdict_text:
                profit_quality = 'ç°é‡‘è´¨é‡æ— æ³•è¯„ä¼°'
            
            # æå–é£é™©çº§åˆ«
            if 'é£é™©çº§åˆ«ä½' in verdict_text or 'é£é™©ä½' in verdict_text:
                risk_level = 'ä½'
            elif 'é£é™©çº§åˆ«ä¸­' in verdict_text or 'é£é™©ä¸­ç­‰' in verdict_text or 'é£é™©çº§åˆ«ä¸­ç­‰' in verdict_text:
                risk_level = 'ä¸­'
            elif 'é£é™©çº§åˆ«é«˜' in verdict_text or 'é£é™©é«˜' in verdict_text:
                risk_level = 'é«˜'
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆç»“è®ºå¤±è´¥: {str(e)}")
            verdict_text = "æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´ç»“è®ºã€‚"
            stage = None
            profit_quality = None
            risk_level = None
        
        # æ„å»ºæœ€ç»ˆè¿”å›æ•°æ®
        overview_data = {
            "roe": snapshot_dict.get("roe"),
            "revenue": snapshot_dict.get("revenue"),
            "net_profit": snapshot_dict.get("net_profit"),
            "total_assets": snapshot_dict.get("total_assets"),
            "net_interest_margin": snapshot_dict.get("net_interest_margin"),
            "cost_income_ratio": snapshot_dict.get("cost_income_ratio"),
            "verdict": verdict_text,
            "stage": stage,
            "profit_quality": profit_quality,
            "risk_level": risk_level,
            "missing_fields": []
        }
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡çš„çŠ¶æ€
        logger.info(f"ğŸ” æœ€ç»ˆè¿”å›æ•°æ®æ£€æŸ¥:")
        for key in ["roe", "revenue", "net_profit", "total_assets", "net_interest_margin", "cost_income_ratio"]:
            value = overview_data.get(key)
            if value:
                if isinstance(value, dict):
                    logger.info(f"  {key}: value={value.get('value')}, is_missing={value.get('is_missing')}")
                else:
                    logger.info(f"  {key}: {value}")
            else:
                logger.info(f"  {key}: None")
        
        # æ£€æŸ¥ç¼ºå¤±å­—æ®µ
        if not overview_data.get("revenue") or (isinstance(overview_data["revenue"], dict) and overview_data["revenue"].get("is_missing")):
            overview_data['missing_fields'].append('è¥ä¸šæ”¶å…¥')
        if not overview_data.get("net_profit") or (isinstance(overview_data["net_profit"], dict) and overview_data["net_profit"].get("is_missing")):
            overview_data['missing_fields'].append('å‡€åˆ©æ¶¦')
        if not overview_data.get("total_assets") or (isinstance(overview_data["total_assets"], dict) and overview_data["total_assets"].get("is_missing")):
            overview_data['missing_fields'].append('èµ„äº§æ€»é¢')
        if not stage:
            overview_data['missing_fields'].append('å…¬å¸é˜¶æ®µ')
        if not profit_quality:
            overview_data['missing_fields'].append('èµšé’±è´¨é‡')
        if not risk_level:
            overview_data['missing_fields'].append('é£é™©çº§åˆ«')
        
        logger.info(f"âœ… è´¢åŠ¡å¿«ç…§ç”ŸæˆæˆåŠŸ")
        
        return JSONResponse(status_code=200, content={
            "status": "success",
            "overview": overview_data
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç”Ÿæˆè´¢åŠ¡å¿«ç…§å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return JSONResponse(status_code=200, content={
            "status": "success",
            "overview": {
                "roe": None,
                "revenue": None,
                "net_profit": None,
                "total_assets": None,
                "verdict": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´ç»“è®ºã€‚",
                "stage": None,
                "profit_quality": None,
                "risk_level": None,
                "missing_fields": ["æ‰€æœ‰å­—æ®µ"]
            }
        })

class ComprehensiveAnalysisRequest(BaseModel):
    """ç»¼åˆèƒ½åŠ›åˆ†æè¯·æ±‚"""
    selected_cards: List[Dict[str, Any]] = Field(description="é€‰ä¸­çš„å¯è§†åŒ–å¡ç‰‡åˆ—è¡¨")
    overview_data: Optional[Dict[str, Any]] = Field(default=None, description="è´¢åŠ¡æ¦‚å†µæ•°æ®")
    context_filter: Optional[Dict[str, Any]] = None

@router.post("/comprehensive-analysis")
async def generate_comprehensive_analysis(request: ComprehensiveAnalysisRequest):
    """
    ç”Ÿæˆç»¼åˆèƒ½åŠ›åˆ†æé›·è¾¾å›¾
    
    åŸºäºé€‰ä¸­çš„å¯è§†åŒ–å¡ç‰‡ï¼Œæå–4ä¸ªæ ¸å¿ƒæŒ‡æ ‡å¹¶ç”Ÿæˆé›·è¾¾å›¾ï¼š
    1. ç›ˆåˆ©èƒ½åŠ›ï¼šROE
    2. è¿è¥èƒ½åŠ›ï¼šæ€»èµ„äº§å‘¨è½¬ç‡
    3. æˆé•¿èƒ½åŠ›ï¼šè¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡
    4. ç°é‡‘èƒ½åŠ›ï¼šç»è¥æ´»åŠ¨ç°é‡‘æµ/å‡€åˆ©æ¶¦
    
    Returns:
        åŒ…å«é›·è¾¾å›¾é…ç½®çš„å¯è§†åŒ–å“åº”
    """
    try:
        logger.info("æ”¶åˆ°ç»¼åˆèƒ½åŠ›åˆ†æè¯·æ±‚")
        
        # è·å–RAGå¼•æ“
        rag_engine = get_rag_engine()
        
        if not rag_engine.query_engine:
            if not rag_engine.load_existing_index():
                raise HTTPException(
                    status_code=400,
                    detail="ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
                )
        
        # ä»é€‰ä¸­çš„å¡ç‰‡ä¸­æå–å·²æœ‰æŒ‡æ ‡
        existing_metrics = {}
        for card in request.selected_cards:
            question = card.get('question', '')
            # å°è¯•ä»é—®é¢˜ä¸­è¯†åˆ«æŒ‡æ ‡
            if 'ROE' in question or 'å‡€èµ„äº§æ”¶ç›Šç‡' in question:
                existing_metrics['roe'] = card
            elif 'è¥ä¸šæ”¶å…¥' in question:
                existing_metrics['revenue'] = card
            elif 'å‡€åˆ©æ¶¦' in question:
                existing_metrics['net_profit'] = card
            elif 'èµ„äº§' in question and 'æ€»é¢' in question:
                existing_metrics['total_assets'] = card
        
        # æå–4ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼ˆå·²å–æ¶ˆå¿å€ºèƒ½åŠ›ï¼‰
        # ä¼˜å…ˆä½¿ç”¨è´¢åŠ¡æ¦‚å†µæ•°æ®
        metrics = await _extract_core_metrics(
            rag_engine,
            existing_metrics,
            request.context_filter,
            request.overview_data
        )
        
        # è®¡ç®—è¯„åˆ†
        scores = _calculate_ability_scores(metrics)
        
        # ç”Ÿæˆé›·è¾¾å›¾é…ç½®
        radar_chart = _generate_radar_chart(scores, metrics)
        
        # ç”Ÿæˆèƒ½åŠ›è§£é‡Šæ–‡æœ¬
        analysis_text = _generate_ability_analysis(scores, metrics)
        
        # æ„å»ºå¯è§†åŒ–å“åº”
        visualization_response = {
            "has_visualization": True,
            "chart_config": radar_chart,
            "analysis_text": analysis_text,
            "scores": scores,
            "metrics": metrics
        }
        
        logger.info("âœ… ç»¼åˆèƒ½åŠ›åˆ†æç”ŸæˆæˆåŠŸ")
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "visualization": visualization_response
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç”Ÿæˆç»¼åˆèƒ½åŠ›åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"ç”Ÿæˆç»¼åˆèƒ½åŠ›åˆ†æå¤±è´¥: {str(e)}"
        )


# ==================== ç»¼åˆèƒ½åŠ›åˆ†æè¾…åŠ©å‡½æ•° ====================

async def _extract_core_metrics(rag_engine, existing_metrics: Dict, context_filter: Optional[Dict] = None, overview_data: Optional[Dict] = None) -> Dict[str, Any]:
    """
    æå–4ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼ˆå·²å–æ¶ˆå¿å€ºèƒ½åŠ›ï¼‰
    
    Returns:
        {
            'roe': {'value': 10.5, 'unit': '%', 'source': 'existing'},
            'asset_turnover': {'value': 0.8, 'unit': '', 'source': 'retrieved'},
            'revenue_growth': {'value': 15.2, 'unit': '%', 'source': 'retrieved'},
            'cash_profit_ratio': {'value': 1.1, 'unit': '', 'source': 'retrieved'}
        }
    """
    metrics = {}
    
    # 1. ROE - ç›ˆåˆ©èƒ½åŠ›ï¼ˆä¼˜å…ˆä½¿ç”¨æ–‡æ¡£æŠ«éœ²å€¼ï¼Œç¼ºå¤±åˆ™è®¡ç®—ï¼‰
    roe_value = None
    roe_source = None
    
    # ä¼˜å…ˆçº§1: ä»æ–‡æ¡£æ£€ç´¢ï¼ˆåŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡/ROE/å‡€èµ„äº§æ”¶ç›Šç‡ï¼‰
    roe_value = await _retrieve_metric(rag_engine, "åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡ ROE å‡€èµ„äº§æ”¶ç›Šç‡", context_filter)
    if roe_value is not None:
        roe_source = 'document'
        logger.info(f"âœ… ä»æ–‡æ¡£æ£€ç´¢ROE: {roe_value}%")
    else:
        logger.info("âŒ æœªä»æ–‡æ¡£æ£€ç´¢åˆ°ROEï¼Œå°è¯•è®¡ç®—ROE")
    
    # ä¼˜å…ˆçº§2: æ–‡æ¡£ç¼ºå¤±æ—¶å°è¯•è®¡ç®—ï¼ˆå‡€åˆ©æ¶¦ / è‚¡ä¸œæƒç›Šï¼‰
    if roe_value is None:
        net_profit_for_roe = await _retrieve_metric(
            rag_engine,
            "å‡€åˆ©æ¶¦ å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦ å½’æ¯å‡€åˆ©æ¶¦",
            context_filter
        )
        equity_for_roe = await _retrieve_metric(
            rag_engine,
            "è‚¡ä¸œæƒç›Š æ‰€æœ‰è€…æƒç›Š å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…æƒç›Š",
            context_filter
        )
        
        if net_profit_for_roe is not None and equity_for_roe is not None and equity_for_roe != 0:
            roe_value = (net_profit_for_roe / equity_for_roe) * 100
            roe_source = 'calculated'
            logger.info(
                f"âœ… è®¡ç®—ROE: {roe_value:.2f}% (å‡€åˆ©æ¶¦: {net_profit_for_roe}, è‚¡ä¸œæƒç›Š: {equity_for_roe})"
            )
        else:
            logger.warning(
                f"âŒ æ— æ³•è®¡ç®—ROEï¼ˆå‡€åˆ©æ¶¦: {net_profit_for_roe}, è‚¡ä¸œæƒç›Š: {equity_for_roe}ï¼‰"
            )
    
    # å…œåº•ï¼šå¦‚æœä»ç¼ºå¤±ï¼Œå°è¯•è´¢åŠ¡æ¦‚å†µæˆ–å·²æœ‰å¡ç‰‡
    if roe_value is None and overview_data and overview_data.get('roe'):
        roe_obj = overview_data['roe']
        if isinstance(roe_obj, dict) and not roe_obj.get('is_missing'):
            roe_value_str = roe_obj.get('value', '')
            if roe_value_str and roe_value_str != 'â€”':
                roe_value = _parse_metric_value(roe_value_str)
                roe_source = 'overview'
                logger.info(f"âœ… ä»è´¢åŠ¡æ¦‚å†µè·å–ROE: {roe_value}%")
    
    if roe_value is None and 'roe' in existing_metrics:
        roe_value = _extract_value_from_card(existing_metrics['roe'], ['ROE', 'å‡€èµ„äº§æ”¶ç›Šç‡', 'åŠ æƒå¹³å‡å‡€èµ„äº§æ”¶ç›Šç‡'])
        if roe_value is not None:
            roe_source = 'existing_card'
            logger.info(f"âœ… ä»å·²æœ‰å¡ç‰‡è·å–ROE: {roe_value}%")
    
    metrics['roe'] = {'value': roe_value, 'unit': '%', 'source': roe_source}
    print(f"ğŸ“Š [æŒ‡æ ‡æå–] ROE: {roe_value}% (æ¥æº: {roe_source})")
    
    # 2. æ€»èµ„äº§å‘¨è½¬ç‡ - è¿è¥èƒ½åŠ›
    asset_turnover = await _retrieve_metric(rag_engine, "æ€»èµ„äº§å‘¨è½¬ç‡ èµ„äº§å‘¨è½¬ç‡", context_filter)
    metrics['asset_turnover'] = {'value': asset_turnover, 'unit': '', 'source': 'retrieved'}
    print(f"ğŸ“Š [æŒ‡æ ‡æå–] æ€»èµ„äº§å‘¨è½¬ç‡: {asset_turnover} (æ¥æº: retrieved)")
    
    # 3. è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡ - æˆé•¿èƒ½åŠ›
    revenue_growth = None
    revenue_growth_source = None
    
    # ä¼˜å…ˆçº§1: ç›´æ¥æ£€ç´¢åŒæ¯”å¢é•¿ç‡
    revenue_growth = await _retrieve_metric(rag_engine, "è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡ è¥ä¸šæ”¶å…¥å¢é•¿ç‡ åŒæ¯” å¢é•¿", context_filter)
    if revenue_growth is not None:
        revenue_growth_source = 'retrieved_direct'
        logger.info(f"âœ… ç›´æ¥æ£€ç´¢åˆ°è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡: {revenue_growth}%")
        print(f"ğŸ“Š [æŒ‡æ ‡æå–] è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡: {revenue_growth}% (æ¥æº: ç›´æ¥æ£€ç´¢)")
    else:
        # ä¼˜å…ˆçº§2: ä»è´¢åŠ¡æ¦‚å†µè·å–å½“å‰å¹´è¥ä¸šæ”¶å…¥ï¼Œç„¶åæ£€ç´¢ä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥è®¡ç®—å¢é•¿ç‡
        current_revenue = None
        previous_revenue = None
        
        # ä»è´¢åŠ¡æ¦‚å†µè·å–å½“å‰å¹´è¥ä¸šæ”¶å…¥
        if overview_data and overview_data.get('revenue'):
            revenue_obj = overview_data['revenue']
            if isinstance(revenue_obj, dict) and not revenue_obj.get('is_missing'):
                revenue_value_str = revenue_obj.get('value', '')
                if revenue_value_str and revenue_value_str != 'â€”':
                    current_revenue = _parse_metric_value(revenue_value_str)
                    logger.info(f"âœ… ä»è´¢åŠ¡æ¦‚å†µè·å–å½“å‰å¹´è¥ä¸šæ”¶å…¥: {current_revenue}")
                    print(f"   ğŸ“Š å½“å‰å¹´è¥ä¸šæ”¶å…¥: {current_revenue}")
        
        # å¦‚æœè¿˜æ²¡æœ‰å½“å‰å¹´æ•°æ®ï¼Œæ£€ç´¢å½“å‰å¹´è¥ä¸šæ”¶å…¥
        if current_revenue is None:
            current_revenue = await _retrieve_metric(rag_engine, "è¥ä¸šæ”¶å…¥ è¥ä¸šæ€»æ”¶å…¥ æœ€æ–°å¹´åº¦ æœ¬å¹´", context_filter)
            if current_revenue:
                logger.info(f"âœ… æ£€ç´¢åˆ°å½“å‰å¹´è¥ä¸šæ”¶å…¥: {current_revenue}")
                print(f"   ğŸ“Š å½“å‰å¹´è¥ä¸šæ”¶å…¥: {current_revenue}")
        
        # æ£€ç´¢ä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥
        if current_revenue is not None:
            # å°è¯•å¤šç§æ–¹å¼æ£€ç´¢ä¸Šä¸€å¹´æ•°æ®
            previous_revenue = None
            
            # æ–¹å¼1: ç›´æ¥æ£€ç´¢ä¸Šä¸€å¹´
            previous_revenue = await _retrieve_metric(rag_engine, "è¥ä¸šæ”¶å…¥ è¥ä¸šæ€»æ”¶å…¥ ä¸Šä¸€å¹´ å»å¹´ å‰ä¸€å¹´ ä¸Šå¹´", context_filter)
            
            # æ–¹å¼2: å¦‚æœæ–¹å¼1å¤±è´¥ï¼Œå°è¯•ä»è¡¨æ ¼ä¸­æå–ï¼ˆé€šå¸¸åˆ©æ¶¦è¡¨ä¼šæœ‰å¤šåˆ—æ•°æ®ï¼‰
            if previous_revenue is None:
                # æ„å»ºæŸ¥è¯¢ï¼Œè¦æ±‚è¿”å›ä¸¤å¹´çš„æ•°æ®
                growth_query = "è¥ä¸šæ”¶å…¥ è¥ä¸šæ€»æ”¶å…¥ åˆ©æ¶¦è¡¨ æœ€è¿‘ä¸¤å¹´ å†å²æ•°æ®"
                if context_filter:
                    growth_result = rag_engine.query(growth_query, context_filter)
                    growth_answer = growth_result.get('answer', '')
                    growth_sources = growth_result.get('sources', [])
                else:
                    growth_response = rag_engine.query_engine.query(growth_query)
                    growth_answer = str(growth_response)
                    growth_sources = []
                
                # ä»å›ç­”æˆ–æ¥æºä¸­æå–ä¸¤å¹´çš„æ•°æ®
                # æŸ¥æ‰¾æ‰€æœ‰è¥ä¸šæ”¶å…¥æ•°å€¼ï¼Œå–ç¬¬äºŒå¤§çš„ä½œä¸ºä¸Šä¸€å¹´ï¼ˆå‡è®¾æœ€å¤§çš„å½“å‰å¹´ï¼‰
                import re
                all_revenue_values = []
                
                # ä»sourcesä¸­æå–
                for source in growth_sources:
                    if isinstance(source, dict):
                        source_text = source.get('text', '')
                        # æŸ¥æ‰¾åŒ…å«è¥ä¸šæ”¶å…¥çš„è¡Œ
                        for line in source_text.split('\n'):
                            if 'è¥ä¸šæ”¶å…¥' in line or 'è¥ä¸šæ€»æ”¶å…¥' in line:
                                # æå–æ‰€æœ‰æ•°å€¼
                                numbers = re.findall(r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)', line)
                                for num_str in numbers:
                                    try:
                                        num = float(num_str.replace(',', '').replace('ï¼Œ', ''))
                                        if not (2000 <= abs(num) <= 2030) and abs(num) > 0.01:
                                            all_revenue_values.append(num)
                                    except:
                                        pass
                
                # ä»å›ç­”ä¸­æå–
                numbers = re.findall(r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)\s*[ä¸‡åƒç™¾åäº¿]?å…ƒ', growth_answer)
                for num_str in numbers:
                    try:
                        num = float(num_str.replace(',', '').replace('ï¼Œ', ''))
                        if not (2000 <= abs(num) <= 2030) and abs(num) > 0.01:
                            all_revenue_values.append(num)
                    except:
                        pass
                
                if len(all_revenue_values) >= 2:
                    # æ’åºï¼Œå–ç¬¬äºŒå¤§çš„ä½œä¸ºä¸Šä¸€å¹´
                    all_revenue_values = sorted([abs(v) for v in all_revenue_values], reverse=True)
                    # å‡è®¾å½“å‰å¹´è¥ä¸šæ”¶å…¥æ˜¯æœ€å¤§çš„ï¼Œä¸Šä¸€å¹´æ˜¯ç¬¬äºŒå¤§çš„
                    if abs(current_revenue) == all_revenue_values[0]:
                        previous_revenue = all_revenue_values[1] if len(all_revenue_values) > 1 else None
                    else:
                        # å¦‚æœå½“å‰å¹´ä¸æ˜¯æœ€å¤§çš„ï¼Œå–ç¬¬äºŒå¤§çš„
                        previous_revenue = all_revenue_values[1] if len(all_revenue_values) > 1 else all_revenue_values[0]
                    
                    if previous_revenue:
                        logger.info(f"âœ… ä»å†å²æ•°æ®ä¸­æå–åˆ°ä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥: {previous_revenue}")
                        print(f"   ğŸ“Š ä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥: {previous_revenue} (ä»å†å²æ•°æ®æå–)")
            
            if previous_revenue is not None and previous_revenue != 0:
                # è®¡ç®—åŒæ¯”å¢é•¿ç‡
                revenue_growth = ((current_revenue - previous_revenue) / previous_revenue) * 100
                revenue_growth_source = 'calculated'
                logger.info(f"âœ… è®¡ç®—è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡: {revenue_growth:.2f}% (å½“å‰: {current_revenue}, ä¸Šå¹´: {previous_revenue})")
                print(f"ğŸ“Š [æŒ‡æ ‡æå–] è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡: {revenue_growth:.2f}% (æ¥æº: è®¡ç®—)")
                print(f"   è¯¦ç»†: å½“å‰å¹´={current_revenue}, ä¸Šä¸€å¹´={previous_revenue}, å¢é•¿ç‡={revenue_growth:.2f}%")
            else:
                logger.warning(f"âŒ æ— æ³•è·å–ä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥ï¼Œæ— æ³•è®¡ç®—å¢é•¿ç‡")
                print(f"   âš ï¸ æ— æ³•è·å–ä¸Šä¸€å¹´è¥ä¸šæ”¶å…¥ï¼Œæ— æ³•è®¡ç®—å¢é•¿ç‡")
        else:
            logger.warning(f"âŒ æ— æ³•è·å–å½“å‰å¹´è¥ä¸šæ”¶å…¥")
            print(f"   âš ï¸ æ— æ³•è·å–å½“å‰å¹´è¥ä¸šæ”¶å…¥")
    
    metrics['revenue_growth'] = {'value': revenue_growth, 'unit': '%', 'source': revenue_growth_source or 'missing'}
    if revenue_growth is None:
        print(f"ğŸ“Š [æŒ‡æ ‡æå–] è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡: ç¼ºå¤±")
    
    # 4. ç»è¥æ´»åŠ¨ç°é‡‘æµ/å‡€åˆ©æ¶¦ - ç°é‡‘èƒ½åŠ›
    # ä¼˜å…ˆä»è´¢åŠ¡æ¦‚å†µè·å–å‡€åˆ©æ¶¦
    net_profit = None
    net_profit_source = None
    
    if overview_data and overview_data.get('net_profit'):
        net_profit_obj = overview_data['net_profit']
        if isinstance(net_profit_obj, dict) and not net_profit_obj.get('is_missing'):
            net_profit_str = net_profit_obj.get('value', '')
            if net_profit_str and net_profit_str != 'â€”':
                net_profit = _parse_metric_value(net_profit_str)
                net_profit_source = 'overview'
                logger.info(f"âœ… ä»è´¢åŠ¡æ¦‚å†µè·å–å‡€åˆ©æ¶¦: {net_profit}")
                print(f"   ğŸ“Š å‡€åˆ©æ¶¦: {net_profit} (æ¥æº: è´¢åŠ¡æ¦‚å†µ)")
    
    if net_profit is None:
        net_profit = await _retrieve_metric(rag_engine, "å‡€åˆ©æ¶¦ å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦ å½’æ¯å‡€åˆ©æ¶¦", context_filter)
        net_profit_source = 'retrieved'
        logger.info(f"{'âœ…' if net_profit else 'âŒ'} ä»æ–‡æ¡£æ£€ç´¢å‡€åˆ©æ¶¦: {net_profit}")
        if net_profit:
            print(f"   ğŸ“Š å‡€åˆ©æ¶¦: {net_profit} (æ¥æº: æ–‡æ¡£æ£€ç´¢)")
        else:
            print(f"   âŒ å‡€åˆ©æ¶¦æ£€ç´¢å¤±è´¥")
    
    # ç»è¥æ´»åŠ¨ç°é‡‘æµ - ä½¿ç”¨å¤šä¸ªå…³é”®è¯ç»„åˆæ£€ç´¢
    cash_flow = None
    cash_flow_source = None
    
    # å°è¯•å¤šä¸ªæŸ¥è¯¢ç­–ç•¥
    cash_flow_queries = [
        "ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢",
        "ç»è¥æ´»åŠ¨ç°é‡‘æµ ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡",
        "ç°é‡‘æµé‡è¡¨ ç»è¥æ´»åŠ¨ ç°é‡‘æµé‡å‡€é¢",
        "ç°é‡‘æµé‡å‡€é¢ ç»è¥æ´»åŠ¨"
    ]
    
    for query in cash_flow_queries:
        cash_flow = await _retrieve_metric(rag_engine, query, context_filter)
        if cash_flow is not None:
            cash_flow_source = 'retrieved'
            logger.info(f"âœ… æ£€ç´¢åˆ°ç»è¥æ´»åŠ¨ç°é‡‘æµ: {cash_flow} (æŸ¥è¯¢: {query})")
            print(f"ğŸ“Š [æŒ‡æ ‡æå–] ç»è¥æ´»åŠ¨ç°é‡‘æµ: {cash_flow} (æ¥æº: æ–‡æ¡£æ£€ç´¢, æŸ¥è¯¢: {query})")
            break
    
    if cash_flow is None:
        logger.warning(f"âŒ æ‰€æœ‰æŸ¥è¯¢ç­–ç•¥éƒ½æœªèƒ½æ£€ç´¢åˆ°ç»è¥æ´»åŠ¨ç°é‡‘æµ")
        print(f"ğŸ“Š [æŒ‡æ ‡æå–] ç»è¥æ´»åŠ¨ç°é‡‘æµ: ç¼ºå¤± (æ‰€æœ‰æŸ¥è¯¢ç­–ç•¥éƒ½å¤±è´¥)")
        print(f"   âš ï¸ è¯·æ£€æŸ¥æ–‡æ¡£ä¸­æ˜¯å¦åŒ…å«ä»¥ä¸‹å…³é”®è¯ä¹‹ä¸€:")
        print(f"      - ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢")
        print(f"      - ç»è¥æ´»åŠ¨ç°é‡‘æµ")
        print(f"      - ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡")
    
    # è®¡ç®—ç°é‡‘æµ/å‡€åˆ©æ¶¦æ¯”ç‡
    if cash_flow is not None and net_profit is not None and net_profit != 0:
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ç°é‡‘æµå’Œå‡€åˆ©æ¶¦çš„å•ä½å·²ç»ä¸€è‡´ï¼ˆéƒ½æ˜¯å…ƒï¼‰
        # å¦‚æœå•ä½ä¸ä¸€è‡´ï¼Œéœ€è¦è½¬æ¢
        cash_ratio = cash_flow / net_profit
        metrics['cash_profit_ratio'] = {'value': cash_ratio, 'unit': '', 'source': 'calculated'}
        print(f"ğŸ“Š [æŒ‡æ ‡æå–] ç°é‡‘æµ/å‡€åˆ©æ¶¦: {cash_ratio:.2f} (æ¥æº: è®¡ç®—)")
        print(f"   è¯¦ç»†: ç°é‡‘æµ={cash_flow}, å‡€åˆ©æ¶¦={net_profit}, æ¯”ç‡={cash_ratio:.2f}")
    else:
        metrics['cash_profit_ratio'] = {'value': None, 'unit': '', 'source': 'missing'}
        print(f"ğŸ“Š [æŒ‡æ ‡æå–] ç°é‡‘æµ/å‡€åˆ©æ¶¦: ç¼ºå¤±")
        if cash_flow is None:
            print(f"   åŸå› : ç»è¥æ´»åŠ¨ç°é‡‘æµæ£€ç´¢å¤±è´¥")
        if net_profit is None:
            print(f"   åŸå› : å‡€åˆ©æ¶¦æ£€ç´¢å¤±è´¥")
        elif net_profit == 0:
            print(f"   åŸå› : å‡€åˆ©æ¶¦ä¸º0ï¼Œæ— æ³•è®¡ç®—æ¯”ç‡")
    
    # æ³¨æ„ï¼šå·²å–æ¶ˆå¿å€ºèƒ½åŠ›ç»´åº¦ï¼ˆèµ„äº§è´Ÿå€ºç‡ï¼‰
    
    print(f"\nğŸ“‹ [æŒ‡æ ‡æå–æ±‡æ€»]")
    print(f"  - ROE: {metrics['roe']['value']}% (æ¥æº: {metrics['roe']['source']})")
    print(f"  - æ€»èµ„äº§å‘¨è½¬ç‡: {metrics['asset_turnover']['value']} (æ¥æº: {metrics['asset_turnover']['source']})")
    print(f"  - è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡: {metrics['revenue_growth']['value']}% (æ¥æº: {metrics['revenue_growth']['source']})")
    print(f"  - ç°é‡‘æµ/å‡€åˆ©æ¶¦: {metrics['cash_profit_ratio']['value']} (æ¥æº: {metrics['cash_profit_ratio']['source']})")
    
    return metrics


def _parse_metric_value(value_str: str) -> Optional[float]:
    """è§£ææŒ‡æ ‡å€¼å­—ç¬¦ä¸²ï¼Œæå–æ•°å€¼"""
    try:
        if not value_str or value_str == 'â€”':
            return None
        # ç§»é™¤æ‰€æœ‰éæ•°å­—å­—ç¬¦ï¼ˆä¿ç•™å°æ•°ç‚¹å’Œè´Ÿå·ï¼‰
        import re
        # åŒ¹é…æ•°å­—ï¼ˆåŒ…æ‹¬å°æ•°å’Œç™¾åˆ†æ¯”ï¼‰
        match = re.search(r'([-+]?\d+\.?\d*)', str(value_str).replace(',', '').replace('ï¼Œ', ''))
        if match:
            return float(match.group(1))
        return None
    except Exception as e:
        logger.warning(f"è§£ææŒ‡æ ‡å€¼å¤±è´¥ {value_str}: {str(e)}")
        return None


def _extract_value_from_card(card: Dict, keywords: List[str]) -> Optional[float]:
    """ä»å·²æœ‰å¡ç‰‡ä¸­æå–æŒ‡æ ‡å€¼"""
    try:
        # æ£€æŸ¥å¡ç‰‡æ•°æ®
        card_data = card.get('data', {})
        question = card.get('question', '')
        
        # å°è¯•ä»å›¾è¡¨é…ç½®ä¸­æå–
        if card_data.get('chart_config'):
            chart_config = card_data['chart_config']
            # ä»tracesä¸­æå–æ•°å€¼
            for trace in chart_config.get('traces', []):
                if trace.get('y'):
                    values = trace['y']
                    if values and len(values) > 0:
                        # å–ç¬¬ä¸€ä¸ªå€¼
                        return float(values[0])
        
        # å°è¯•ä»é—®é¢˜æ–‡æœ¬ä¸­æå–
        for keyword in keywords:
            if keyword in question:
                # åœ¨é—®é¢˜ä¸­æŸ¥æ‰¾æ•°å€¼
                percent_match = re.search(r'([\d,\.]+)\s*%', question)
                if percent_match:
                    return float(percent_match.group(1).replace(',', ''))
                number_match = re.search(r'([\d,\.]+)', question)
                if number_match:
                    return float(number_match.group(1).replace(',', ''))
        
        return None
    except Exception as e:
        logger.warning(f"ä»å¡ç‰‡æå–å€¼å¤±è´¥: {str(e)}")
        return None


async def _retrieve_metric(rag_engine, query_keywords: str, context_filter: Optional[Dict] = None) -> Optional[float]:
    """ä»æ–‡æ¡£ä¸­æ£€ç´¢æŒ‡æ ‡å€¼ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒä»è¡¨æ ¼å’Œæ–‡æœ¬ä¸­æå–ï¼‰"""
    try:
        # æ„å»ºæ›´æ˜ç¡®çš„æŸ¥è¯¢é—®é¢˜
        query_question = f"{query_keywords}çš„å…·ä½“æ•°å€¼æ˜¯å¤šå°‘ï¼Ÿè¯·ç»™å‡ºå‡†ç¡®çš„æ•°å€¼å’Œå•ä½"
        
        if context_filter:
            result = rag_engine.query(query_question, context_filter)
            answer = result.get('answer', '')
            sources = result.get('sources', [])
        else:
            response = rag_engine.query_engine.query(query_question)
            answer = str(response)
            sources = []
        
        logger.info(f"ğŸ” æ£€ç´¢æŒ‡æ ‡ '{query_keywords}' - å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
        print(f"ğŸ” [æ£€ç´¢æŒ‡æ ‡] {query_keywords}")
        print(f"   å›ç­”é¢„è§ˆ: {answer[:300]}...")
        
        if sources:
            logger.info(f"ğŸ” æ¥æºæ•°é‡: {len(sources)}")
            print(f"   æ¥æºæ•°é‡: {len(sources)}")
            # è®°å½•æ¥æºé¢„è§ˆ
            for i, source in enumerate(sources[:2]):
                if isinstance(source, dict):
                    source_text = source.get('text', '')[:200]
                    metadata = source.get('metadata', {})
                    doc_type = metadata.get('document_type', 'unknown')
                    print(f"   æ¥æº{i+1} ({doc_type}): {source_text}...")
        
        # ä¼˜å…ˆä»sourcesä¸­æå–ï¼ˆç‰¹åˆ«æ˜¯è¡¨æ ¼æ•°æ®ï¼‰
        if sources:
            for source in sources:
                if isinstance(source, dict):
                    source_text = source.get('text', '')
                    metadata = source.get('metadata', {})
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼æ•°æ®
                    is_table = metadata.get('document_type') == 'table_data' or 'table' in str(metadata).lower()
                    
                    # å¯¹äºç»è¥ç°é‡‘æµï¼Œç‰¹åˆ«å…³æ³¨åŒ…å«ç›¸å…³å…³é”®è¯çš„æ¥æº
                    keywords_in_text = any(kw in source_text for kw in ['ç»è¥æ´»åŠ¨', 'ç°é‡‘æµé‡', 'ç°é‡‘æµ', 'ç°é‡‘æµé‡å‡€é¢'])
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯
                    query_keywords_list = query_keywords.split()
                    has_query_keywords = any(kw in source_text for kw in query_keywords_list)
                    
                    if is_table or keywords_in_text or has_query_keywords:
                        # ä»è¡¨æ ¼æ–‡æœ¬ä¸­æå–æ•°å€¼
                        # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„è¡Œ
                        lines = source_text.split('\n')
                        for line in lines:
                            # æ£€æŸ¥è¿™ä¸€è¡Œæ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯
                            line_has_keywords = any(kw in line for kw in query_keywords_list) or \
                                               any(kw in line for kw in ['ç»è¥æ´»åŠ¨', 'ç°é‡‘æµé‡', 'ç°é‡‘æµ', 'è¥ä¸šæ”¶å…¥', 'æ”¶å…¥', 'åŒæ¯”', 'å¢é•¿'])
                            
                            if line_has_keywords:
                                # å°è¯•ä»è¿™ä¸€è¡Œæå–æ•°å€¼
                                # åŒ¹é…å„ç§æ ¼å¼ï¼šæ•°å­—ã€å¸¦å•ä½çš„æ•°å­—ç­‰
                                
                                # å¯¹äºè¡¨æ ¼æ ¼å¼ï¼š| æŒ‡æ ‡å | 2024å¹´ | 2023å¹´ | æ•°å€¼ |
                                # æå–æ‰€æœ‰æ•°å€¼ï¼Œé€‰æ‹©æœ€å¤§çš„ï¼ˆé€šå¸¸æ˜¯ä¸»è¦æŒ‡æ ‡å€¼ï¼‰
                                table_patterns = [
                                    r'[|]\s*([-+]?\d+[,ï¼Œ]?\d*\.?\d*)\s*[|]',  # è¡¨æ ¼æ ¼å¼ï¼š| æ•°å€¼ |
                                    r'[|]\s*([-+]?\d+[,ï¼Œ]?\d*\.?\d*)\s*[ä¸‡åƒç™¾åäº¿]?å…ƒ',  # è¡¨æ ¼æ ¼å¼ï¼š| æ•°å€¼å…ƒ |
                                ]
                                
                                # å¯¹äºæ–‡æœ¬æ ¼å¼
                                text_patterns = [
                                    r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)\s*[ä¸‡åƒç™¾åäº¿]å…ƒ',  # å¸¦å•ä½çš„é‡‘é¢
                                    r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)\s*%',  # ç™¾åˆ†æ¯”
                                    r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)',  # çº¯æ•°å­—
                                ]
                                
                                all_patterns = table_patterns + text_patterns
                                
                                for pattern in all_patterns:
                                    matches = re.findall(pattern, line)
                                    if matches:
                                        # æå–æ‰€æœ‰æ•°å€¼
                                        values = []
                                        for m in matches:
                                            try:
                                                v_str = m.replace(',', '').replace('ï¼Œ', '')
                                                v = float(v_str)
                                                # æ’é™¤å¹´ä»½ã€é¡µç ç­‰
                                                if not (2000 <= abs(v) <= 2030) and abs(v) > 0.01:
                                                    values.append(v)
                                            except:
                                                pass
                                        
                                        if values:
                                            # å–ç»å¯¹å€¼æœ€å¤§çš„æ•°å€¼ï¼ˆé€šå¸¸æ˜¯ä¸»è¦æŒ‡æ ‡å€¼ï¼‰
                                            max_value = max([abs(v) for v in values])
                                            # æ¢å¤ç¬¦å·
                                            for v in values:
                                                if abs(v) == max_value:
                                                    logger.info(f"âœ… ä»è¡¨æ ¼æ¥æºæå–åˆ°æ•°å€¼: {v} (è¡Œ: {line[:100]}...)")
                                                    print(f"   âœ… ä»è¡¨æ ¼æå–: {v} (åŒ¹é…è¡Œ: {line[:80]}...)")
                                                    return v
        
        # ä»å›ç­”ä¸­æå–æ•°å€¼
        # åŒ¹é…å¸¦å•ä½çš„é‡‘é¢ï¼šå¦‚ "1,234,567ä¸‡å…ƒ"ã€"123.45äº¿å…ƒ"
        amount_patterns = [
            r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)\s*([ä¸‡åƒç™¾åäº¿]å…ƒ)',  # å¸¦å•ä½çš„é‡‘é¢
            r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)\s*å…ƒ',  # å¸¦"å…ƒ"çš„é‡‘é¢
        ]
        for pattern in amount_patterns:
            match = re.search(pattern, answer)
            if match:
                value_str = match.group(1).replace(',', '').replace('ï¼Œ', '')
                unit = match.group(2) if len(match.groups()) > 1 else ''
                value = float(value_str)
                # å•ä½è½¬æ¢
                if 'äº¿' in unit:
                    value = value * 100000000
                elif 'ä¸‡' in unit:
                    value = value * 10000
                elif 'åƒ' in unit:
                    value = value * 1000
                logger.info(f"âœ… ä»å›ç­”ä¸­æå–åˆ°æ•°å€¼ï¼ˆå¸¦å•ä½ï¼‰: {value} ({unit})")
                print(f"   âœ… ä»å›ç­”æå–ï¼ˆå¸¦å•ä½ï¼‰: {value} ({unit})")
                return value
        
        # åŒ¹é…ç™¾åˆ†æ¯”ï¼š10.5%ã€10.5
        percent_match = re.search(r'([-+]?\d+[,ï¼Œ]?\.?\d*)\s*%', answer)
        if percent_match:
            value_str = percent_match.group(1).replace(',', '').replace('ï¼Œ', '')
            logger.info(f"âœ… ä»å›ç­”ä¸­æå–åˆ°ç™¾åˆ†æ¯”: {value_str}%")
            print(f"   âœ… ä»å›ç­”æå–ï¼ˆç™¾åˆ†æ¯”ï¼‰: {value_str}%")
            return float(value_str)
        
        # åŒ¹é…æ™®é€šæ•°å€¼ï¼ˆå–æœ€å¤§çš„æ•°å€¼ï¼Œé€šå¸¸æ˜¯ä¸»è¦æŒ‡æ ‡å€¼ï¼‰
        number_matches = re.findall(r'([-+]?\d+[,ï¼Œ]?\d*\.?\d*)', answer)
        if number_matches:
            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æŒ‡æ ‡å€¼çš„æ•°å­—ï¼ˆå¦‚å¹´ä»½ã€é¡µç ç­‰ï¼‰
            values = []
            for match in number_matches:
                value_str = match.replace(',', '').replace('ï¼Œ', '')
                try:
                    v = float(value_str)
                    # æ’é™¤å¹´ä»½ï¼ˆ2000-2030ï¼‰ã€é¡µç ç­‰
                    if not (2000 <= abs(v) <= 2030) and abs(v) > 0.01:
                        values.append(v)
                except:
                    pass
            
            if values:
                # å–ç»å¯¹å€¼æœ€å¤§çš„ï¼ˆé€šå¸¸æ˜¯ä¸»è¦æŒ‡æ ‡å€¼ï¼‰
                max_value = max([abs(v) for v in values])
                for v in values:
                    if abs(v) == max_value:
                        logger.info(f"âœ… ä»å›ç­”ä¸­æå–åˆ°æ•°å€¼: {v}")
                        print(f"   âœ… ä»å›ç­”æå–: {v}")
                        return v
        
        logger.warning(f"âŒ æœªèƒ½ä»å›ç­”ä¸­æå–åˆ°æ•°å€¼: {answer[:200]}...")
        print(f"   âŒ æœªèƒ½æå–æ•°å€¼")
        return None
    except Exception as e:
        logger.warning(f"æ£€ç´¢æŒ‡æ ‡å¤±è´¥ {query_keywords}: {str(e)}")
        import traceback
        logger.warning(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        print(f"   âŒ æ£€ç´¢å¤±è´¥: {str(e)}")
        return None


def _calculate_ability_scores(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ ¹æ®æŒ‡æ ‡å€¼è®¡ç®—èƒ½åŠ›è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰
    
    Returns:
        {
            'profitability': {'score': 75, 'level': 'ä¸­é«˜'},
            'operation': {'score': 60, 'level': 'æ­£å¸¸'},
            'growth': {'score': 80, 'level': 'é«˜æˆé•¿'},
            'cash': {'score': 70, 'level': 'åŸºæœ¬åŒ¹é…'},
            'debt': {'score': 65, 'level': 'åˆç†'}
        }
    """
    scores = {}
    
    # 1. ç›ˆåˆ©èƒ½åŠ› - ROE
    roe_value = metrics.get('roe', {}).get('value')
    if roe_value is not None:
        if roe_value >= 15:
            score = 80 + min(20, (roe_value - 15) * 2)  # 15-25% æ˜ å°„åˆ° 80-100
        elif roe_value >= 10:
            score = 60 + (roe_value - 10) * 4  # 10-15% æ˜ å°„åˆ° 60-80
        elif roe_value >= 5:
            score = 40 + (roe_value - 5) * 4  # 5-10% æ˜ å°„åˆ° 40-60
        else:
            score = max(0, 40 * (roe_value / 5))  # 0-5% æ˜ å°„åˆ° 0-40
        scores['profitability'] = {'score': min(100, max(0, score)), 'value': roe_value}
    else:
        scores['profitability'] = {'score': 50, 'value': None}  # ç¼ºå¤±æ•°æ®è®¾ä¸ºä¸­æ€§å€¼
    
    # 2. è¿è¥èƒ½åŠ› - æ€»èµ„äº§å‘¨è½¬ç‡
    turnover_value = metrics.get('asset_turnover', {}).get('value')
    if turnover_value is not None:
        if turnover_value >= 1.2:
            score = 80 + min(20, (turnover_value - 1.2) * 25)  # â‰¥1.2 æ˜ å°„åˆ° 80-100
        elif turnover_value >= 0.8:
            score = 60 + (turnover_value - 0.8) * 50  # 0.8-1.2 æ˜ å°„åˆ° 60-80
        elif turnover_value >= 0.5:
            score = 40 + (turnover_value - 0.5) * 66.67  # 0.5-0.8 æ˜ å°„åˆ° 40-60
        else:
            score = max(0, 40 * (turnover_value / 0.5))  # <0.5 æ˜ å°„åˆ° 0-40
        scores['operation'] = {'score': min(100, max(0, score)), 'value': turnover_value}
    else:
        scores['operation'] = {'score': 50, 'value': None}
    
    # 3. æˆé•¿èƒ½åŠ› - è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡
    growth_value = metrics.get('revenue_growth', {}).get('value')
    if growth_value is not None:
        if growth_value >= 20:
            score = 80 + min(20, (growth_value - 20) * 1)  # â‰¥20% æ˜ å°„åˆ° 80-100
        elif growth_value >= 10:
            score = 60 + (growth_value - 10) * 2  # 10-20% æ˜ å°„åˆ° 60-80
        elif growth_value >= 0:
            score = 40 + growth_value * 2  # 0-10% æ˜ å°„åˆ° 40-60
        else:
            score = max(0, 40 + growth_value * 4)  # è´Ÿå¢é•¿ æ˜ å°„åˆ° 0-40
        scores['growth'] = {'score': min(100, max(0, score)), 'value': growth_value}
    else:
        scores['growth'] = {'score': 50, 'value': None}
    
    # 4. ç°é‡‘èƒ½åŠ› - ç»è¥æ´»åŠ¨ç°é‡‘æµ/å‡€åˆ©æ¶¦
    cash_ratio_value = metrics.get('cash_profit_ratio', {}).get('value')
    if cash_ratio_value is not None:
        if cash_ratio_value >= 1.2:
            score = 80 + min(20, (cash_ratio_value - 1.2) * 50)  # â‰¥1.2 æ˜ å°„åˆ° 80-100
        elif cash_ratio_value >= 0.8:
            score = 60 + (cash_ratio_value - 0.8) * 50  # 0.8-1.2 æ˜ å°„åˆ° 60-80
        elif cash_ratio_value >= 0.5:
            score = 40 + (cash_ratio_value - 0.5) * 66.67  # 0.5-0.8 æ˜ å°„åˆ° 40-60
        else:
            score = max(0, 40 * (cash_ratio_value / 0.5))  # <0.5 æ˜ å°„åˆ° 0-40
        scores['cash'] = {'score': min(100, max(0, score)), 'value': cash_ratio_value}
    else:
        scores['cash'] = {'score': 50, 'value': None}
    
    # æ³¨æ„ï¼šå·²å–æ¶ˆå¿å€ºèƒ½åŠ›ç»´åº¦
    
    return scores


def _generate_radar_chart(scores: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç”Ÿæˆé›·è¾¾å›¾é…ç½®ï¼ˆPlotlyæ ¼å¼ï¼‰
    """
    # èƒ½åŠ›ç»´åº¦æ ‡ç­¾ï¼ˆå·²å–æ¶ˆå¿å€ºèƒ½åŠ›ï¼‰
    categories = ['ç›ˆåˆ©èƒ½åŠ›', 'è¿è¥èƒ½åŠ›', 'æˆé•¿èƒ½åŠ›', 'ç°é‡‘èƒ½åŠ›']
    
    # è·å–å„ç»´åº¦åˆ†æ•°
    values = [
        scores.get('profitability', {}).get('score', 50),
        scores.get('operation', {}).get('score', 50),
        scores.get('growth', {}).get('score', 50),
        scores.get('cash', {}).get('score', 50)
    ]
    
    # ä¸ºäº†é—­åˆé›·è¾¾å›¾ï¼Œéœ€è¦é‡å¤ç¬¬ä¸€ä¸ªå€¼
    categories_closed = categories + [categories[0]]
    values_closed = values + [values[0]]
    
    # æ„å»ºPlotlyé›·è¾¾å›¾é…ç½®
    chart_config = {
        "chart_type": "radar",
        "traces": [
            {
                "name": "ç»¼åˆèƒ½åŠ›",
                "type": "scatterpolar",
                "r": values_closed,
                "theta": categories_closed,
                "fill": "toself",
                "mode": "lines+markers",
                "line": {"color": "rgb(55, 128, 191)", "width": 2},
                "marker": {"size": 8, "color": "rgb(55, 128, 191)"}
            }
        ],
        "layout": {
            "title": "ç»¼åˆèƒ½åŠ›åˆ†æé›·è¾¾å›¾",
            "polar": {
                "radialaxis": {
                    "visible": True,
                    "range": [0, 100],
                    "tickmode": "linear",
                    "tick0": 0,
                    "dtick": 20,
                    "tickvals": [0, 20, 40, 60, 80, 100],
                    "ticktext": ["0", "20", "40", "60", "80", "100"],
                    "gridcolor": "#e0e0e0",
                    "linecolor": "#999"
                },
                "angularaxis": {
                    "rotation": 90,
                    "direction": "counterclockwise"
                }
            },
            "height": 500,
            "showlegend": False,
            "template": "plotly_white"
        }
    }
    
    return chart_config


def _generate_ability_analysis(scores: Dict[str, Any], metrics: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆèƒ½åŠ›åˆ†ææ–‡æœ¬
    """
    # è®¡ç®—å¹³å‡åˆ†ï¼ˆå·²å–æ¶ˆå¿å€ºèƒ½åŠ›ï¼‰
    avg_score = sum([
        scores.get('profitability', {}).get('score', 50),
        scores.get('operation', {}).get('score', 50),
        scores.get('growth', {}).get('score', 50),
        scores.get('cash', {}).get('score', 50)
    ]) / 4
    
    # æ ¹æ®å¹³å‡åˆ†ç¡®å®šæ•´ä½“è¯„ä»·
    if avg_score >= 80:
        overall = "èƒ½åŠ›è¡¨ç°è¾ƒå¼º"
    elif avg_score >= 60:
        overall = "èƒ½åŠ›ä¿æŒç¨³å®š"
    elif avg_score >= 40:
        overall = "èƒ½åŠ›æ‰¿å‹"
    else:
        overall = "èƒ½åŠ›é£é™©è¾ƒé«˜"
    
    analysis = f"**ç»¼åˆèƒ½åŠ›è¯„ä»·ï¼š{overall}**\n\n"
    
    # å„ç»´åº¦ä¸€å¥è¯åˆ†æ
    profitability_score = scores.get('profitability', {}).get('score', 50)
    if profitability_score >= 80:
        profitability_desc = "ç›ˆåˆ©èƒ½åŠ›çªå‡º"
    elif profitability_score >= 60:
        profitability_desc = "ç›ˆåˆ©èƒ½åŠ›è‰¯å¥½"
    elif profitability_score >= 40:
        profitability_desc = "ç›ˆåˆ©èƒ½åŠ›ä¸€èˆ¬"
    else:
        profitability_desc = "ç›ˆåˆ©èƒ½åŠ›åå¼±"
    analysis += f"- **ç›ˆåˆ©èƒ½åŠ›**ï¼š{profitability_desc}\n"
    
    operation_score = scores.get('operation', {}).get('score', 50)
    if operation_score >= 80:
        operation_desc = "è¿è¥æ•ˆç‡è¾ƒé«˜"
    elif operation_score >= 60:
        operation_desc = "è¿è¥æ•ˆç‡æ­£å¸¸"
    elif operation_score >= 40:
        operation_desc = "è¿è¥æ•ˆç‡åä½"
    else:
        operation_desc = "è¿è¥æ•ˆç‡è¾ƒå¼±"
    analysis += f"- **è¿è¥èƒ½åŠ›**ï¼š{operation_desc}\n"
    
    growth_score = scores.get('growth', {}).get('score', 50)
    if growth_score >= 80:
        growth_desc = "æˆé•¿èƒ½åŠ›å¼ºåŠ²"
    elif growth_score >= 60:
        growth_desc = "æˆé•¿èƒ½åŠ›ç¨³å¥"
    elif growth_score >= 40:
        growth_desc = "æˆé•¿èƒ½åŠ›æ”¾ç¼“"
    else:
        growth_desc = "æˆé•¿èƒ½åŠ›æ‰¿å‹"
    analysis += f"- **æˆé•¿èƒ½åŠ›**ï¼š{growth_desc}\n"
    
    cash_score = scores.get('cash', {}).get('score', 50)
    if cash_score >= 80:
        cash_desc = "ç°é‡‘è´¨é‡ä¼˜ç§€"
    elif cash_score >= 60:
        cash_desc = "ç°é‡‘è´¨é‡è‰¯å¥½"
    elif cash_score >= 40:
        cash_desc = "ç°é‡‘è´¨é‡ä¸€èˆ¬"
    else:
        cash_desc = "ç°é‡‘è´¨é‡å­˜åœ¨é£é™©"
    analysis += f"- **ç°é‡‘èƒ½åŠ›**ï¼š{cash_desc}\n"
    
    return analysis
