"""
æ–‡æ¡£å¤„ç†APIæ¥å£
"""

from pathlib import Path
from typing import Dict, Any, Optional
from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import logging

from core.document_processor import DocumentProcessor
from core.table_extractor import TableExtractor
from core.rag_engine import RAGEngine

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/process", tags=["process"])

# å…¨å±€å¤„ç†å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
document_processor = None
table_extractor = None
rag_engine = None

def get_processors():
    """è·å–å¤„ç†å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global document_processor, table_extractor, rag_engine

    if document_processor is None:
        document_processor = DocumentProcessor()
    if table_extractor is None:
        table_extractor = TableExtractor()
    if rag_engine is None:
        rag_engine = RAGEngine()

    return document_processor, table_extractor, rag_engine

class ProcessRequest(BaseModel):
    filename: str
    build_index: bool = True

class ProcessMultipleRequest(BaseModel):
    filenames: list[str]
    build_index: bool = True

@router.post("/file")
async def process_file(request: ProcessRequest):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶
    
    Args:
        request: å¤„ç†è¯·æ±‚
        
    Returns:
        å¤„ç†ç»“æœ
    """
    try:
        # è·å–å¤„ç†å™¨å®ä¾‹
        document_processor, table_extractor, rag_engine = get_processors()

        filename = request.filename
        build_index = request.build_index

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_path = Path("uploads") / filename
        if not file_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")

        # éªŒè¯æ–‡ä»¶
        if not document_processor.validate_file(str(file_path)):
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶éªŒè¯å¤±è´¥: {filename}")

        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")

        # å¤„ç†æ–‡æ¡£
        doc_result = document_processor.process_file(str(file_path), filename)

        # æå–è¡¨æ ¼
        processed_docs = {filename: doc_result}
        extracted_tables = table_extractor.extract_tables(processed_docs)
        
        # æ„å»ºç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼Œé»˜è®¤ä½¿ç”¨å¢é‡æ¨¡å¼ï¼‰
        index_built = False
        if build_index:
            try:
                logger.info("ğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
                logger.info(f"   æ–‡æ¡£æ•°: {len(processed_docs)}")
                logger.info(f"   è¡¨æ ¼æ•°: {sum(len(tables) for tables in extracted_tables.values())}")
                
                # é»˜è®¤ä½¿ç”¨å¢é‡æ¨¡å¼ï¼Œåªç´¢å¼•æ–°æ–‡ä»¶ï¼Œä¿ç•™å·²æœ‰ç´¢å¼•
                index_built = rag_engine.build_index(processed_docs, extracted_tables, incremental=True)
                
                if index_built:
                    index_stats = rag_engine.get_index_stats()
                    logger.info(f"âœ… ç´¢å¼•æ„å»ºæˆåŠŸ!")
                    logger.info(f"   çŠ¶æ€: {index_stats.get('status', 'unknown')}")
                    logger.info(f"   æ–‡æ¡£æ•°: {index_stats.get('document_count', 0)}")
                    logger.info(f"   å‘é‡æ•°: {index_stats.get('vector_count', 0)}")
                    logger.info(f"   å­˜å‚¨ç›®å½•: {index_stats.get('storage_dir', 'unknown')}")
                else:
                    logger.error("âŒ ç´¢å¼•æ„å»ºå¤±è´¥: build_index è¿”å› False")
                    logger.error("   è¯·æ£€æŸ¥æ—¥å¿—ä»¥è·å–è¯¦ç»†ä¿¡æ¯")
            except Exception as e:
                logger.error(f"âŒ ç´¢å¼•æ„å»ºå¼‚å¸¸: {str(e)}", exc_info=True)
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä½†è®°å½•è¯¦ç»†é”™è¯¯
        
        # ç”Ÿæˆå¤„ç†æ‘˜è¦
        doc_summary = document_processor.get_document_summary(doc_result.get('documents', []))
        table_stats = table_extractor.get_table_statistics(extracted_tables)

        # å°†Documentå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        serializable_doc_result = {
            'filename': doc_result['filename'],
            'documents': [
                {
                    'doc_id': doc.doc_id if hasattr(doc, 'doc_id') else None,
                    'text': doc.text[:500] + "..." if len(doc.text) > 500 else doc.text,  # æˆªæ–­é•¿æ–‡æœ¬
                    'metadata': doc.metadata if hasattr(doc, 'metadata') else {},
                    'text_length': len(doc.text) if hasattr(doc, 'text') else 0
                } for doc in doc_result.get('documents', [])
            ],
            'page_count': doc_result.get('page_count', 0),
            'total_text_length': doc_result.get('total_text_length', 0),
            'processing_method': doc_result.get('processing_method', 'unknown')
        }
        
        # åªæœ‰PDFæ–‡ä»¶æ‰æœ‰detailed_content
        if 'detailed_content' in doc_result:
            serializable_doc_result['detailed_content'] = doc_result['detailed_content']
        
        # Excelæ–‡ä»¶å¯èƒ½æœ‰sheet_count
        if 'sheet_count' in doc_result:
            serializable_doc_result['sheet_count'] = doc_result['sheet_count']

        # ç¡®å®špage_countï¼ˆExcelæ–‡ä»¶ä½¿ç”¨sheet_countï¼‰
        page_count = doc_result.get('page_count', 0)
        if 'sheet_count' in doc_result:
            page_count = doc_result.get('sheet_count', 0)
        
        result = {
            "message": "æ–‡ä»¶å¤„ç†å®Œæˆ",
            "filename": filename,
            "processing_summary": {
                "document_info": {
                    "page_count": page_count,
                    "total_text_length": doc_result.get('total_text_length', 0),
                    "processing_method": doc_result.get('processing_method', 'unknown')
                },
                "document_summary": doc_summary,
                "table_info": {
                    "total_tables": table_stats['total_tables'],
                    "financial_tables": table_stats['financial_tables'],
                    "average_importance": table_stats['average_importance']
                },
                "index_info": {
                    "index_built": index_built,
                    "index_stats": rag_engine.get_index_stats() if (index_built and rag_engine) else None
                }
            },
            "detailed_results": {
                "document_data": serializable_doc_result,
                "extracted_tables": extracted_tables[filename] if filename in extracted_tables else []
            }
        }
        
        logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {filename}")
        return JSONResponse(status_code=200, content=result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {request.filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}")

@router.post("/files")
async def process_multiple_files(request: ProcessMultipleRequest):
    """
    å¤„ç†å¤šä¸ªæ–‡ä»¶ï¼ˆæ”¯æŒPDFå’ŒExcelï¼‰
    
    Args:
        request: æ‰¹é‡å¤„ç†è¯·æ±‚
        
    Returns:
        æ‰¹é‡å¤„ç†ç»“æœ
    """
    try:
        # è·å–å¤„ç†å™¨å®ä¾‹
        document_processor, table_extractor, rag_engine = get_processors()
        
        filenames = request.filenames
        build_index = request.build_index
        
        if not filenames:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æŒ‡å®šæ–‡ä»¶")
        
        if len(filenames) > 10:
            raise HTTPException(status_code=400, detail="ä¸€æ¬¡æœ€å¤šå¤„ç†10ä¸ªæ–‡ä»¶")
        
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(filenames)} ä¸ªæ–‡ä»¶")
        
        results = []
        all_processed_docs = {}
        all_extracted_tables = {}
        failed_files = []
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for filename in filenames:
            try:
                file_path = Path("uploads") / filename
                if not file_path.exists():
                    error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}"
                    results.append({
                        "filename": filename,
                        "status": "error",
                        "message": error_msg
                    })
                    failed_files.append({"filename": filename, "error": error_msg})
                    logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                file_ext = file_path.suffix.lower()
                if file_ext not in {'.pdf', '.xlsx', '.xls'}:
                    error_msg = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
                    results.append({
                        "filename": filename,
                        "status": "error",
                        "message": error_msg
                    })
                    failed_files.append({"filename": filename, "error": error_msg})
                    logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
                    continue
                
                # éªŒè¯æ–‡ä»¶ï¼ˆå¯¹äºPDFå’ŒExceléƒ½æ”¯æŒï¼‰
                if not document_processor.validate_file(str(file_path)):
                    error_msg = f"æ–‡ä»¶éªŒè¯å¤±è´¥: {filename}"
                    results.append({
                        "filename": filename,
                        "status": "error",
                        "message": error_msg
                    })
                    failed_files.append({"filename": filename, "error": error_msg})
                    logger.error(f"æ–‡ä»¶éªŒè¯å¤±è´¥: {filename}")
                    continue
                
                logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {filename} (ç±»å‹: {file_ext})")
                
                # å¤„ç†æ–‡æ¡£ï¼ˆæ”¯æŒPDFå’ŒExcelï¼‰
                doc_result = document_processor.process_file(str(file_path), filename)
                all_processed_docs[filename] = doc_result
                
                # æå–è¡¨æ ¼ï¼ˆExcelæ–‡ä»¶å¯èƒ½å·²ç»åŒ…å«è¡¨æ ¼æ•°æ®ï¼‰
                processed_docs = {filename: doc_result}
                extracted_tables = table_extractor.extract_tables(processed_docs)
                all_extracted_tables.update(extracted_tables)
                
                # ç”Ÿæˆæ‘˜è¦
                doc_summary = document_processor.get_document_summary(doc_result.get('documents', []))
                table_stats = table_extractor.get_table_statistics(extracted_tables)
                
                # å¯¹äºExcelæ–‡ä»¶ï¼Œpage_countå¯èƒ½æ˜¯sheet_count
                page_count = doc_result.get('page_count', 0)
                if file_ext in {'.xlsx', '.xls'}:
                    # Excelæ–‡ä»¶ä½¿ç”¨sheet_count
                    page_count = doc_result.get('sheet_count', 0)
                
                results.append({
                    "filename": filename,
                    "status": "success",
                    "summary": {
                        "page_count": page_count,
                        "total_text_length": doc_result.get('total_text_length', 0),
                        "table_count": table_stats['total_tables'],
                        "financial_tables": table_stats['financial_tables']
                    }
                })
                
                logger.info(f"æ–‡ä»¶å¤„ç†æˆåŠŸ: {filename}")
                
            except Exception as e:
                error_msg = str(e)
                results.append({
                    "filename": filename,
                    "status": "error",
                    "message": error_msg
                })
                failed_files.append({"filename": filename, "error": error_msg})
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {filename}: {error_msg}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # æ„å»ºç»Ÿä¸€ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼ï¼‰
        index_built = False
        if build_index and all_processed_docs:
            try:
                logger.info("ğŸ”¨ å¼€å§‹æ„å»ºç»Ÿä¸€ç´¢å¼•ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
                logger.info(f"   æ–‡æ¡£æ•°: {len(all_processed_docs)}")
                logger.info(f"   è¡¨æ ¼æ•°: {sum(len(tables) for tables in all_extracted_tables.values())}")
                
                # ä½¿ç”¨å¢é‡æ¨¡å¼ï¼Œåªç´¢å¼•æ–°æ–‡ä»¶ï¼Œä¿ç•™å·²æœ‰ç´¢å¼•
                index_built = rag_engine.build_index(all_processed_docs, all_extracted_tables, incremental=True)
                
                if index_built:
                    index_stats = rag_engine.get_index_stats()
                    logger.info(f"âœ… ç»Ÿä¸€ç´¢å¼•æ„å»ºæˆåŠŸ!")
                    logger.info(f"   çŠ¶æ€: {index_stats.get('status', 'unknown')}")
                    logger.info(f"   æ–‡æ¡£æ•°: {index_stats.get('document_count', 0)}")
                    logger.info(f"   å‘é‡æ•°: {index_stats.get('vector_count', 0)}")
                else:
                    logger.warning("âš ï¸ ç»Ÿä¸€ç´¢å¼•æ„å»ºå¤±è´¥")
            except Exception as e:
                logger.error(f"ç»Ÿä¸€ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r["status"] == "success")
        error_count = len(results) - success_count
        
        # ç”Ÿæˆæ€»ä½“ç»Ÿè®¡
        total_table_stats = table_extractor.get_table_statistics(all_extracted_tables)
        
        # è®¡ç®—æ€»é¡µæ•°/å·¥ä½œè¡¨æ•°
        total_pages = sum(
            r.get("summary", {}).get("page_count", 0) 
            for r in results 
            if r["status"] == "success"
        )
        
        result = {
            "message": f"æ‰¹é‡å¤„ç†å®Œæˆ: {success_count} æˆåŠŸ, {error_count} å¤±è´¥",
            "total_files": len(results),
            "success_count": success_count,
            "error_count": error_count,
            "processing_summary": {
                "document_info": {
                    "page_count": total_pages,
                    "total_documents": len(all_processed_docs)
                },
                "table_info": {
                    "total_tables": total_table_stats['total_tables'],
                    "financial_tables": total_table_stats['financial_tables']
                },
                "index_info": {
                    "index_built": index_built,
                    "index_stats": rag_engine.get_index_stats() if (index_built and rag_engine) else None
                }
            },
            "file_results": results,
            "failed_files": failed_files
        }
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(filenames)} æˆåŠŸ")
        return JSONResponse(status_code=200, content=result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")

@router.get("/status")
async def get_processing_status():
    """
    è·å–å¤„ç†çŠ¶æ€
    
    Returns:
        å½“å‰å¤„ç†çŠ¶æ€
    """
    try:
        # è·å–ä¸Šä¼ æ–‡ä»¶æ•°é‡
        upload_dir = Path("uploads")
        uploaded_files = 0
        if upload_dir.exists():
            uploaded_files = len([f for f in upload_dir.iterdir() if f.is_file()])
        
        # è·å–ç´¢å¼•çŠ¶æ€ - å…ˆè·å–å¤„ç†å™¨å®ä¾‹
        index_stats = {
            "index_built": False,
            "status": "not_initialized",
            "message": "ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
        }
        try:
            _, _, rag_engine_instance = get_processors()
            if rag_engine_instance:
                stats = rag_engine_instance.get_index_stats()
                if stats:
                    # ç»Ÿä¸€è¿”å›æ ¼å¼
                    if stats.get('status') == 'ready':
                        index_stats = {
                            "index_built": True,
                            "status": "ready",
                            "document_count": stats.get('document_count', 0),
                            "vector_count": stats.get('vector_count', 0),
                            "storage_dir": stats.get('storage_dir', ''),
                            "collection_name": stats.get('collection_name', '')
                        }
                    elif stats.get('status') == 'not_initialized':
                        index_stats = {
                            "index_built": False,
                            "status": "not_initialized",
                            "message": "ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
                        }
                    else:
                        # å…¼å®¹æ—§æ ¼å¼ï¼Œå¦‚æœæœ‰ index_built å­—æ®µ
                        index_stats = {
                            "index_built": stats.get('index_built', False),
                            "status": stats.get('status', 'unknown'),
                            **stats
                        }
        except Exception as e:
            logger.warning(f"è·å–ç´¢å¼•çŠ¶æ€å¤±è´¥: {str(e)}")
            index_stats = {
                "index_built": False,
                "status": "error",
                "message": f"è·å–ç´¢å¼•çŠ¶æ€å¤±è´¥: {str(e)}"
            }
        
        status = {
            "system_status": "ready",
            "uploaded_files": uploaded_files,
            "index_status": index_stats,
            "supported_formats": [".pdf", ".xlsx", ".xls"],
            "max_file_size": "50MB",
            "max_batch_size": 10
        }
        
        return JSONResponse(status_code=200, content=status)
        
    except Exception as e:
        logger.error(f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")

@router.post("/rebuild-index")
async def rebuild_index():
    """
    é‡å»ºç´¢å¼•
    
    Returns:
        é‡å»ºç»“æœ
    """
    try:
        # è·å–å¤„ç†å™¨å®ä¾‹
        document_processor, table_extractor, rag_engine = get_processors()
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        if rag_engine:
            rag_engine.clear_index()
        
        # è·å–æ‰€æœ‰å·²å¤„ç†çš„æ–‡æ¡£ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»å­˜å‚¨ä¸­æ¢å¤ï¼‰
        upload_dir = Path("uploads")
        if not upload_dir.exists():
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æ‰¾åˆ°ä¸Šä¼ çš„æ–‡ä»¶")
        
        # é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶å¹¶æ„å»ºç´¢å¼•
        pdf_files = [f.name for f in upload_dir.iterdir() if f.suffix.lower() == '.pdf']
        
        if not pdf_files:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
        
        all_processed_docs = {}
        all_extracted_tables = {}
        
        for filename in pdf_files:
            try:
                file_path = upload_dir / filename
                
                # å¤„ç†æ–‡æ¡£
                doc_result = document_processor.process_file(str(file_path), filename)
                all_processed_docs[filename] = doc_result
                
                # æå–è¡¨æ ¼
                processed_docs = {filename: doc_result}
                extracted_tables = table_extractor.extract_tables(processed_docs)
                all_extracted_tables.update(extracted_tables)
                
            except Exception as e:
                logger.warning(f"é‡å»ºç´¢å¼•æ—¶å¤„ç†æ–‡ä»¶å¤±è´¥ {filename}: {str(e)}")
        
        # æ„å»ºç´¢å¼•
        if all_processed_docs and rag_engine:
            index_built = rag_engine.build_index(all_processed_docs, all_extracted_tables)
            
            if index_built:
                try:
                    index_stats = rag_engine.get_index_stats()
                except Exception as e:
                    logger.warning(f"è·å–ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {str(e)}")
                    index_stats = None
                    
                return JSONResponse(
                    status_code=200,
                    content={
                        "message": "ç´¢å¼•é‡å»ºæˆåŠŸ",
                        "processed_files": len(all_processed_docs),
                        "index_stats": index_stats
                    }
                )
            else:
                raise HTTPException(status_code=500, detail="ç´¢å¼•é‡å»ºå¤±è´¥")
        else:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰å¯å¤„ç†çš„æ–‡æ¡£æˆ–RAGå¼•æ“æœªåˆå§‹åŒ–")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")
