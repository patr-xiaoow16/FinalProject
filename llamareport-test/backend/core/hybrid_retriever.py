"""
Hybrid Retriever - æ··åˆæ£€ç´¢å™¨
åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ Ã— æŒ‡æ ‡åŠ æƒ Ã— å¹´ä»½è¿‡æ»¤çš„è´¢åŠ¡æ•°æ®æ£€ç´¢ç³»ç»Ÿ
"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from llama_index.core import Document, VectorStoreIndex, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.retrievers import VectorIndexRetriever
import chromadb

logger = logging.getLogger(__name__)

class HybridRetrievalScorer:
    """æ··åˆæ£€ç´¢è¯„åˆ†å™¨"""
    
    def __init__(self):
        # æƒé‡é…ç½®
        self.weights = {
            'semantic_similarity': 0.6,  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
            'metric_matching': 0.3,      # æŒ‡æ ‡åŒ¹é…åº¦æƒé‡
            'year_consistency': 0.1      # å¹´ä»½ä¸€è‡´æ€§æƒé‡
        }
        
        # è´¢åŠ¡æŒ‡æ ‡å…³é”®è¯åº“
        self.financial_metrics = [
            'å‡€åˆ©æ¶¦', 'ROE', 'ROA', 'è´Ÿå€ºç‡', 'èµ„äº§è´Ÿå€ºç‡', 'æµåŠ¨æ¯”ç‡',
            'è¥ä¸šæ”¶å…¥', 'è¥ä¸šåˆ©æ¶¦', 'æ¯›åˆ©ç‡', 'å‡€åˆ©ç‡', 'èµ„äº§å‘¨è½¬ç‡',
            'ç°é‡‘æµ', 'è‚¡ä¸œæƒç›Š', 'æ€»èµ„äº§', 'æ€»è´Ÿå€º', 'æ¯è‚¡æ”¶ç›Š',
            'å‡€èµ„äº§', 'æµåŠ¨èµ„äº§', 'éæµåŠ¨èµ„äº§', 'æµåŠ¨è´Ÿå€º', 'éæµåŠ¨è´Ÿå€º',
            'è¥ä¸šæˆæœ¬', 'é”€å”®è´¹ç”¨', 'ç®¡ç†è´¹ç”¨', 'è´¢åŠ¡è´¹ç”¨', 'ç ”å‘è´¹ç”¨'
        ]
        
        # è´¢åŠ¡æŒ‡æ ‡åŒä¹‰è¯
        self.financial_synonyms = {
            'å‡€åˆ©æ¶¦': ['å‡€åˆ©æ¶¦', 'ç›ˆä½™', 'æ”¶ç›Š', 'Profit', 'Earnings', 'å‡€åˆ©'],
            'ROE': ['ROE', 'å‡€èµ„äº§æ”¶ç›Šç‡', 'æƒç›Šå›æŠ¥ç‡', 'Return on Equity'],
            'è¥ä¸šæ”¶å…¥': ['è¥ä¸šæ”¶å…¥', 'è¥æ”¶', 'æ”¶å…¥', 'Revenue', 'Sales'],
            'èµ„äº§': ['èµ„äº§', 'Assets', 'æ€»èµ„äº§', 'å‡€èµ„äº§'],
            'è´Ÿå€º': ['è´Ÿå€º', 'Liabilities', 'æ€»è´Ÿå€º', 'å€ºåŠ¡']
        }
    
    def calculate_comprehensive_score(self, 
                                    query: str, 
                                    document: Document, 
                                    semantic_score: float) -> Dict[str, Any]:
        """
        è®¡ç®—ç»¼åˆè¯„åˆ†ï¼Œè´¢åŠ¡æŠ¥è¡¨æ–‡æ¡£ä¼šè·å¾—é¢å¤–åŠ åˆ†
        """
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦ (sim_score)
        sim_score = semantic_score
        
        # 2. æŒ‡æ ‡åŒ¹é…åº¦ (metric_score)
        metric_score = self._calculate_metric_score(query, document)
        
        # 3. å¹´ä»½ä¸€è‡´æ€§ (year_score)
        year_score = self._calculate_year_score(query, document)
        
        # 4. è´¢åŠ¡æŠ¥è¡¨åŠ åˆ†ï¼ˆå¦‚æœæ–‡æ¡£æ˜¯è´¢åŠ¡æŠ¥è¡¨ï¼Œç»™äºˆé¢å¤–æƒé‡ï¼‰
        financial_statement_bonus = 0.0
        if document.metadata.get('is_financial_statement', False):
            financial_statement_bonus = 0.2  # è´¢åŠ¡æŠ¥è¡¨é¢å¤–åŠ åˆ†20%
            # å¦‚æœæŸ¥è¯¢åŒ…å«è´¢åŠ¡æŠ¥è¡¨ç›¸å…³å…³é”®è¯ï¼Œé¢å¤–åŠ åˆ†
            query_lower = query.lower()
            statement_type = document.metadata.get('financial_statement_type', '')
            if statement_type:
                type_keywords = {
                    'åˆ©æ¶¦è¡¨': ['åˆ©æ¶¦', 'æ”¶å…¥', 'æˆæœ¬', 'profit', 'revenue', 'income'],
                    'èµ„äº§è´Ÿå€ºè¡¨': ['èµ„äº§', 'è´Ÿå€º', 'æƒç›Š', 'asset', 'liability', 'equity'],
                    'ç°é‡‘æµé‡è¡¨': ['ç°é‡‘æµ', 'ç°é‡‘', 'cash flow', 'cash']
                }
                if statement_type in type_keywords:
                    for keyword in type_keywords[statement_type]:
                        if keyword in query_lower:
                            financial_statement_bonus = 0.3  # åŒ¹é…æ—¶é¢å¤–åŠ åˆ†30%
                            break
        
        # 5. è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆè´¢åŠ¡æŠ¥è¡¨ä¼šè·å¾—é¢å¤–åŠ åˆ†ï¼‰
        base_score = (
            sim_score * self.weights['semantic_similarity'] +
            metric_score * self.weights['metric_matching'] +
            year_score * self.weights['year_consistency']
        )
        
        # åº”ç”¨è´¢åŠ¡æŠ¥è¡¨åŠ åˆ†ï¼ˆä½†ä¸è¶…è¿‡1.0ï¼‰
        comprehensive_score = min(1.0, base_score + financial_statement_bonus)
        
        return {
            'comprehensive_score': comprehensive_score,
            'sim_score': sim_score,
            'metric_score': metric_score,
            'year_score': year_score,
            'financial_statement_bonus': financial_statement_bonus,
            'weights': self.weights
        }
    
    def _calculate_metric_score(self, query: str, document: Document) -> float:
        """è®¡ç®—æŒ‡æ ‡åŒ¹é…åº¦"""
        query_lower = query.lower()
        doc_text = document.text.lower()
        
        # æ£€æŸ¥æŸ¥è¯¢ä¸­çš„è´¢åŠ¡æŒ‡æ ‡
        query_metrics = [metric for metric in self.financial_metrics 
                        if metric in query_lower]
        
        if not query_metrics:
            return 0.5  # ä¸­æ€§åˆ†æ•°
        
        # æ£€æŸ¥æ–‡æ¡£ä¸­æ˜¯å¦åŒ…å«è¿™äº›æŒ‡æ ‡
        matched_metrics = [metric for metric in query_metrics 
                          if metric in doc_text]
        
        # è®¡ç®—åŒ¹é…åº¦
        base_score = len(matched_metrics) / len(query_metrics)
        
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœæ–‡æ¡£æ˜¯è¡¨æ ¼ç±»å‹ä¸”åŒ…å«è´¢åŠ¡æŒ‡æ ‡
        if document.metadata.get('doc_type') == 'table' and matched_metrics:
            base_score += 0.2
        
        return min(base_score, 1.0)
    
    def _calculate_year_score(self, query: str, document: Document) -> float:
        """è®¡ç®—å¹´ä»½ä¸€è‡´æ€§"""
        # ä»æŸ¥è¯¢ä¸­æå–å¹´ä»½
        query_years = self._extract_years_from_text(query)
        
        if not query_years:
            return 0.0
        
        # ä»æ–‡æ¡£å…ƒæ•°æ®ä¸­è·å–å¹´ä»½
        doc_year = document.metadata.get('year')
        if not doc_year:
            return 0.0
        
        # æ£€æŸ¥å¹´ä»½åŒ¹é…
        if str(doc_year) in query_years:
            return 1.0
        else:
            return 0.0
    
    def _extract_years_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å¹´ä»½ä¿¡æ¯"""
        patterns = [
            r'(\d{4})-(\d{4})å¹´',  # 2020-2022å¹´
            r'(\d{4})å¹´',          # 2023å¹´
            r'(\d{4})åˆ°(\d{4})',   # 2020åˆ°2022
            r'(\d{4})è‡³(\d{4})',   # 2020è‡³2022
            r'è¿‘(\d)å¹´',           # è¿‘ä¸‰å¹´
        ]
        
        years = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) == 2:  # å¹´ä»½èŒƒå›´
                        start_year, end_year = match
                        years.extend(range(int(start_year), int(end_year) + 1))
                    elif len(match) == 1:  # è¿‘Nå¹´
                        n_years = int(match[0])
                        current_year = datetime.now().year
                        years.extend(range(current_year - n_years + 1, current_year + 1))
                else:
                    years.append(int(match))
        
        return [str(year) for year in sorted(set(years))]

class QueryExpansion:
    """æŸ¥è¯¢æ‰©å±•å™¨"""
    
    def __init__(self):
        self.financial_synonyms = {
            'å‡€åˆ©æ¶¦': ['å‡€åˆ©æ¶¦', 'ç›ˆä½™', 'æ”¶ç›Š', 'Profit', 'Earnings', 'å‡€åˆ©'],
            'ROE': ['ROE', 'å‡€èµ„äº§æ”¶ç›Šç‡', 'æƒç›Šå›æŠ¥ç‡', 'Return on Equity'],
            'è¥ä¸šæ”¶å…¥': ['è¥ä¸šæ”¶å…¥', 'è¥æ”¶', 'æ”¶å…¥', 'Revenue', 'Sales'],
            'èµ„äº§': ['èµ„äº§', 'Assets', 'æ€»èµ„äº§', 'å‡€èµ„äº§'],
            'è´Ÿå€º': ['è´Ÿå€º', 'Liabilities', 'æ€»è´Ÿå€º', 'å€ºåŠ¡'],
            'èµ„äº§æ€»é¢': ['èµ„äº§æ€»é¢', 'æ€»èµ„äº§', 'èµ„äº§åˆè®¡', 'Total Assets', 'Assets'],
            'æ¯›åˆ©ç‡': ['æ¯›åˆ©ç‡', 'Gross Margin', 'æ¯›åˆ©æ¶¦ç‡'],
            'å‡€åˆ©ç‡': ['å‡€åˆ©ç‡', 'Net Margin', 'å‡€åˆ©æ¶¦ç‡']
        }
    
    def expand_query(self, query: str) -> str:
        """æ‰©å±•æŸ¥è¯¢è¯"""
        expanded_terms = []
        
        for term, synonyms in self.financial_synonyms.items():
            if term in query:
                expanded_terms.extend(synonyms)
        
        if expanded_terms:
            return f"{query} {' '.join(expanded_terms)}"
        
        return query

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""
    
    def __init__(self, storage_dir: str = "./storage"):
        self.storage_dir = storage_dir
        self.scorer = HybridRetrievalScorer()
        self.query_expander = QueryExpansion()
        
        # åŒé€šé“ç´¢å¼•
        self.text_index = None
        self.table_index = None
        
        # ChromaDBå®¢æˆ·ç«¯
        self.chroma_client = None
        self.text_collection = None
        self.table_collection = None
        
        # åˆå§‹åŒ–ChromaDB
        self._setup_chroma()
    
    def _setup_chroma(self):
        """è®¾ç½®ChromaDB"""
        try:
            chroma_persist_dir = f"{self.storage_dir}/chroma_hybrid"
            
            self.chroma_client = chromadb.PersistentClient(path=chroma_persist_dir)
            
            # åˆ›å»ºä¸¤ä¸ªé›†åˆ
            try:
                self.text_collection = self.chroma_client.get_collection("text_index")
            except:
                self.text_collection = self.chroma_client.create_collection("text_index")
            
            try:
                self.table_collection = self.chroma_client.get_collection("table_index")
            except:
                self.table_collection = self.chroma_client.create_collection("table_index")
            
            logger.info("âœ… Hybrid Retriever ChromaDBåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ ChromaDBåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def build_hybrid_index(self, processed_documents: Dict, extracted_tables: Dict) -> bool:
        """æ„å»ºæ··åˆæ£€ç´¢ç´¢å¼•"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ„å»ºHybrid Retrieverç´¢å¼•")
            
            # 1. æ„å»ºæ–‡æœ¬ç´¢å¼•
            text_documents = []
            for doc_name, doc_data in processed_documents.items():
                for doc in doc_data['documents']:
                    doc.metadata.update({
                        'doc_type': 'text',
                        'channel': 'text_index',
                        'source_file': doc_name
                    })
                    text_documents.append(doc)
            
            if text_documents:
                text_vector_store = ChromaVectorStore(chroma_collection=self.text_collection)
                text_storage_context = StorageContext.from_defaults(vector_store=text_vector_store)
                self.text_index = VectorStoreIndex.from_documents(
                    text_documents,
                    storage_context=text_storage_context
                )
                logger.info(f"âœ… æ–‡æœ¬ç´¢å¼•æ„å»ºå®Œæˆ: {len(text_documents)}ä¸ªæ–‡æ¡£")
            
            # 2. æ„å»ºè¡¨æ ¼ç´¢å¼•
            table_documents = []
            for doc_name, tables in extracted_tables.items():
                for table in tables:
                    table_text = self._table_to_text(table)
                    table_doc = Document(
                        text=table_text,
                        metadata={
                            'doc_type': 'table',
                            'channel': 'table_index',
                            'indicator': table.get('summary', ''),
                            'year': self._extract_year_from_table(table),
                            'source': f"{doc_name}_page_{table['page_number']}",
                            'source_file': doc_name,  # æ·»åŠ source_fileå­—æ®µç”¨äºè¿‡æ»¤
                            'filename': doc_name,     # æ·»åŠ filenameå­—æ®µç”¨äºè¿‡æ»¤
                            'table_id': table['table_id'],
                            'is_financial': table.get('is_financial', False)
                        }
                    )
                    table_documents.append(table_doc)
            
            if table_documents:
                table_vector_store = ChromaVectorStore(chroma_collection=self.table_collection)
                table_storage_context = StorageContext.from_defaults(vector_store=table_vector_store)
                self.table_index = VectorStoreIndex.from_documents(
                    table_documents,
                    storage_context=table_storage_context
                )
                logger.info(f"âœ… è¡¨æ ¼ç´¢å¼•æ„å»ºå®Œæˆ: {len(table_documents)}ä¸ªæ–‡æ¡£")
            
            logger.info("âœ… Hybrid Retrieverç´¢å¼•æ„å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ··åˆç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def load_existing_index(self) -> bool:
        """ä»ç°æœ‰çš„ChromaDBé›†åˆåŠ è½½Hybrid Retrieverç´¢å¼•"""
        try:
            logger.info("ğŸ”„ å¼€å§‹åŠ è½½Hybrid Retrieverç´¢å¼•...")
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦æœ‰æ•°æ®
            text_count = self.text_collection.count() if self.text_collection else 0
            table_count = self.table_collection.count() if self.table_collection else 0
            
            logger.info(f"ğŸ“Š æ–‡æœ¬é›†åˆ: {text_count} ä¸ªå‘é‡")
            logger.info(f"ğŸ“Š è¡¨æ ¼é›†åˆ: {table_count} ä¸ªå‘é‡")
            
            if text_count == 0 and table_count == 0:
                logger.warning("âš ï¸ Hybrid Retrieveré›†åˆä¸ºç©ºï¼Œæ— æ³•åŠ è½½ç´¢å¼•")
                return False
            
            # åŠ è½½æ–‡æœ¬ç´¢å¼•
            if text_count > 0:
                try:
                    from llama_index.vector_stores.chroma import ChromaVectorStore
                    from llama_index.core import StorageContext
                    
                    text_vector_store = ChromaVectorStore(chroma_collection=self.text_collection)
                    text_storage_context = StorageContext.from_defaults(vector_store=text_vector_store)
                    self.text_index = VectorStoreIndex.from_vector_store(text_vector_store)
                    logger.info(f"âœ… æ–‡æœ¬ç´¢å¼•åŠ è½½æˆåŠŸ: {text_count} ä¸ªå‘é‡")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–‡æœ¬ç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
                    self.text_index = None
            
            # åŠ è½½è¡¨æ ¼ç´¢å¼•
            if table_count > 0:
                try:
                    from llama_index.vector_stores.chroma import ChromaVectorStore
                    from llama_index.core import StorageContext
                    
                    table_vector_store = ChromaVectorStore(chroma_collection=self.table_collection)
                    table_storage_context = StorageContext.from_defaults(vector_store=table_vector_store)
                    self.table_index = VectorStoreIndex.from_vector_store(table_vector_store)
                    logger.info(f"âœ… è¡¨æ ¼ç´¢å¼•åŠ è½½æˆåŠŸ: {table_count} ä¸ªå‘é‡")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¡¨æ ¼ç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
                    self.table_index = None
            
            # è‡³å°‘æœ‰ä¸€ä¸ªç´¢å¼•åŠ è½½æˆåŠŸå°±ç®—æˆåŠŸ
            if self.text_index or self.table_index:
                logger.info("âœ… Hybrid Retrieverç´¢å¼•åŠ è½½å®Œæˆ")
                return True
            else:
                logger.warning("âš ï¸ Hybrid Retrieverç´¢å¼•åŠ è½½å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½Hybrid Retrieverç´¢å¼•å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def retrieve(self, query: str, top_k: int = 10, 
                strategy: str = 'auto', context_filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            strategy: æ£€ç´¢ç­–ç•¥ ('auto', 'text_first', 'table_first', 'hybrid')
            context_filter: ä¸Šä¸‹æ–‡è¿‡æ»¤å™¨ï¼Œæ”¯æŒ:
                - filename: æ–‡ä»¶åè¿‡æ»¤
                - company: å…¬å¸åè¿‡æ»¤
                - year: å¹´ä»½è¿‡æ»¤
                - source_file: æºæ–‡ä»¶è¿‡æ»¤
        """
        try:
            # 1. æŸ¥è¯¢æ‰©å±•
            expanded_query = self.query_expander.expand_query(query)
            
            # 2. ç¡®å®šæ£€ç´¢ç­–ç•¥
            if strategy == 'auto':
                strategy = self._determine_retrieval_strategy(query)
            
            # 3. æ‰§è¡Œæ£€ç´¢ï¼ˆæ‰©å¤§æ£€ç´¢èŒƒå›´ï¼Œå› ä¸ºåç»­ä¼šè¿‡æ»¤ï¼‰
            expanded_top_k = top_k * 3 if context_filter else top_k  # å¦‚æœæœ‰è¿‡æ»¤æ¡ä»¶ï¼Œæ‰©å¤§æ£€ç´¢èŒƒå›´
            
            if strategy == 'text_first':
                results = self._retrieve_text_first(expanded_query, expanded_top_k)
            elif strategy == 'table_first':
                results = self._retrieve_table_first(expanded_query, expanded_top_k)
            else:  # hybrid
                results = self._retrieve_hybrid(expanded_query, expanded_top_k)
            
            # 4. åº”ç”¨ä¸Šä¸‹æ–‡è¿‡æ»¤å™¨
            if context_filter:
                results = self._apply_context_filter(results, context_filter)
            
            # 5. ç»¼åˆè¯„åˆ†å’Œæ’åº
            scored_results = []
            for result in results:
                score_result = self.scorer.calculate_comprehensive_score(
                    query, result['document'], result['semantic_score']
                )
                
                scored_results.append({
                    'document': result['document'],
                    'semantic_score': result['semantic_score'],
                    'comprehensive_score': score_result['comprehensive_score'],
                    'sim_score': score_result['sim_score'],
                    'metric_score': score_result['metric_score'],
                    'year_score': score_result['year_score'],
                    'strategy': strategy
                })
            
            # 6. æŒ‰ç»¼åˆè¯„åˆ†æ’åº
            scored_results.sort(key=lambda x: x['comprehensive_score'], reverse=True)
            
            # 7. è¿”å›Top-Kç»“æœ
            return scored_results[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _determine_retrieval_strategy(self, query: str) -> str:
        """ç¡®å®šæ£€ç´¢ç­–ç•¥"""
        # æ˜ç¡®çš„è´¢åŠ¡æŒ‡æ ‡å…³é”®è¯ï¼ˆåº”è¯¥ä¼˜å…ˆæ£€ç´¢è¡¨æ ¼ï¼‰
        financial_indicator_keywords = [
            'è¥ä¸šæ”¶å…¥', 'è¥æ”¶', 'æ”¶å…¥', 'å‡€åˆ©æ¶¦', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º', 'èµ„äº§æ€»é¢',
            'ROE', 'ROA', 'æ¯›åˆ©ç‡', 'å‡€åˆ©ç‡', 'æ€»èµ„äº§', 'å‡€èµ„äº§', 'è‚¡ä¸œæƒç›Š',
            'è¥ä¸šæˆæœ¬', 'é”€å”®è´¹ç”¨', 'ç®¡ç†è´¹ç”¨', 'è´¢åŠ¡è´¹ç”¨'
        ]
        
        # æ•°å€¼ç±»å…³é”®è¯
        numeric_keywords = ['å¢é•¿ç‡', 'å˜åŒ–å¹…åº¦', 'åŒæ¯”', 'ç¯æ¯”', 'æ•°æ®', 'æ•°å€¼', 'é‡‘é¢', 'æ¯”ä¾‹', 'è¶‹åŠ¿']
        
        # è¯­ä¹‰åˆ†æç±»å…³é”®è¯  
        semantic_keywords = ['è¡¨ç°å¦‚ä½•', 'è¶‹åŠ¿è¯´æ˜', 'åˆ†æ', 'è¯„ä»·', 'æƒ…å†µ', 'æ¦‚è¿°']
        
        # å¦‚æœæŸ¥è¯¢åŒ…å«æ˜ç¡®çš„è´¢åŠ¡æŒ‡æ ‡ï¼Œä¼˜å…ˆä½¿ç”¨è¡¨æ ¼æ£€ç´¢
        if any(keyword in query for keyword in financial_indicator_keywords):
            logger.info(f"ğŸ“Š æ£€æµ‹åˆ°è´¢åŠ¡æŒ‡æ ‡å…³é”®è¯ï¼Œä½¿ç”¨è¡¨æ ¼ä¼˜å…ˆæ£€ç´¢ç­–ç•¥")
            return 'table_first'  # è¡¨æ ¼ä¼˜å…ˆ
        elif any(keyword in query for keyword in numeric_keywords):
            return 'table_first'  # è¡¨æ ¼ä¼˜å…ˆ
        elif any(keyword in query for keyword in semantic_keywords):
            return 'text_first'   # æ–‡æœ¬ä¼˜å…ˆ
        else:
            return 'hybrid'       # æ··åˆæ£€ç´¢
    
    def _retrieve_text_first(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ–‡æœ¬ä¼˜å…ˆæ£€ç´¢"""
        results = []
        
        if self.text_index:
            retriever = VectorIndexRetriever(index=self.text_index, similarity_top_k=top_k)
            nodes = retriever.retrieve(query)
            
            for node in nodes:
                results.append({
                    'document': node,
                    'semantic_score': getattr(node, 'score', 0.0)
                })
        
        return results
    
    def _retrieve_table_first(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """è¡¨æ ¼ä¼˜å…ˆæ£€ç´¢"""
        results = []
        
        if self.table_index:
            retriever = VectorIndexRetriever(index=self.table_index, similarity_top_k=top_k)
            nodes = retriever.retrieve(query)
            
            for node in nodes:
                results.append({
                    'document': node,
                    'semantic_score': getattr(node, 'score', 0.0)
                })
        
        return results
    
    def _retrieve_hybrid(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢"""
        results = []
        
        # æ–‡æœ¬æ£€ç´¢
        if self.text_index:
            text_retriever = VectorIndexRetriever(index=self.text_index, similarity_top_k=top_k//2)
            text_nodes = text_retriever.retrieve(query)
            
            for node in text_nodes:
                results.append({
                    'document': node,
                    'semantic_score': getattr(node, 'score', 0.0)
                })
        
        # è¡¨æ ¼æ£€ç´¢
        if self.table_index:
            table_retriever = VectorIndexRetriever(index=self.table_index, similarity_top_k=top_k//2)
            table_nodes = table_retriever.retrieve(query)
            
            for node in table_nodes:
                results.append({
                    'document': node,
                    'semantic_score': getattr(node, 'score', 0.0)
                })
        
        return results
    
    def _table_to_text(self, table: Dict[str, Any]) -> str:
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤º"""
        try:
            text_parts = []
            
            # æ·»åŠ è¡¨æ ¼åŸºæœ¬ä¿¡æ¯
            text_parts.append(f"ğŸ“Š è¡¨æ ¼æ•°æ® - {table['table_id']}")
            text_parts.append(f"ğŸ“„ æ¥æºé¡µç : ç¬¬{table['page_number']}é¡µ")
            
            if table.get('is_financial'):
                text_parts.append("ğŸ’° ç±»å‹: è´¢åŠ¡æ•°æ®è¡¨æ ¼")
            
            if table.get('summary'):
                text_parts.append(f"ğŸ“ è¡¨æ ¼æ‘˜è¦: {table['summary']}")
            
            # æ·»åŠ è¡¨æ ¼æ•°æ®
            if 'table_data' in table:
                table_data = table['table_data']
                columns = table_data['columns']
                data_rows = table_data['data']
                
                # ç”ŸæˆMarkdownè¡¨æ ¼
                text_parts.append("\n**è¡¨æ ¼å†…å®¹ï¼š**\n")
                
                # è¡¨å¤´
                header = "| " + " | ".join([str(col) for col in columns]) + " |"
                text_parts.append(header)
                
                # åˆ†éš”çº¿
                separator = "|" + "|".join(["---" for _ in columns]) + "|"
                text_parts.append(separator)
                
                # æ•°æ®è¡Œ
                max_rows = min(len(data_rows), 30)
                for row in data_rows[:max_rows]:
                    row_str = "| " + " | ".join([str(cell) if cell else "" for cell in row]) + " |"
                    text_parts.append(row_str)
                
                if len(data_rows) > max_rows:
                    text_parts.append(f"\n... (è¡¨æ ¼å…±æœ‰ {len(data_rows)} è¡Œæ•°æ®)")
            
            return "\n".join(text_parts)
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼è½¬æ–‡æœ¬å¤±è´¥: {str(e)}")
            return f"è¡¨æ ¼ {table.get('table_id', 'unknown')}"
    
    def _extract_year_from_table(self, table: Dict[str, Any]) -> Optional[int]:
        """ä»è¡¨æ ¼ä¸­æå–å¹´ä»½"""
        try:
            # ä»è¡¨æ ¼æ‘˜è¦ä¸­æå–å¹´ä»½
            summary = table.get('summary', '')
            year_match = re.search(r'(\d{4})', summary)
            if year_match:
                return int(year_match.group(1))
            
            # ä»è¡¨æ ¼æ•°æ®ä¸­æå–å¹´ä»½
            if 'table_data' in table:
                table_data = table['table_data']
                for row in table_data.get('data', []):
                    for cell in row:
                        if isinstance(cell, str):
                            year_match = re.search(r'(\d{4})', cell)
                            if year_match:
                                year = int(year_match.group(1))
                                if 2000 <= year <= 2030:  # åˆç†çš„å¹´ä»½èŒƒå›´
                                    return year
            
            return None
            
        except Exception as e:
            logger.error(f"æå–å¹´ä»½å¤±è´¥: {str(e)}")
            return None
    
    def _apply_context_filter(self, results: List[Dict[str, Any]], 
                             context_filter: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åº”ç”¨ä¸Šä¸‹æ–‡è¿‡æ»¤å™¨è¿‡æ»¤æ£€ç´¢ç»“æœ"""
        filtered_results = []
        
        for result in results:
            document = result['document']
            metadata = document.metadata
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶
            match = True
            
            # æ–‡ä»¶åè¿‡æ»¤ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
            if 'filename' in context_filter:
                filename = context_filter['filename']
                # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„å­—æ®µï¼šfilename, source_file, source
                doc_filename = (metadata.get('filename') or 
                              metadata.get('source_file') or 
                              metadata.get('source', ''))
                
                # å¦‚æœsourceå­—æ®µåŒ…å«æ–‡ä»¶åï¼ˆæ ¼å¼ï¼šfilename_page_xxxï¼‰ï¼Œæå–æ–‡ä»¶åéƒ¨åˆ†
                if not doc_filename and metadata.get('source'):
                    source = metadata.get('source', '')
                    # sourceæ ¼å¼å¯èƒ½æ˜¯ "filename_page_1"ï¼Œæå–æ–‡ä»¶åéƒ¨åˆ†
                    if '_page_' in source:
                        doc_filename = source.split('_page_')[0]
                    else:
                        doc_filename = source
                
                # æ ‡å‡†åŒ–æ–‡ä»¶åï¼ˆç§»é™¤è·¯å¾„ã€ç»Ÿä¸€å¤§å°å†™ï¼‰
                filename_normalized = filename.lower().strip()
                doc_filename_normalized = doc_filename.lower().strip()
                
                # ä¸¥æ ¼åŒ¹é…ï¼šæ–‡ä»¶åå¿…é¡»å®Œå…¨åŒ¹é…æˆ–åŒ…å«å…³é”®éƒ¨åˆ†
                # ä¾‹å¦‚ï¼š"å¹³å®‰é“¶è¡Œ2024å¹´å¹´æŠ¥.PDF" åº”è¯¥åŒ¹é… "å¹³å®‰é“¶è¡Œ2024å¹´å¹´æŠ¥.PDF"
                # ä½†ä¸åº”è¯¥åŒ¹é… "æ•°æºç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸2023å¹´å¹´åº¦æŠ¥å‘Š_1766454332.PDF"
                if filename_normalized != doc_filename_normalized:
                    # å¦‚æœå®Œå…¨åŒ¹é…å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®éƒ¨åˆ†ï¼ˆè‡³å°‘å‰3ä¸ªå­—ç¬¦ï¼‰
                    if len(filename_normalized) >= 3:
                        # æå–æ–‡ä»¶åçš„ä¸»è¦éƒ¨åˆ†ï¼ˆç§»é™¤æ‰©å±•åå’Œç‰¹æ®Šå­—ç¬¦ï¼‰
                        import re
                        filename_key = re.sub(r'\.[^.]+$', '', filename_normalized)  # ç§»é™¤æ‰©å±•å
                        filename_key = re.sub(r'[_\-\s]+', '', filename_key)  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                        doc_filename_key = re.sub(r'\.[^.]+$', '', doc_filename_normalized)
                        doc_filename_key = re.sub(r'[_\-\s]+', '', doc_filename_key)
                        
                        # æ£€æŸ¥å…³é”®éƒ¨åˆ†æ˜¯å¦åŒ¹é…ï¼ˆè‡³å°‘å‰3ä¸ªå­—ç¬¦ï¼‰
                        if len(filename_key) >= 3 and len(doc_filename_key) >= 3:
                            if filename_key[:3] != doc_filename_key[:3]:
                                match = False
                                logger.debug(f"æ–‡ä»¶åå…³é”®éƒ¨åˆ†ä¸åŒ¹é…: è¿‡æ»¤æ¡ä»¶='{filename_key[:3]}', æ–‡æ¡£æ–‡ä»¶å='{doc_filename_key[:3]}'")
                        else:
                            match = False
                    else:
                        match = False
                
                # è°ƒè¯•æ—¥å¿—
                if not match:
                    logger.debug(f"æ–‡ä»¶åä¸åŒ¹é…: è¿‡æ»¤æ¡ä»¶='{filename}', æ–‡æ¡£æ–‡ä»¶å='{doc_filename}', å…ƒæ•°æ®={list(metadata.keys())}")
                else:
                    logger.debug(f"âœ… æ–‡ä»¶ååŒ¹é…: è¿‡æ»¤æ¡ä»¶='{filename}', æ–‡æ¡£æ–‡ä»¶å='{doc_filename}'")
            
            # æºæ–‡ä»¶è¿‡æ»¤
            if match and 'source_file' in context_filter:
                source_file = context_filter['source_file']
                doc_source = metadata.get('source_file') or metadata.get('filename', '')
                if source_file not in doc_source and doc_source not in source_file:
                    match = False
            
            # å…¬å¸åè¿‡æ»¤ï¼ˆä»æ–‡æ¡£æ–‡æœ¬æˆ–å…ƒæ•°æ®ä¸­æ£€æŸ¥ï¼‰
            if match and 'company' in context_filter:
                company = context_filter['company']
                doc_text = document.text.lower()
                doc_company = metadata.get('company', '').lower()
                doc_filename = (metadata.get('filename') or metadata.get('source_file', '')).lower()
                
                # æ£€æŸ¥æ–‡æ¡£æ–‡æœ¬æˆ–å…ƒæ•°æ®ä¸­æ˜¯å¦åŒ…å«å…¬å¸å
                company_lower = company.lower()
                
                # ä»æ–‡ä»¶åä¸­æå–å…¬å¸åï¼ˆå¦‚æœæ–‡ä»¶ååŒ…å«å…¬å¸åï¼‰
                filename_company = None
                if doc_filename:
                    # ç§»é™¤å¸¸è§çš„æŠ¥è¡¨ç±»å‹å…³é”®è¯å’Œå¹´ä»½
                    import re
                    clean_filename = re.sub(r'(åˆ©æ¶¦è¡¨|èµ„äº§è´Ÿå€ºè¡¨|ç°é‡‘æµé‡è¡¨|å¹´æŠ¥|æŠ¥å‘Š|è´¢åŠ¡æŠ¥è¡¨|è´¢åŠ¡æŠ¥å‘Š|\d{4}å¹´?)', '', doc_filename, flags=re.IGNORECASE)
                    clean_filename = re.sub(r'å¹´åº¦\d+', '', clean_filename)  # ç§»é™¤"å¹´åº¦60"ç­‰
                    clean_filename = re.sub(r'[_\-\s\.]+', '', clean_filename)
                    if len(clean_filename) >= 2:
                        filename_company = clean_filename
                
                # ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶ååŒ¹é…ï¼ˆæœ€å‡†ç¡®ï¼‰
                company_found = False
                
                # 1. ä¼˜å…ˆæ£€æŸ¥æ–‡ä»¶ååŒ¹é…ï¼ˆæœ€ä¸¥æ ¼ï¼‰
                if filename_company:
                    # æ–‡ä»¶ååŒ¹é…ï¼šå…¬å¸ååº”è¯¥åœ¨æ–‡ä»¶åä¸­ï¼Œæˆ–è€…æ–‡ä»¶ååœ¨å…¬å¸åä¸­
                    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„åŒ¹é…ï¼šè‡³å°‘åŒ¹é…å‰3ä¸ªå­—ç¬¦
                    if len(company_lower) >= 3 and len(filename_company) >= 3:
                        # æ£€æŸ¥å‰3ä¸ªå­—ç¬¦æ˜¯å¦åŒ¹é…
                        if company_lower[:3] == filename_company[:3]:
                            company_found = True
                            logger.debug(f"âœ… æ–‡ä»¶ååŒ¹é…: å…¬å¸å='{company_lower[:3]}', æ–‡ä»¶å='{filename_company[:3]}'")
                        # æˆ–è€…æ£€æŸ¥æ˜¯å¦åŒ…å«ï¼ˆåŒå‘ï¼‰
                        elif company_lower in filename_company or filename_company in company_lower:
                            company_found = True
                            logger.debug(f"âœ… æ–‡ä»¶ååŒ…å«åŒ¹é…: å…¬å¸å='{company}', æ–‡ä»¶å='{filename_company}'")
                    elif len(company_lower) >= 2 and len(filename_company) >= 2:
                        # å¦‚æœå…¬å¸åè¾ƒçŸ­ï¼Œè‡³å°‘åŒ¹é…å‰2ä¸ªå­—ç¬¦
                        if company_lower[:2] == filename_company[:2]:
                            company_found = True
                            logger.debug(f"âœ… æ–‡ä»¶ååŒ¹é…ï¼ˆçŸ­åï¼‰: å…¬å¸å='{company_lower[:2]}', æ–‡ä»¶å='{filename_company[:2]}'")
                
                # 2. å¦‚æœæ–‡ä»¶ååŒ¹é…å¤±è´¥ï¼Œæ£€æŸ¥æ–‡æ¡£æ–‡æœ¬å’Œå…ƒæ•°æ®
                if not company_found:
                    # æ£€æŸ¥æ–‡æ¡£æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å…¬å¸åï¼ˆè¦æ±‚è‡³å°‘3ä¸ªå­—ç¬¦åŒ¹é…ï¼‰
                    if len(company_lower) >= 3:
                        # åœ¨æ–‡æ¡£æ–‡æœ¬ä¸­æŸ¥æ‰¾å…¬å¸åï¼ˆè¦æ±‚å®Œæ•´åŒ¹é…ï¼Œé¿å…è¯¯åŒ¹é…ï¼‰
                        # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                        import re
                        # æ„å»ºåŒ¹é…æ¨¡å¼ï¼šå…¬å¸åå‰åå¯ä»¥æœ‰æ ‡ç‚¹æˆ–ç©ºæ ¼
                        pattern = re.escape(company_lower)
                        if re.search(pattern, doc_text):
                            company_found = True
                            logger.debug(f"âœ… æ–‡æ¡£æ–‡æœ¬åŒ¹é…: å…¬å¸å='{company}'")
                    
                    # æ£€æŸ¥å…ƒæ•°æ®ä¸­çš„å…¬å¸å
                    if not company_found and doc_company:
                        if company_lower in doc_company or doc_company in company_lower:
                            company_found = True
                            logger.debug(f"âœ… å…ƒæ•°æ®åŒ¹é…: å…¬å¸å='{company}', å…ƒæ•°æ®='{doc_company}'")
                
                if not company_found:
                    # å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›®æ ‡å…¬å¸åï¼Œåˆ™æ’é™¤
                    match = False
                    logger.debug(f"âŒ å…¬å¸åä¸åŒ¹é…: è¿‡æ»¤æ¡ä»¶='{company}', æ–‡æ¡£æ–‡ä»¶å='{doc_filename}', æ–‡ä»¶åæå–='{filename_company}'")
            
            # å¹´ä»½è¿‡æ»¤
            if match and 'year' in context_filter:
                filter_year = str(context_filter['year'])
                doc_year = str(metadata.get('year', ''))
                if doc_year and filter_year != doc_year:
                    match = False
            
            if match:
                filtered_results.append(result)
        
        if context_filter and filtered_results:
            logger.info(f"âœ… ä¸Šä¸‹æ–‡è¿‡æ»¤: ä» {len(results)} ä¸ªç»“æœä¸­è¿‡æ»¤å‡º {len(filtered_results)} ä¸ªåŒ¹é…ç»“æœ")
            if 'filename' in context_filter:
                logger.info(f"   è¿‡æ»¤æ¡ä»¶: filename={context_filter['filename']}")
            if 'company' in context_filter:
                logger.info(f"   è¿‡æ»¤æ¡ä»¶: company={context_filter['company']}")
        
        return filtered_results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'text_index_ready': self.text_index is not None,
            'table_index_ready': self.table_index is not None,
            'text_collection_count': self.text_collection.count() if self.text_collection else 0,
            'table_collection_count': self.table_collection.count() if self.table_collection else 0,
            'weights': self.scorer.weights,
            'financial_metrics_count': len(self.scorer.financial_metrics)
        }
        
        return stats
